{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:42.069579Z",
     "start_time": "2025-11-13T08:29:42.063306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "def setup_logging(log_dir:Path):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    log_filename = log_dir/f\"{timestamp}.log\"\n",
    "\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[stdout_handler, file_handler]\n",
    "    )\n",
    "\n",
    "def flush_logger():\n",
    "    stdout_handler.flush()\n",
    "\n",
    "setup_logging(Path('../logs'))\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "e5ac7a92777dcdbf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:43.485082Z",
     "start_time": "2025-11-13T08:29:42.077593Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from config import FieldConfig, ExperimentConfig\n",
    "from pathlib import Path\n",
    "DATA_PATH = Path('C:/Work/Data/proc/')\n",
    "field_config = FieldConfig()\n",
    "\n",
    "df = pd.read_csv(DATA_PATH/'rec_data2.csv')\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "df_cleaned = df.dropna(subset=[field_config.date, field_config.amount, field_config.text, field_config.label])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "id                    0\n",
      "accountId             0\n",
      "date                  0\n",
      "amount                0\n",
      "bankRawDescription    0\n",
      "isRecurring           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:43.695185Z",
     "start_time": "2025-11-13T08:29:43.541459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from feature_processor import FeatProcParams\n",
    "from config import *\n",
    "\n",
    "feat_params = FeatProcParams(n_bins=20, k_top=50)\n",
    "feat_params_off = FeatProcParams.NOP()\n",
    "\n",
    "exp_config = ExperimentConfig()\n",
    "\n",
    "fracs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]"
   ],
   "id": "64ad44e449b7478f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:45.326378Z",
     "start_time": "2025-11-13T08:29:43.700971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from runner import ExpRunner\n",
    "from config import EmbModel\n",
    "from embedder import EmbeddingService\n",
    "\n",
    "runner1 = ExpRunner.create(\n",
    "    exp_params=exp_config,\n",
    "    full_df=df_cleaned,\n",
    "    emb_params=EmbeddingService.Params(model_name=EmbModel.ALBERT),\n",
    "    feat_proc_params=feat_params,\n",
    "    field_config=FieldConfig()\n",
    ")"
   ],
   "id": "d651a17edece86d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoav.haimovitch\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:49.021161Z",
     "start_time": "2025-11-13T08:29:45.347410Z"
    }
   },
   "cell_type": "code",
   "source": "results = runner1.run_torch(fracs)",
   "id": "97b17b877b9520a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 10:29:45,353 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-13 10:29:45,421 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-13 10:29:45,422 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-13 10:29:45,429 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-13 10:29:45,432 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-13 10:29:45,444 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-13 10:29:45,445 - INFO - Creating new EmbeddingService(model_name=albert-base-v2)\n",
      "2025-11-13 10:29:45,448 - INFO - Loading embedding model: albert-base-v2...\n",
      "2025-11-13 10:29:47,111 - INFO - Model albert-base-v2 loaded onto cpu. Cache at cache/albert-base-v2\n",
      "2025-11-13 10:29:47,111 - INFO - Embedding 16704 train texts...\n",
      "2025-11-13 10:29:47,258 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-13 10:29:47,519 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:29:47,792 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:29:48,466 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-13 10:29:48,467 - INFO - Fitting categorical amount features...\n",
      "2025-11-13 10:29:48,475 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:29:48,475 - INFO - Created 20 fallback bins.\n",
      "2025-11-13 10:29:48,476 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-13 10:29:48,477 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:29:48,478 - INFO - Transforming 16704 rows...\n",
      "2025-11-13 10:29:48,545 - INFO - Transform complete.\n",
      "2025-11-13 10:29:48,545 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:29:48,666 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 887\n",
      "2025-11-13 10:29:48,694 - INFO - Starting PyTorch training on cpu for 10 epochs...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m results = \u001B[43mrunner1\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_torch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfracs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\src\\runner.py:341\u001B[39m, in \u001B[36mExpRunner.run_torch\u001B[39m\u001B[34m(self, fractions)\u001B[39m\n\u001B[32m    339\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m frac, sub_train_df \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.create_learning_curve_splits(df_train, fractions):\n\u001B[32m    340\u001B[39m     train_feature_set, test_feature_set, processor, meta = \u001B[38;5;28mself\u001B[39m.build_data_for_pytorch(sub_train_df, df_test)\n\u001B[32m--> \u001B[39m\u001B[32m341\u001B[39m     res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_experiment_pytorch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_feature_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_feature_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    342\u001B[39m     d = {\n\u001B[32m    343\u001B[39m         **res,\n\u001B[32m    344\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mtrain_frac\u001B[39m\u001B[33m\"\u001B[39m: r(frac),\n\u001B[32m   (...)\u001B[39m\u001B[32m    348\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mtest_accounts\u001B[39m\u001B[33m\"\u001B[39m: df_test[\u001B[38;5;28mself\u001B[39m.field_config.accountId].nunique()\n\u001B[32m    349\u001B[39m     }\n\u001B[32m    350\u001B[39m     logger.info(d)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\src\\runner.py:311\u001B[39m, in \u001B[36mExpRunner.run_experiment_pytorch\u001B[39m\u001B[34m(self, train_features, test_features, processor)\u001B[39m\n\u001B[32m    308\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m, NUM_EPOCHS + \u001B[32m1\u001B[39m):\n\u001B[32m    309\u001B[39m     start_time = time.time()\n\u001B[32m--> \u001B[39m\u001B[32m311\u001B[39m     train_loss = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    312\u001B[39m     metrics = evaluate_model(model, test_loader, criterion, DEVICE)\n\u001B[32m    314\u001B[39m     epoch_time = time.time() - start_time\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\src\\trainer.py:28\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, optimizer, criterion, device)\u001B[39m\n\u001B[32m     24\u001B[39m y_true = batch.y.to(device)\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# -----------------------------\u001B[39;00m\n\u001B[32m     26\u001B[39m \n\u001B[32m     27\u001B[39m \u001B[38;5;66;03m# 1. Forward pass\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m logits = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_continuous\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_categorical\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# 2. Calculate loss\u001B[39;00m\n\u001B[32m     31\u001B[39m loss = criterion(logits, y_true)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\src\\classifier.py:97\u001B[39m, in \u001B[36mHybridModel.forward\u001B[39m\u001B[34m(self, x_text, x_continuous, x_categorical)\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.categorical_feature_names):\n\u001B[32m     96\u001B[39m     cat_ids = x_categorical[:, i]\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m     embedded_cats.append(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membedding_layers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcat_ids\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m     99\u001B[39m all_embedded_cats = torch.cat(embedded_cats, dim=\u001B[32m1\u001B[39m)\n\u001B[32m    100\u001B[39m active_features.append(all_embedded_cats)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:192\u001B[39m, in \u001B[36mEmbedding.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    191\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    194\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    196\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    199\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    200\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2546\u001B[39m, in \u001B[36membedding\u001B[39m\u001B[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[39m\n\u001B[32m   2540\u001B[39m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[32m   2541\u001B[39m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[32m   2542\u001B[39m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[32m   2543\u001B[39m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[32m   2544\u001B[39m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[32m   2545\u001B[39m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[32m-> \u001B[39m\u001B[32m2546\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mIndexError\u001B[39m: index out of range in self"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:49.024097100Z",
     "start_time": "2025-11-12T19:09:20.363958Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "992cb940d94cdeb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16704: {'loss': 0.317,\n",
       "  'accuracy': 0.877,\n",
       "  'f1': 0.712,\n",
       "  'roc_auc': 0.913,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.1,\n",
       "  'train_size': 16704,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 98,\n",
       "  'test_accounts': 246},\n",
       " 29736: {'loss': 0.261,\n",
       "  'accuracy': 0.901,\n",
       "  'f1': 0.705,\n",
       "  'roc_auc': 0.927,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.2,\n",
       "  'train_size': 29736,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 196,\n",
       "  'test_accounts': 246},\n",
       " 42410: {'loss': 0.232,\n",
       "  'accuracy': 0.915,\n",
       "  'f1': 0.769,\n",
       "  'roc_auc': 0.941,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.3,\n",
       "  'train_size': 42410,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 295,\n",
       "  'test_accounts': 246},\n",
       " 59801: {'loss': 0.213,\n",
       "  'accuracy': 0.92,\n",
       "  'f1': 0.783,\n",
       "  'roc_auc': 0.949,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.4,\n",
       "  'train_size': 59801,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 393,\n",
       "  'test_accounts': 246},\n",
       " 75623: {'loss': 0.222,\n",
       "  'accuracy': 0.917,\n",
       "  'f1': 0.78,\n",
       "  'roc_auc': 0.945,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.5,\n",
       "  'train_size': 75623,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 492,\n",
       "  'test_accounts': 246},\n",
       " 92281: {'loss': 0.205,\n",
       "  'accuracy': 0.924,\n",
       "  'f1': 0.799,\n",
       "  'roc_auc': 0.953,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.6,\n",
       "  'train_size': 92281,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 590,\n",
       "  'test_accounts': 246},\n",
       " 108248: {'loss': 0.204,\n",
       "  'accuracy': 0.924,\n",
       "  'f1': 0.799,\n",
       "  'roc_auc': 0.954,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.7,\n",
       "  'train_size': 108248,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 688,\n",
       "  'test_accounts': 246}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:49.024097100Z",
     "start_time": "2025-11-12T19:09:20.411471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "runner2 = ExpRunner.copy(runner1)\n",
    "runner2.feat_proc_params = feat_params_off\n",
    "results2 = runner2.run_torch(fracs)\n"
   ],
   "id": "96f96304f4d9f7cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12 21:09:20,419 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-12 21:09:20,520 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-12 21:09:20,521 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-12 21:09:20,528 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-12 21:09:20,535 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-12 21:09:20,548 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-12 21:09:20,549 - INFO - Creating new EmbeddingService(model_name=albert-base-v2)\n",
      "2025-11-12 21:09:20,550 - INFO - Loading embedding model: albert-base-v2...\n",
      "2025-11-12 21:09:22,093 - INFO - Model albert-base-v2 loaded onto cpu. Cache at cache/albert-base-v2\n",
      "2025-11-12 21:09:22,195 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-12 21:09:22,719 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:09:23,272 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-12 21:09:23,273 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:09:23,273 - INFO - Transforming 16704 rows...\n",
      "2025-11-12 21:09:23,274 - INFO - Transform complete.\n",
      "2025-11-12 21:09:23,275 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:09:23,276 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:09:23,293 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:09:24,581 - INFO - Epoch 1/10 [1.29s] | Train Loss: 0.5879 | Test Loss: 0.4392 | F1: 0.6652 | ROC-AUC: 0.8812\n",
      "2025-11-12 21:09:26,047 - INFO - Epoch 2/10 [1.47s] | Train Loss: 0.4066 | Test Loss: 1.3574 | F1: 0.3921 | ROC-AUC: 0.7680\n",
      "2025-11-12 21:09:27,294 - INFO - Epoch 3/10 [1.25s] | Train Loss: 0.3228 | Test Loss: 0.3234 | F1: 0.5573 | ROC-AUC: 0.8683\n",
      "2025-11-12 21:09:28,618 - INFO - Epoch 4/10 [1.32s] | Train Loss: 0.2807 | Test Loss: 0.2990 | F1: 0.6646 | ROC-AUC: 0.8999\n",
      "2025-11-12 21:09:29,985 - INFO - Epoch 5/10 [1.37s] | Train Loss: 0.2606 | Test Loss: 0.3402 | F1: 0.5993 | ROC-AUC: 0.8831\n",
      "2025-11-12 21:09:31,291 - INFO - Epoch 6/10 [1.31s] | Train Loss: 0.2516 | Test Loss: 0.3102 | F1: 0.7457 | ROC-AUC: 0.9089\n",
      "2025-11-12 21:09:32,875 - INFO - Epoch 7/10 [1.58s] | Train Loss: 0.2458 | Test Loss: 0.2681 | F1: 0.7221 | ROC-AUC: 0.9216\n",
      "2025-11-12 21:09:34,251 - INFO - Epoch 8/10 [1.37s] | Train Loss: 0.2384 | Test Loss: 0.2649 | F1: 0.7182 | ROC-AUC: 0.9196\n",
      "2025-11-12 21:09:35,571 - INFO - Epoch 9/10 [1.32s] | Train Loss: 0.2299 | Test Loss: 0.2618 | F1: 0.7318 | ROC-AUC: 0.9235\n",
      "2025-11-12 21:09:36,946 - INFO - Epoch 10/10 [1.37s] | Train Loss: 0.2289 | Test Loss: 0.2964 | F1: 0.5529 | ROC-AUC: 0.9202\n",
      "2025-11-12 21:09:36,946 - INFO - Training complete.\n",
      "2025-11-12 21:09:36,952 - INFO - {'loss': 0.31, 'accuracy': 0.899, 'f1': 0.746, 'roc_auc': 0.909, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-12 21:09:36,961 - INFO - Yielding 20% split: 196 accounts, 29736 rows\n",
      "2025-11-12 21:09:37,122 - INFO - len(text_list)=29736 len(unique_texts)=19347 len(texts_to_embed)=0\n",
      "2025-11-12 21:09:37,735 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:09:38,255 - INFO - Fitting processor on 29736 rows...\n",
      "2025-11-12 21:09:38,256 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:09:38,256 - INFO - Transforming 29736 rows...\n",
      "2025-11-12 21:09:38,258 - INFO - Transform complete.\n",
      "2025-11-12 21:09:38,259 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:09:38,260 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:09:38,282 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:09:40,136 - INFO - Epoch 1/10 [1.85s] | Train Loss: 0.4982 | Test Loss: 0.3814 | F1: 0.3863 | ROC-AUC: 0.8279\n",
      "2025-11-12 21:09:42,026 - INFO - Epoch 2/10 [1.89s] | Train Loss: 0.3169 | Test Loss: 0.3001 | F1: 0.7260 | ROC-AUC: 0.8923\n",
      "2025-11-12 21:09:43,855 - INFO - Epoch 3/10 [1.83s] | Train Loss: 0.2724 | Test Loss: 0.2615 | F1: 0.7353 | ROC-AUC: 0.9163\n",
      "2025-11-12 21:09:45,812 - INFO - Epoch 4/10 [1.96s] | Train Loss: 0.2548 | Test Loss: 0.2938 | F1: 0.5841 | ROC-AUC: 0.9253\n",
      "2025-11-12 21:09:48,118 - INFO - Epoch 5/10 [2.31s] | Train Loss: 0.2408 | Test Loss: 0.2943 | F1: 0.7355 | ROC-AUC: 0.9147\n",
      "2025-11-12 21:09:50,006 - INFO - Epoch 6/10 [1.89s] | Train Loss: 0.2328 | Test Loss: 0.3181 | F1: 0.5879 | ROC-AUC: 0.9125\n",
      "2025-11-12 21:09:52,036 - INFO - Epoch 7/10 [2.03s] | Train Loss: 0.2259 | Test Loss: 0.2572 | F1: 0.7416 | ROC-AUC: 0.9162\n",
      "2025-11-12 21:09:54,053 - INFO - Epoch 8/10 [2.02s] | Train Loss: 0.2262 | Test Loss: 0.3589 | F1: 0.6472 | ROC-AUC: 0.8951\n",
      "2025-11-12 21:09:56,196 - INFO - Epoch 9/10 [2.14s] | Train Loss: 0.2197 | Test Loss: 0.3344 | F1: 0.7258 | ROC-AUC: 0.9205\n",
      "2025-11-12 21:09:58,332 - INFO - Epoch 10/10 [2.14s] | Train Loss: 0.2166 | Test Loss: 0.3044 | F1: 0.5662 | ROC-AUC: 0.9284\n",
      "2025-11-12 21:09:58,333 - INFO - Training complete.\n",
      "2025-11-12 21:09:58,338 - INFO - {'loss': 0.257, 'accuracy': 0.907, 'f1': 0.742, 'roc_auc': 0.916, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 29736, 'test_size': 33464, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-12 21:09:58,347 - INFO - Yielding 30% split: 295 accounts, 42410 rows\n",
      "2025-11-12 21:09:58,628 - INFO - len(text_list)=42410 len(unique_texts)=26700 len(texts_to_embed)=0\n",
      "2025-11-12 21:09:59,714 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:10:00,249 - INFO - Fitting processor on 42410 rows...\n",
      "2025-11-12 21:10:00,249 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:10:00,250 - INFO - Transforming 42410 rows...\n",
      "2025-11-12 21:10:00,251 - INFO - Transform complete.\n",
      "2025-11-12 21:10:00,251 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:10:00,252 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:10:00,283 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:10:02,915 - INFO - Epoch 1/10 [2.63s] | Train Loss: 0.4736 | Test Loss: 0.2969 | F1: 0.7117 | ROC-AUC: 0.8977\n",
      "2025-11-12 21:10:05,378 - INFO - Epoch 2/10 [2.46s] | Train Loss: 0.3052 | Test Loss: 0.3007 | F1: 0.7379 | ROC-AUC: 0.9045\n",
      "2025-11-12 21:10:07,742 - INFO - Epoch 3/10 [2.36s] | Train Loss: 0.2753 | Test Loss: 0.3257 | F1: 0.5370 | ROC-AUC: 0.9139\n",
      "2025-11-12 21:10:10,047 - INFO - Epoch 4/10 [2.30s] | Train Loss: 0.2565 | Test Loss: 0.3488 | F1: 0.5328 | ROC-AUC: 0.9284\n",
      "2025-11-12 21:10:12,552 - INFO - Epoch 5/10 [2.50s] | Train Loss: 0.2437 | Test Loss: 0.2516 | F1: 0.7491 | ROC-AUC: 0.9265\n",
      "2025-11-12 21:10:15,208 - INFO - Epoch 6/10 [2.65s] | Train Loss: 0.2386 | Test Loss: 0.2296 | F1: 0.7730 | ROC-AUC: 0.9389\n",
      "2025-11-12 21:10:18,239 - INFO - Epoch 7/10 [3.03s] | Train Loss: 0.2325 | Test Loss: 0.2324 | F1: 0.7677 | ROC-AUC: 0.9386\n",
      "2025-11-12 21:10:20,918 - INFO - Epoch 8/10 [2.68s] | Train Loss: 0.2283 | Test Loss: 0.2498 | F1: 0.7238 | ROC-AUC: 0.9387\n",
      "2025-11-12 21:10:23,531 - INFO - Epoch 9/10 [2.61s] | Train Loss: 0.2234 | Test Loss: 0.3730 | F1: 0.6871 | ROC-AUC: 0.9123\n",
      "2025-11-12 21:10:26,208 - INFO - Epoch 10/10 [2.68s] | Train Loss: 0.2187 | Test Loss: 0.2318 | F1: 0.7756 | ROC-AUC: 0.9381\n",
      "2025-11-12 21:10:26,209 - INFO - Training complete.\n",
      "2025-11-12 21:10:26,212 - INFO - {'loss': 0.232, 'accuracy': 0.916, 'f1': 0.776, 'roc_auc': 0.938, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 42410, 'test_size': 33464, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-12 21:10:26,222 - INFO - Yielding 40% split: 393 accounts, 59801 rows\n",
      "2025-11-12 21:10:26,573 - INFO - len(text_list)=59801 len(unique_texts)=37409 len(texts_to_embed)=0\n",
      "2025-11-12 21:10:27,738 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:10:28,381 - INFO - Fitting processor on 59801 rows...\n",
      "2025-11-12 21:10:28,382 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:10:28,382 - INFO - Transforming 59801 rows...\n",
      "2025-11-12 21:10:28,383 - INFO - Transform complete.\n",
      "2025-11-12 21:10:28,384 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:10:28,385 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:10:28,415 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:10:31,751 - INFO - Epoch 1/10 [3.33s] | Train Loss: 0.4346 | Test Loss: 0.3561 | F1: 0.7241 | ROC-AUC: 0.8964\n",
      "2025-11-12 21:10:35,058 - INFO - Epoch 2/10 [3.31s] | Train Loss: 0.2954 | Test Loss: 0.3950 | F1: 0.6590 | ROC-AUC: 0.8940\n",
      "2025-11-12 21:10:38,088 - INFO - Epoch 3/10 [3.03s] | Train Loss: 0.2652 | Test Loss: 0.2551 | F1: 0.7585 | ROC-AUC: 0.9196\n",
      "2025-11-12 21:10:41,427 - INFO - Epoch 4/10 [3.34s] | Train Loss: 0.2518 | Test Loss: 0.2421 | F1: 0.7633 | ROC-AUC: 0.9291\n",
      "2025-11-12 21:10:44,750 - INFO - Epoch 5/10 [3.32s] | Train Loss: 0.2434 | Test Loss: 0.2888 | F1: 0.7442 | ROC-AUC: 0.9270\n",
      "2025-11-12 21:10:48,489 - INFO - Epoch 6/10 [3.74s] | Train Loss: 0.2354 | Test Loss: 0.2874 | F1: 0.6357 | ROC-AUC: 0.9340\n",
      "2025-11-12 21:10:51,884 - INFO - Epoch 7/10 [3.39s] | Train Loss: 0.2283 | Test Loss: 0.2336 | F1: 0.7791 | ROC-AUC: 0.9375\n",
      "2025-11-12 21:10:55,269 - INFO - Epoch 8/10 [3.38s] | Train Loss: 0.2283 | Test Loss: 0.2280 | F1: 0.7665 | ROC-AUC: 0.9423\n",
      "2025-11-12 21:10:58,480 - INFO - Epoch 9/10 [3.21s] | Train Loss: 0.2186 | Test Loss: 0.3159 | F1: 0.7078 | ROC-AUC: 0.9205\n",
      "2025-11-12 21:11:01,731 - INFO - Epoch 10/10 [3.25s] | Train Loss: 0.2197 | Test Loss: 0.2380 | F1: 0.7675 | ROC-AUC: 0.9405\n",
      "2025-11-12 21:11:01,732 - INFO - Training complete.\n",
      "2025-11-12 21:11:01,737 - INFO - {'loss': 0.234, 'accuracy': 0.92, 'f1': 0.779, 'roc_auc': 0.937, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 59801, 'test_size': 33464, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-12 21:11:01,746 - INFO - Yielding 50% split: 492 accounts, 75623 rows\n",
      "2025-11-12 21:11:02,260 - INFO - len(text_list)=75623 len(unique_texts)=46835 len(texts_to_embed)=0\n",
      "2025-11-12 21:11:04,176 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:11:04,775 - INFO - Fitting processor on 75623 rows...\n",
      "2025-11-12 21:11:04,776 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:11:04,777 - INFO - Transforming 75623 rows...\n",
      "2025-11-12 21:11:04,778 - INFO - Transform complete.\n",
      "2025-11-12 21:11:04,779 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:11:04,780 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:11:04,820 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:11:08,529 - INFO - Epoch 1/10 [3.71s] | Train Loss: 0.3927 | Test Loss: 0.2711 | F1: 0.7097 | ROC-AUC: 0.9143\n",
      "2025-11-12 21:11:12,414 - INFO - Epoch 2/10 [3.88s] | Train Loss: 0.2774 | Test Loss: 0.2331 | F1: 0.7715 | ROC-AUC: 0.9360\n",
      "2025-11-12 21:11:16,264 - INFO - Epoch 3/10 [3.85s] | Train Loss: 0.2543 | Test Loss: 0.2531 | F1: 0.7342 | ROC-AUC: 0.9346\n",
      "2025-11-12 21:11:20,579 - INFO - Epoch 4/10 [4.31s] | Train Loss: 0.2412 | Test Loss: 0.2273 | F1: 0.7906 | ROC-AUC: 0.9384\n",
      "2025-11-12 21:11:24,637 - INFO - Epoch 5/10 [4.06s] | Train Loss: 0.2351 | Test Loss: 0.3092 | F1: 0.7250 | ROC-AUC: 0.9299\n",
      "2025-11-12 21:11:28,670 - INFO - Epoch 6/10 [4.03s] | Train Loss: 0.2289 | Test Loss: 0.2472 | F1: 0.7091 | ROC-AUC: 0.9402\n",
      "2025-11-12 21:11:33,107 - INFO - Epoch 7/10 [4.44s] | Train Loss: 0.2261 | Test Loss: 0.2180 | F1: 0.7668 | ROC-AUC: 0.9491\n",
      "2025-11-12 21:11:37,381 - INFO - Epoch 8/10 [4.27s] | Train Loss: 0.2226 | Test Loss: 0.2313 | F1: 0.7348 | ROC-AUC: 0.9469\n",
      "2025-11-12 21:11:41,532 - INFO - Epoch 9/10 [4.15s] | Train Loss: 0.2175 | Test Loss: 0.2161 | F1: 0.7918 | ROC-AUC: 0.9450\n",
      "2025-11-12 21:11:45,674 - INFO - Epoch 10/10 [4.14s] | Train Loss: 0.2147 | Test Loss: 0.2171 | F1: 0.7857 | ROC-AUC: 0.9433\n",
      "2025-11-12 21:11:45,675 - INFO - Training complete.\n",
      "2025-11-12 21:11:45,680 - INFO - {'loss': 0.216, 'accuracy': 0.924, 'f1': 0.792, 'roc_auc': 0.945, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.5, 'train_size': 75623, 'test_size': 33464, 'train_accounts': 492, 'test_accounts': 246}\n",
      "2025-11-12 21:11:45,692 - INFO - Yielding 60% split: 590 accounts, 92281 rows\n",
      "2025-11-12 21:11:46,249 - INFO - len(text_list)=92281 len(unique_texts)=56280 len(texts_to_embed)=0\n",
      "2025-11-12 21:11:48,577 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:11:49,165 - INFO - Fitting processor on 92281 rows...\n",
      "2025-11-12 21:11:49,166 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:11:49,167 - INFO - Transforming 92281 rows...\n",
      "2025-11-12 21:11:49,168 - INFO - Transform complete.\n",
      "2025-11-12 21:11:49,169 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:11:49,170 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:11:49,212 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:11:53,673 - INFO - Epoch 1/10 [4.46s] | Train Loss: 0.3826 | Test Loss: 0.2747 | F1: 0.7241 | ROC-AUC: 0.9118\n",
      "2025-11-12 21:11:58,234 - INFO - Epoch 2/10 [4.56s] | Train Loss: 0.2695 | Test Loss: 0.2385 | F1: 0.7585 | ROC-AUC: 0.9378\n",
      "2025-11-12 21:12:03,739 - INFO - Epoch 3/10 [5.50s] | Train Loss: 0.2476 | Test Loss: 0.2404 | F1: 0.7677 | ROC-AUC: 0.9397\n",
      "2025-11-12 21:12:09,030 - INFO - Epoch 4/10 [5.29s] | Train Loss: 0.2372 | Test Loss: 0.2334 | F1: 0.7702 | ROC-AUC: 0.9348\n",
      "2025-11-12 21:12:14,959 - INFO - Epoch 5/10 [5.93s] | Train Loss: 0.2307 | Test Loss: 0.2150 | F1: 0.7899 | ROC-AUC: 0.9462\n",
      "2025-11-12 21:12:20,961 - INFO - Epoch 6/10 [6.00s] | Train Loss: 0.2241 | Test Loss: 0.2136 | F1: 0.7933 | ROC-AUC: 0.9482\n",
      "2025-11-12 21:12:25,944 - INFO - Epoch 7/10 [4.98s] | Train Loss: 0.2205 | Test Loss: 0.2109 | F1: 0.7854 | ROC-AUC: 0.9502\n",
      "2025-11-12 21:12:30,862 - INFO - Epoch 8/10 [4.92s] | Train Loss: 0.2176 | Test Loss: 0.2173 | F1: 0.7752 | ROC-AUC: 0.9483\n",
      "2025-11-12 21:12:36,370 - INFO - Epoch 9/10 [5.51s] | Train Loss: 0.2157 | Test Loss: 0.2203 | F1: 0.7797 | ROC-AUC: 0.9482\n",
      "2025-11-12 21:12:41,328 - INFO - Epoch 10/10 [4.96s] | Train Loss: 0.2137 | Test Loss: 0.2202 | F1: 0.7745 | ROC-AUC: 0.9464\n",
      "2025-11-12 21:12:41,329 - INFO - Training complete.\n",
      "2025-11-12 21:12:41,334 - INFO - {'loss': 0.214, 'accuracy': 0.922, 'f1': 0.793, 'roc_auc': 0.948, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.6, 'train_size': 92281, 'test_size': 33464, 'train_accounts': 590, 'test_accounts': 246}\n",
      "2025-11-12 21:12:41,345 - INFO - Yielding 70% split: 688 accounts, 108248 rows\n",
      "2025-11-12 21:12:42,004 - INFO - len(text_list)=108248 len(unique_texts)=65808 len(texts_to_embed)=0\n",
      "2025-11-12 21:12:44,090 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:12:44,628 - INFO - Fitting processor on 108248 rows...\n",
      "2025-11-12 21:12:44,629 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:12:44,630 - INFO - Transforming 108248 rows...\n",
      "2025-11-12 21:12:44,631 - INFO - Transform complete.\n",
      "2025-11-12 21:12:44,632 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:12:44,633 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:12:44,692 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:12:50,183 - INFO - Epoch 1/10 [5.49s] | Train Loss: 0.3685 | Test Loss: 0.3295 | F1: 0.7243 | ROC-AUC: 0.9114\n",
      "2025-11-12 21:12:56,727 - INFO - Epoch 2/10 [6.54s] | Train Loss: 0.2658 | Test Loss: 0.2423 | F1: 0.7450 | ROC-AUC: 0.9368\n",
      "2025-11-12 21:13:02,364 - INFO - Epoch 3/10 [5.64s] | Train Loss: 0.2454 | Test Loss: 0.2580 | F1: 0.7675 | ROC-AUC: 0.9315\n",
      "2025-11-12 21:13:09,867 - INFO - Epoch 4/10 [7.50s] | Train Loss: 0.2337 | Test Loss: 0.2473 | F1: 0.7135 | ROC-AUC: 0.9427\n",
      "2025-11-12 21:13:17,512 - INFO - Epoch 5/10 [7.64s] | Train Loss: 0.2256 | Test Loss: 0.2476 | F1: 0.7862 | ROC-AUC: 0.9465\n",
      "2025-11-12 21:13:23,728 - INFO - Epoch 6/10 [6.22s] | Train Loss: 0.2220 | Test Loss: 0.2196 | F1: 0.7736 | ROC-AUC: 0.9465\n",
      "2025-11-12 21:13:29,693 - INFO - Epoch 7/10 [5.96s] | Train Loss: 0.2187 | Test Loss: 0.2269 | F1: 0.7731 | ROC-AUC: 0.9437\n",
      "2025-11-12 21:13:35,914 - INFO - Epoch 8/10 [6.22s] | Train Loss: 0.2170 | Test Loss: 0.2404 | F1: 0.6972 | ROC-AUC: 0.9448\n",
      "2025-11-12 21:13:41,791 - INFO - Epoch 9/10 [5.88s] | Train Loss: 0.2137 | Test Loss: 0.2170 | F1: 0.7696 | ROC-AUC: 0.9502\n",
      "2025-11-12 21:13:47,975 - INFO - Epoch 10/10 [6.18s] | Train Loss: 0.2120 | Test Loss: 0.2256 | F1: 0.7833 | ROC-AUC: 0.9408\n",
      "2025-11-12 21:13:47,976 - INFO - Training complete.\n",
      "2025-11-12 21:13:47,985 - INFO - {'loss': 0.248, 'accuracy': 0.91, 'f1': 0.786, 'roc_auc': 0.947, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.7, 'train_size': 108248, 'test_size': 33464, 'train_accounts': 688, 'test_accounts': 246}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{16704: {'loss': 0.31,\n",
       "  'accuracy': 0.899,\n",
       "  'f1': 0.746,\n",
       "  'roc_auc': 0.909,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.1,\n",
       "  'train_size': 16704,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 98,\n",
       "  'test_accounts': 246},\n",
       " 29736: {'loss': 0.257,\n",
       "  'accuracy': 0.907,\n",
       "  'f1': 0.742,\n",
       "  'roc_auc': 0.916,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.2,\n",
       "  'train_size': 29736,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 196,\n",
       "  'test_accounts': 246},\n",
       " 42410: {'loss': 0.232,\n",
       "  'accuracy': 0.916,\n",
       "  'f1': 0.776,\n",
       "  'roc_auc': 0.938,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.3,\n",
       "  'train_size': 42410,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 295,\n",
       "  'test_accounts': 246},\n",
       " 59801: {'loss': 0.234,\n",
       "  'accuracy': 0.92,\n",
       "  'f1': 0.779,\n",
       "  'roc_auc': 0.937,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.4,\n",
       "  'train_size': 59801,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 393,\n",
       "  'test_accounts': 246},\n",
       " 75623: {'loss': 0.216,\n",
       "  'accuracy': 0.924,\n",
       "  'f1': 0.792,\n",
       "  'roc_auc': 0.945,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.5,\n",
       "  'train_size': 75623,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 492,\n",
       "  'test_accounts': 246},\n",
       " 92281: {'loss': 0.214,\n",
       "  'accuracy': 0.922,\n",
       "  'f1': 0.793,\n",
       "  'roc_auc': 0.948,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.6,\n",
       "  'train_size': 92281,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 590,\n",
       "  'test_accounts': 246},\n",
       " 108248: {'loss': 0.248,\n",
       "  'accuracy': 0.91,\n",
       "  'f1': 0.786,\n",
       "  'roc_auc': 0.947,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.7,\n",
       "  'train_size': 108248,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 688,\n",
       "  'test_accounts': 246}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:49.024097100Z",
     "start_time": "2025-11-12T19:13:48.322479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# runner3 = ExpRunner.copy(runner1)\n",
    "# runner3.feat_proc_params = FeatProcParams(n_bins=20, k_top=50)\n",
    "# runner3.run_torch([0.1])\n"
   ],
   "id": "a26f355bbf7e1888",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:49.024097100Z",
     "start_time": "2025-11-12T21:03:30.169793Z"
    }
   },
   "cell_type": "code",
   "source": "results2 = runner2.run_torch(fracs)\n",
   "id": "5b1cc1626d140f29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12 23:03:30,179 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-12 23:03:30,267 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-12 23:03:30,269 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-12 23:03:30,280 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-12 23:03:30,288 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-12 23:03:30,310 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-12 23:03:30,438 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:31,234 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:31,888 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-12 23:03:31,888 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:03:31,889 - INFO - Transforming 16704 rows...\n",
      "2025-11-12 23:03:31,891 - INFO - Transform complete.\n",
      "2025-11-12 23:03:31,892 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:03:31,893 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:03:31,913 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:03:33,113 - INFO - Epoch 1/10 [1.20s] | Train Loss: 0.5402 | Test Loss: 0.3942 | F1: 0.3268 | ROC-AUC: 0.8249\n",
      "2025-11-12 23:03:34,319 - INFO - Epoch 2/10 [1.21s] | Train Loss: 0.3881 | Test Loss: 0.3652 | F1: 0.7097 | ROC-AUC: 0.8953\n",
      "2025-11-12 23:03:35,496 - INFO - Epoch 3/10 [1.18s] | Train Loss: 0.3180 | Test Loss: 0.6218 | F1: 0.5331 | ROC-AUC: 0.8380\n",
      "2025-11-12 23:03:36,691 - INFO - Epoch 4/10 [1.19s] | Train Loss: 0.2786 | Test Loss: 0.2922 | F1: 0.5913 | ROC-AUC: 0.9210\n",
      "2025-11-12 23:03:37,814 - INFO - Epoch 5/10 [1.12s] | Train Loss: 0.2624 | Test Loss: 0.2841 | F1: 0.7387 | ROC-AUC: 0.9122\n",
      "2025-11-12 23:03:38,931 - INFO - Epoch 6/10 [1.12s] | Train Loss: 0.2503 | Test Loss: 0.3447 | F1: 0.6682 | ROC-AUC: 0.8837\n",
      "2025-11-12 23:03:40,104 - INFO - Epoch 7/10 [1.17s] | Train Loss: 0.2392 | Test Loss: 0.2801 | F1: 0.7168 | ROC-AUC: 0.9146\n",
      "2025-11-12 23:03:41,290 - INFO - Epoch 8/10 [1.19s] | Train Loss: 0.2363 | Test Loss: 0.2715 | F1: 0.7301 | ROC-AUC: 0.9194\n",
      "2025-11-12 23:03:42,452 - INFO - Epoch 9/10 [1.16s] | Train Loss: 0.2326 | Test Loss: 0.3754 | F1: 0.7030 | ROC-AUC: 0.9140\n",
      "2025-11-12 23:03:43,609 - INFO - Epoch 10/10 [1.16s] | Train Loss: 0.2237 | Test Loss: 0.2592 | F1: 0.7343 | ROC-AUC: 0.9295\n",
      "2025-11-12 23:03:43,610 - INFO - Training complete.\n",
      "2025-11-12 23:03:43,636 - INFO - {'loss': 0.284, 'accuracy': 0.902, 'f1': 0.739, 'roc_auc': 0.912, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-12 23:03:43,645 - INFO - Yielding 20% split: 196 accounts, 29736 rows\n",
      "2025-11-12 23:03:43,814 - INFO - len(text_list)=29736 len(unique_texts)=19347 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:44,452 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:45,001 - INFO - Fitting processor on 29736 rows...\n",
      "2025-11-12 23:03:45,002 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:03:45,003 - INFO - Transforming 29736 rows...\n",
      "2025-11-12 23:03:45,004 - INFO - Transform complete.\n",
      "2025-11-12 23:03:45,004 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:03:45,006 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:03:45,042 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:03:46,608 - INFO - Epoch 1/10 [1.56s] | Train Loss: 0.5137 | Test Loss: 0.4197 | F1: 0.6848 | ROC-AUC: 0.8764\n",
      "2025-11-12 23:03:48,170 - INFO - Epoch 2/10 [1.56s] | Train Loss: 0.3277 | Test Loss: 0.3060 | F1: 0.7422 | ROC-AUC: 0.9043\n",
      "2025-11-12 23:03:49,726 - INFO - Epoch 3/10 [1.55s] | Train Loss: 0.2764 | Test Loss: 0.3424 | F1: 0.6772 | ROC-AUC: 0.8781\n",
      "2025-11-12 23:03:51,266 - INFO - Epoch 4/10 [1.54s] | Train Loss: 0.2596 | Test Loss: 0.2621 | F1: 0.7282 | ROC-AUC: 0.9191\n",
      "2025-11-12 23:03:52,868 - INFO - Epoch 5/10 [1.60s] | Train Loss: 0.2402 | Test Loss: 0.2612 | F1: 0.7608 | ROC-AUC: 0.9209\n",
      "2025-11-12 23:03:54,500 - INFO - Epoch 6/10 [1.63s] | Train Loss: 0.2377 | Test Loss: 0.2826 | F1: 0.6204 | ROC-AUC: 0.9271\n",
      "2025-11-12 23:03:56,217 - INFO - Epoch 7/10 [1.72s] | Train Loss: 0.2333 | Test Loss: 0.2713 | F1: 0.7554 | ROC-AUC: 0.9110\n",
      "2025-11-12 23:03:57,998 - INFO - Epoch 8/10 [1.78s] | Train Loss: 0.2269 | Test Loss: 0.2600 | F1: 0.7304 | ROC-AUC: 0.9173\n",
      "2025-11-12 23:03:59,826 - INFO - Epoch 9/10 [1.83s] | Train Loss: 0.2189 | Test Loss: 0.2483 | F1: 0.7235 | ROC-AUC: 0.9292\n",
      "2025-11-12 23:04:01,685 - INFO - Epoch 10/10 [1.86s] | Train Loss: 0.2161 | Test Loss: 0.2551 | F1: 0.7225 | ROC-AUC: 0.9279\n",
      "2025-11-12 23:04:01,686 - INFO - Training complete.\n",
      "2025-11-12 23:04:01,688 - INFO - {'loss': 0.261, 'accuracy': 0.908, 'f1': 0.761, 'roc_auc': 0.921, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 29736, 'test_size': 33464, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-12 23:04:01,697 - INFO - Yielding 30% split: 295 accounts, 42410 rows\n",
      "2025-11-12 23:04:01,961 - INFO - len(text_list)=42410 len(unique_texts)=26700 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:03,766 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:04,861 - INFO - Fitting processor on 42410 rows...\n",
      "2025-11-12 23:04:04,862 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:04:04,863 - INFO - Transforming 42410 rows...\n",
      "2025-11-12 23:04:04,863 - INFO - Transform complete.\n",
      "2025-11-12 23:04:04,864 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:04:04,865 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:04:04,902 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:04:08,540 - INFO - Epoch 1/10 [3.64s] | Train Loss: 0.4850 | Test Loss: 0.3557 | F1: 0.6780 | ROC-AUC: 0.8853\n",
      "2025-11-12 23:04:12,099 - INFO - Epoch 2/10 [3.56s] | Train Loss: 0.3029 | Test Loss: 0.2977 | F1: 0.5623 | ROC-AUC: 0.9281\n",
      "2025-11-12 23:04:15,692 - INFO - Epoch 3/10 [3.59s] | Train Loss: 0.2740 | Test Loss: 0.6315 | F1: 0.5537 | ROC-AUC: 0.9015\n",
      "2025-11-12 23:04:19,180 - INFO - Epoch 4/10 [3.49s] | Train Loss: 0.2605 | Test Loss: 0.3186 | F1: 0.5851 | ROC-AUC: 0.9271\n",
      "2025-11-12 23:04:22,812 - INFO - Epoch 5/10 [3.63s] | Train Loss: 0.2490 | Test Loss: 0.2394 | F1: 0.7435 | ROC-AUC: 0.9330\n",
      "2025-11-12 23:04:26,639 - INFO - Epoch 6/10 [3.83s] | Train Loss: 0.2398 | Test Loss: 0.2460 | F1: 0.7331 | ROC-AUC: 0.9384\n",
      "2025-11-12 23:04:30,319 - INFO - Epoch 7/10 [3.68s] | Train Loss: 0.2326 | Test Loss: 0.2581 | F1: 0.7001 | ROC-AUC: 0.9257\n",
      "2025-11-12 23:04:34,047 - INFO - Epoch 8/10 [3.73s] | Train Loss: 0.2289 | Test Loss: 0.2550 | F1: 0.7125 | ROC-AUC: 0.9377\n",
      "2025-11-12 23:04:37,804 - INFO - Epoch 9/10 [3.76s] | Train Loss: 0.2219 | Test Loss: 0.2299 | F1: 0.7668 | ROC-AUC: 0.9390\n",
      "2025-11-12 23:04:41,580 - INFO - Epoch 10/10 [3.78s] | Train Loss: 0.2201 | Test Loss: 0.2259 | F1: 0.7727 | ROC-AUC: 0.9443\n",
      "2025-11-12 23:04:41,581 - INFO - Training complete.\n",
      "2025-11-12 23:04:41,585 - INFO - {'loss': 0.226, 'accuracy': 0.917, 'f1': 0.773, 'roc_auc': 0.944, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 42410, 'test_size': 33464, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-12 23:04:41,596 - INFO - Yielding 40% split: 393 accounts, 59801 rows\n",
      "2025-11-12 23:04:42,041 - INFO - len(text_list)=59801 len(unique_texts)=37409 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:43,698 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:44,494 - INFO - Fitting processor on 59801 rows...\n",
      "2025-11-12 23:04:44,495 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:04:44,496 - INFO - Transforming 59801 rows...\n",
      "2025-11-12 23:04:44,497 - INFO - Transform complete.\n",
      "2025-11-12 23:04:44,497 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:04:44,499 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:04:44,552 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:04:49,102 - INFO - Epoch 1/10 [4.55s] | Train Loss: 0.4326 | Test Loss: 0.3931 | F1: 0.3665 | ROC-AUC: 0.8420\n",
      "2025-11-12 23:04:53,774 - INFO - Epoch 2/10 [4.67s] | Train Loss: 0.2958 | Test Loss: 0.2713 | F1: 0.7420 | ROC-AUC: 0.9065\n",
      "2025-11-12 23:04:58,620 - INFO - Epoch 3/10 [4.84s] | Train Loss: 0.2667 | Test Loss: 0.2543 | F1: 0.7486 | ROC-AUC: 0.9231\n",
      "2025-11-12 23:05:03,570 - INFO - Epoch 4/10 [4.95s] | Train Loss: 0.2516 | Test Loss: 0.2458 | F1: 0.7537 | ROC-AUC: 0.9275\n",
      "2025-11-12 23:05:08,314 - INFO - Epoch 5/10 [4.74s] | Train Loss: 0.2435 | Test Loss: 0.2845 | F1: 0.7347 | ROC-AUC: 0.9094\n",
      "2025-11-12 23:05:13,131 - INFO - Epoch 6/10 [4.82s] | Train Loss: 0.2369 | Test Loss: 0.2336 | F1: 0.7699 | ROC-AUC: 0.9345\n",
      "2025-11-12 23:05:17,813 - INFO - Epoch 7/10 [4.68s] | Train Loss: 0.2291 | Test Loss: 0.2563 | F1: 0.7485 | ROC-AUC: 0.9255\n",
      "2025-11-12 23:05:22,407 - INFO - Epoch 8/10 [4.59s] | Train Loss: 0.2277 | Test Loss: 0.2225 | F1: 0.7806 | ROC-AUC: 0.9424\n",
      "2025-11-12 23:05:27,105 - INFO - Epoch 9/10 [4.70s] | Train Loss: 0.2233 | Test Loss: 0.2362 | F1: 0.7782 | ROC-AUC: 0.9400\n",
      "2025-11-12 23:05:32,033 - INFO - Epoch 10/10 [4.93s] | Train Loss: 0.2197 | Test Loss: 0.2244 | F1: 0.7513 | ROC-AUC: 0.9437\n",
      "2025-11-12 23:05:32,034 - INFO - Training complete.\n",
      "2025-11-12 23:05:32,041 - INFO - {'loss': 0.222, 'accuracy': 0.92, 'f1': 0.781, 'roc_auc': 0.942, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 59801, 'test_size': 33464, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-12 23:05:32,062 - INFO - Yielding 50% split: 492 accounts, 75623 rows\n",
      "2025-11-12 23:05:38,224 - INFO - len(text_list)=75623 len(unique_texts)=46835 len(texts_to_embed)=0\n",
      "2025-11-12 23:05:44,988 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:05:45,804 - INFO - Fitting processor on 75623 rows...\n",
      "2025-11-12 23:05:45,805 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:05:45,806 - INFO - Transforming 75623 rows...\n",
      "2025-11-12 23:05:45,807 - INFO - Transform complete.\n",
      "2025-11-12 23:05:45,807 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:05:45,808 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:05:45,861 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:05:51,115 - INFO - Epoch 1/10 [5.25s] | Train Loss: 0.4109 | Test Loss: 0.2848 | F1: 0.6521 | ROC-AUC: 0.9228\n",
      "2025-11-12 23:05:56,273 - INFO - Epoch 2/10 [5.16s] | Train Loss: 0.2780 | Test Loss: 0.2568 | F1: 0.7728 | ROC-AUC: 0.9215\n",
      "2025-11-12 23:06:01,806 - INFO - Epoch 3/10 [5.53s] | Train Loss: 0.2578 | Test Loss: 0.2307 | F1: 0.7689 | ROC-AUC: 0.9372\n",
      "2025-11-12 23:06:07,470 - INFO - Epoch 4/10 [5.66s] | Train Loss: 0.2456 | Test Loss: 0.2719 | F1: 0.7565 | ROC-AUC: 0.9296\n",
      "2025-11-12 23:06:13,066 - INFO - Epoch 5/10 [5.60s] | Train Loss: 0.2331 | Test Loss: 0.2173 | F1: 0.7790 | ROC-AUC: 0.9469\n",
      "2025-11-12 23:06:18,674 - INFO - Epoch 6/10 [5.61s] | Train Loss: 0.2290 | Test Loss: 0.2490 | F1: 0.7756 | ROC-AUC: 0.9269\n",
      "2025-11-12 23:06:24,550 - INFO - Epoch 7/10 [5.88s] | Train Loss: 0.2228 | Test Loss: 0.2804 | F1: 0.6344 | ROC-AUC: 0.9378\n",
      "2025-11-12 23:06:30,265 - INFO - Epoch 8/10 [5.71s] | Train Loss: 0.2197 | Test Loss: 0.2292 | F1: 0.7852 | ROC-AUC: 0.9349\n",
      "2025-11-12 23:06:35,929 - INFO - Epoch 9/10 [5.66s] | Train Loss: 0.2171 | Test Loss: 0.2127 | F1: 0.7905 | ROC-AUC: 0.9459\n",
      "2025-11-12 23:06:41,982 - INFO - Epoch 10/10 [6.05s] | Train Loss: 0.2135 | Test Loss: 0.2176 | F1: 0.7729 | ROC-AUC: 0.9513\n",
      "2025-11-12 23:06:41,983 - INFO - Training complete.\n",
      "2025-11-12 23:06:41,990 - INFO - {'loss': 0.213, 'accuracy': 0.923, 'f1': 0.79, 'roc_auc': 0.946, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.5, 'train_size': 75623, 'test_size': 33464, 'train_accounts': 492, 'test_accounts': 246}\n",
      "2025-11-12 23:06:42,003 - INFO - Yielding 60% split: 590 accounts, 92281 rows\n",
      "2025-11-12 23:06:43,983 - INFO - len(text_list)=92281 len(unique_texts)=56280 len(texts_to_embed)=0\n",
      "2025-11-12 23:06:46,428 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:06:47,082 - INFO - Fitting processor on 92281 rows...\n",
      "2025-11-12 23:06:47,082 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:06:47,083 - INFO - Transforming 92281 rows...\n",
      "2025-11-12 23:06:47,084 - INFO - Transform complete.\n",
      "2025-11-12 23:06:47,084 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:06:47,085 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:06:47,146 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:06:53,103 - INFO - Epoch 1/10 [5.96s] | Train Loss: 0.3903 | Test Loss: 0.3132 | F1: 0.6007 | ROC-AUC: 0.9043\n",
      "2025-11-12 23:06:59,219 - INFO - Epoch 2/10 [6.11s] | Train Loss: 0.2732 | Test Loss: 0.2501 | F1: 0.7334 | ROC-AUC: 0.9321\n",
      "2025-11-12 23:07:06,054 - INFO - Epoch 3/10 [6.83s] | Train Loss: 0.2520 | Test Loss: 0.2468 | F1: 0.7436 | ROC-AUC: 0.9314\n",
      "2025-11-12 23:07:12,663 - INFO - Epoch 4/10 [6.61s] | Train Loss: 0.2383 | Test Loss: 0.2255 | F1: 0.7777 | ROC-AUC: 0.9400\n",
      "2025-11-12 23:07:19,484 - INFO - Epoch 5/10 [6.82s] | Train Loss: 0.2307 | Test Loss: 0.2335 | F1: 0.7436 | ROC-AUC: 0.9449\n",
      "2025-11-12 23:07:26,178 - INFO - Epoch 6/10 [6.69s] | Train Loss: 0.2275 | Test Loss: 0.2238 | F1: 0.7817 | ROC-AUC: 0.9410\n",
      "2025-11-12 23:07:32,843 - INFO - Epoch 7/10 [6.66s] | Train Loss: 0.2239 | Test Loss: 0.2535 | F1: 0.7614 | ROC-AUC: 0.9257\n",
      "2025-11-12 23:07:39,629 - INFO - Epoch 8/10 [6.79s] | Train Loss: 0.2185 | Test Loss: 0.2216 | F1: 0.7895 | ROC-AUC: 0.9435\n",
      "2025-11-12 23:07:46,493 - INFO - Epoch 9/10 [6.86s] | Train Loss: 0.2190 | Test Loss: 0.2264 | F1: 0.7876 | ROC-AUC: 0.9442\n",
      "2025-11-12 23:07:53,347 - INFO - Epoch 10/10 [6.85s] | Train Loss: 0.2176 | Test Loss: 0.2244 | F1: 0.7880 | ROC-AUC: 0.9471\n",
      "2025-11-12 23:07:53,348 - INFO - Training complete.\n",
      "2025-11-12 23:07:53,352 - INFO - {'loss': 0.222, 'accuracy': 0.92, 'f1': 0.789, 'roc_auc': 0.944, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.6, 'train_size': 92281, 'test_size': 33464, 'train_accounts': 590, 'test_accounts': 246}\n",
      "2025-11-12 23:07:53,369 - INFO - Yielding 70% split: 688 accounts, 108248 rows\n",
      "2025-11-12 23:07:55,617 - INFO - len(text_list)=108248 len(unique_texts)=65808 len(texts_to_embed)=0\n",
      "2025-11-12 23:07:58,167 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:07:58,873 - INFO - Fitting processor on 108248 rows...\n",
      "2025-11-12 23:07:58,873 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:07:58,874 - INFO - Transforming 108248 rows...\n",
      "2025-11-12 23:07:58,875 - INFO - Transform complete.\n",
      "2025-11-12 23:07:58,876 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:07:58,876 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:07:58,930 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:08:05,961 - INFO - Epoch 1/10 [7.03s] | Train Loss: 0.3721 | Test Loss: 0.2713 | F1: 0.6816 | ROC-AUC: 0.9313\n",
      "2025-11-12 23:08:13,296 - INFO - Epoch 2/10 [7.33s] | Train Loss: 0.2660 | Test Loss: 0.2425 | F1: 0.7351 | ROC-AUC: 0.9400\n",
      "2025-11-12 23:08:20,981 - INFO - Epoch 3/10 [7.68s] | Train Loss: 0.2447 | Test Loss: 0.2411 | F1: 0.7811 | ROC-AUC: 0.9380\n",
      "2025-11-12 23:08:28,734 - INFO - Epoch 4/10 [7.75s] | Train Loss: 0.2353 | Test Loss: 0.2441 | F1: 0.7750 | ROC-AUC: 0.9336\n",
      "2025-11-12 23:08:36,473 - INFO - Epoch 5/10 [7.74s] | Train Loss: 0.2326 | Test Loss: 0.2413 | F1: 0.7459 | ROC-AUC: 0.9367\n",
      "2025-11-12 23:08:44,290 - INFO - Epoch 6/10 [7.82s] | Train Loss: 0.2253 | Test Loss: 0.2277 | F1: 0.7488 | ROC-AUC: 0.9467\n",
      "2025-11-12 23:08:52,268 - INFO - Epoch 7/10 [7.98s] | Train Loss: 0.2209 | Test Loss: 0.2335 | F1: 0.7512 | ROC-AUC: 0.9424\n",
      "2025-11-12 23:09:00,130 - INFO - Epoch 8/10 [7.86s] | Train Loss: 0.2179 | Test Loss: 0.2166 | F1: 0.7871 | ROC-AUC: 0.9489\n",
      "2025-11-12 23:09:08,070 - INFO - Epoch 9/10 [7.94s] | Train Loss: 0.2150 | Test Loss: 0.2288 | F1: 0.7693 | ROC-AUC: 0.9486\n",
      "2025-11-12 23:09:15,744 - INFO - Epoch 10/10 [7.67s] | Train Loss: 0.2137 | Test Loss: 0.2354 | F1: 0.7860 | ROC-AUC: 0.9440\n",
      "2025-11-12 23:09:15,745 - INFO - Training complete.\n",
      "2025-11-12 23:09:15,752 - INFO - {'loss': 0.217, 'accuracy': 0.92, 'f1': 0.787, 'roc_auc': 0.949, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.7, 'train_size': 108248, 'test_size': 33464, 'train_accounts': 688, 'test_accounts': 246}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e6aef1cf8d361873"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
