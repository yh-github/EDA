{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:51:48.237307Z",
     "start_time": "2025-11-13T08:51:48.232398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "def setup_logging(log_dir:Path):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    log_filename = log_dir/f\"{timestamp}.log\"\n",
    "\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[stdout_handler, file_handler]\n",
    "    )\n",
    "\n",
    "def flush_logger():\n",
    "    stdout_handler.flush()\n",
    "\n",
    "setup_logging(Path('../logs'))\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "e5ac7a92777dcdbf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-13T08:51:49.632389Z",
     "start_time": "2025-11-13T08:51:48.243978Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from config import FieldConfig, ExperimentConfig\n",
    "from pathlib import Path\n",
    "DATA_PATH = Path('C:/Work/Data/proc/')\n",
    "field_config = FieldConfig()\n",
    "\n",
    "df = pd.read_csv(DATA_PATH/'rec_data2.csv')\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "df_cleaned = df.dropna(subset=[field_config.date, field_config.amount, field_config.text, field_config.label])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "id                    0\n",
      "accountId             0\n",
      "date                  0\n",
      "amount                0\n",
      "bankRawDescription    0\n",
      "isRecurring           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:51:49.843680Z",
     "start_time": "2025-11-13T08:51:49.690908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from feature_processor import FeatProcParams\n",
    "from config import *\n",
    "\n",
    "feat_params = FeatProcParams(n_bins=20, k_top=50)\n",
    "feat_params_off = FeatProcParams.NOP()\n",
    "\n",
    "exp_config = ExperimentConfig()\n",
    "\n",
    "fracs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]"
   ],
   "id": "64ad44e449b7478f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:51:51.586552Z",
     "start_time": "2025-11-13T08:51:49.853335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from runner import ExpRunner\n",
    "from config import EmbModel\n",
    "from embedder import EmbeddingService\n",
    "\n",
    "runner1 = ExpRunner.create(\n",
    "    exp_params=exp_config,\n",
    "    full_df=df_cleaned,\n",
    "    emb_params=EmbeddingService.Params(model_name=EmbModel.ALBERT),\n",
    "    feat_proc_params=feat_params,\n",
    "    field_config=FieldConfig()\n",
    ")"
   ],
   "id": "d651a17edece86d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoav.haimovitch\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:58:17.545099Z",
     "start_time": "2025-11-13T08:51:51.610467Z"
    }
   },
   "cell_type": "code",
   "source": "results = runner1.run_torch(fracs)",
   "id": "97b17b877b9520a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 10:51:51,617 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-13 10:51:51,688 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-13 10:51:51,689 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-13 10:51:51,696 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-13 10:51:51,700 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-13 10:51:51,713 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-13 10:51:51,714 - INFO - Creating new EmbeddingService(model_name=albert-base-v2)\n",
      "2025-11-13 10:51:51,715 - INFO - Loading embedding model: albert-base-v2...\n",
      "2025-11-13 10:51:53,367 - INFO - Model albert-base-v2 loaded onto cpu. Cache at cache/albert-base-v2\n",
      "2025-11-13 10:51:53,368 - INFO - Embedding 16704 train texts...\n",
      "2025-11-13 10:51:53,504 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-13 10:51:53,764 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:51:54,032 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:51:54,664 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-13 10:51:54,665 - INFO - Fitting categorical amount features...\n",
      "2025-11-13 10:51:54,671 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:51:54,671 - INFO - Created 20 fallback bins.\n",
      "2025-11-13 10:51:54,672 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-13 10:51:54,672 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:51:54,673 - INFO - Transforming 16704 rows...\n",
      "2025-11-13 10:51:54,731 - INFO - Transform complete.\n",
      "2025-11-13 10:51:54,732 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:51:54,844 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-13 10:51:54,875 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:51:55,908 - INFO - Epoch 1/10 [1.03s] | Train Loss: 0.5377 | Test Loss: 0.3997 | F1: 0.6518 | ROC-AUC: 0.8734\n",
      "2025-11-13 10:51:56,952 - INFO - Epoch 2/10 [1.04s] | Train Loss: 0.3826 | Test Loss: 0.3011 | F1: 0.6838 | ROC-AUC: 0.9044\n",
      "2025-11-13 10:51:57,953 - INFO - Epoch 3/10 [1.00s] | Train Loss: 0.3059 | Test Loss: 0.3104 | F1: 0.5899 | ROC-AUC: 0.9051\n",
      "2025-11-13 10:51:58,941 - INFO - Epoch 4/10 [0.99s] | Train Loss: 0.2715 | Test Loss: 0.3225 | F1: 0.5667 | ROC-AUC: 0.9124\n",
      "2025-11-13 10:52:00,214 - INFO - Epoch 5/10 [1.27s] | Train Loss: 0.2489 | Test Loss: 0.3105 | F1: 0.5576 | ROC-AUC: 0.9241\n",
      "2025-11-13 10:52:01,391 - INFO - Epoch 6/10 [1.18s] | Train Loss: 0.2400 | Test Loss: 0.3423 | F1: 0.5638 | ROC-AUC: 0.8724\n",
      "2025-11-13 10:52:02,504 - INFO - Epoch 7/10 [1.11s] | Train Loss: 0.2263 | Test Loss: 0.3031 | F1: 0.6575 | ROC-AUC: 0.9106\n",
      "2025-11-13 10:52:03,717 - INFO - Epoch 8/10 [1.21s] | Train Loss: 0.2220 | Test Loss: 0.3622 | F1: 0.4462 | ROC-AUC: 0.9115\n",
      "2025-11-13 10:52:04,887 - INFO - Epoch 9/10 [1.17s] | Train Loss: 0.2142 | Test Loss: 0.3092 | F1: 0.5703 | ROC-AUC: 0.9275\n",
      "2025-11-13 10:52:06,013 - INFO - Epoch 10/10 [1.13s] | Train Loss: 0.2071 | Test Loss: 0.2622 | F1: 0.7149 | ROC-AUC: 0.9257\n",
      "2025-11-13 10:52:06,014 - INFO - Training complete.\n",
      "2025-11-13 10:52:06,017 - INFO - {'loss': 0.262, 'accuracy': 0.902, 'f1': 0.715, 'roc_auc': 0.926, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-13 10:52:06,024 - INFO - Yielding 20% split: 196 accounts, 29736 rows\n",
      "2025-11-13 10:52:06,025 - INFO - Embedding 29736 train texts...\n",
      "2025-11-13 10:52:06,512 - INFO - len(text_list)=29736 len(unique_texts)=19347 len(texts_to_embed)=0\n",
      "2025-11-13 10:52:06,978 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:52:07,243 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:52:07,909 - INFO - Fitting processor on 29736 rows...\n",
      "2025-11-13 10:52:07,909 - INFO - Fitting categorical amount features...\n",
      "2025-11-13 10:52:07,918 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:52:07,919 - INFO - Created 20 fallback bins.\n",
      "2025-11-13 10:52:07,920 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-13 10:52:07,920 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:52:07,921 - INFO - Transforming 29736 rows...\n",
      "2025-11-13 10:52:08,021 - INFO - Transform complete.\n",
      "2025-11-13 10:52:08,022 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:52:08,130 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-13 10:52:08,164 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:52:09,808 - INFO - Epoch 1/10 [1.64s] | Train Loss: 0.4976 | Test Loss: 0.3455 | F1: 0.6329 | ROC-AUC: 0.8531\n",
      "2025-11-13 10:52:11,279 - INFO - Epoch 2/10 [1.47s] | Train Loss: 0.3158 | Test Loss: 0.2901 | F1: 0.7139 | ROC-AUC: 0.9119\n",
      "2025-11-13 10:52:12,871 - INFO - Epoch 3/10 [1.59s] | Train Loss: 0.2709 | Test Loss: 0.2665 | F1: 0.7078 | ROC-AUC: 0.9202\n",
      "2025-11-13 10:52:14,293 - INFO - Epoch 4/10 [1.42s] | Train Loss: 0.2500 | Test Loss: 0.2925 | F1: 0.5827 | ROC-AUC: 0.9290\n",
      "2025-11-13 10:52:15,818 - INFO - Epoch 5/10 [1.52s] | Train Loss: 0.2330 | Test Loss: 0.3143 | F1: 0.7249 | ROC-AUC: 0.9137\n",
      "2025-11-13 10:52:17,417 - INFO - Epoch 6/10 [1.60s] | Train Loss: 0.2237 | Test Loss: 0.2475 | F1: 0.7185 | ROC-AUC: 0.9316\n",
      "2025-11-13 10:52:19,185 - INFO - Epoch 7/10 [1.77s] | Train Loss: 0.2163 | Test Loss: 0.2440 | F1: 0.7372 | ROC-AUC: 0.9372\n",
      "2025-11-13 10:52:20,933 - INFO - Epoch 8/10 [1.75s] | Train Loss: 0.2117 | Test Loss: 0.3328 | F1: 0.5090 | ROC-AUC: 0.9074\n",
      "2025-11-13 10:52:22,590 - INFO - Epoch 9/10 [1.66s] | Train Loss: 0.2104 | Test Loss: 0.2408 | F1: 0.7559 | ROC-AUC: 0.9336\n",
      "2025-11-13 10:52:24,348 - INFO - Epoch 10/10 [1.76s] | Train Loss: 0.2008 | Test Loss: 0.2404 | F1: 0.7819 | ROC-AUC: 0.9411\n",
      "2025-11-13 10:52:24,349 - INFO - Training complete.\n",
      "2025-11-13 10:52:24,352 - INFO - {'loss': 0.24, 'accuracy': 0.913, 'f1': 0.782, 'roc_auc': 0.941, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 29736, 'test_size': 33464, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-13 10:52:24,362 - INFO - Yielding 30% split: 295 accounts, 42410 rows\n",
      "2025-11-13 10:52:24,365 - INFO - Embedding 42410 train texts...\n",
      "2025-11-13 10:52:25,202 - INFO - len(text_list)=42410 len(unique_texts)=26700 len(texts_to_embed)=0\n",
      "2025-11-13 10:52:26,057 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:52:26,341 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:52:26,993 - INFO - Fitting processor on 42410 rows...\n",
      "2025-11-13 10:52:26,994 - INFO - Fitting categorical amount features...\n",
      "2025-11-13 10:52:27,005 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:52:27,006 - INFO - Created 20 fallback bins.\n",
      "2025-11-13 10:52:27,007 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-13 10:52:27,007 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:52:27,008 - INFO - Transforming 42410 rows...\n",
      "2025-11-13 10:52:27,168 - INFO - Transform complete.\n",
      "2025-11-13 10:52:27,169 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:52:27,300 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-13 10:52:27,342 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:52:29,662 - INFO - Epoch 1/10 [2.32s] | Train Loss: 0.4720 | Test Loss: 0.3122 | F1: 0.6624 | ROC-AUC: 0.8894\n",
      "2025-11-13 10:52:31,669 - INFO - Epoch 2/10 [2.01s] | Train Loss: 0.3024 | Test Loss: 0.2972 | F1: 0.5841 | ROC-AUC: 0.9255\n",
      "2025-11-13 10:52:33,741 - INFO - Epoch 3/10 [2.07s] | Train Loss: 0.2646 | Test Loss: 0.2650 | F1: 0.7323 | ROC-AUC: 0.9208\n",
      "2025-11-13 10:52:35,956 - INFO - Epoch 4/10 [2.21s] | Train Loss: 0.2468 | Test Loss: 0.2490 | F1: 0.7431 | ROC-AUC: 0.9309\n",
      "2025-11-13 10:52:38,171 - INFO - Epoch 5/10 [2.21s] | Train Loss: 0.2367 | Test Loss: 0.2311 | F1: 0.7509 | ROC-AUC: 0.9399\n",
      "2025-11-13 10:52:40,395 - INFO - Epoch 6/10 [2.22s] | Train Loss: 0.2276 | Test Loss: 0.2345 | F1: 0.7444 | ROC-AUC: 0.9395\n",
      "2025-11-13 10:52:42,431 - INFO - Epoch 7/10 [2.04s] | Train Loss: 0.2212 | Test Loss: 0.2272 | F1: 0.7805 | ROC-AUC: 0.9403\n",
      "2025-11-13 10:52:44,801 - INFO - Epoch 8/10 [2.37s] | Train Loss: 0.2164 | Test Loss: 0.2778 | F1: 0.6596 | ROC-AUC: 0.9353\n",
      "2025-11-13 10:52:47,041 - INFO - Epoch 9/10 [2.24s] | Train Loss: 0.2088 | Test Loss: 0.2218 | F1: 0.7627 | ROC-AUC: 0.9456\n",
      "2025-11-13 10:52:49,250 - INFO - Epoch 10/10 [2.21s] | Train Loss: 0.2072 | Test Loss: 0.2293 | F1: 0.7476 | ROC-AUC: 0.9413\n",
      "2025-11-13 10:52:49,250 - INFO - Training complete.\n",
      "2025-11-13 10:52:49,254 - INFO - {'loss': 0.227, 'accuracy': 0.917, 'f1': 0.781, 'roc_auc': 0.94, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 42410, 'test_size': 33464, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-13 10:52:49,269 - INFO - Yielding 40% split: 393 accounts, 59801 rows\n",
      "2025-11-13 10:52:49,271 - INFO - Embedding 59801 train texts...\n",
      "2025-11-13 10:52:51,821 - INFO - len(text_list)=59801 len(unique_texts)=37409 len(texts_to_embed)=0\n",
      "2025-11-13 10:52:53,743 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:52:54,078 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:52:54,981 - INFO - Fitting processor on 59801 rows...\n",
      "2025-11-13 10:52:54,982 - INFO - Fitting categorical amount features...\n",
      "2025-11-13 10:52:54,994 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:52:54,995 - INFO - Created 20 fallback bins.\n",
      "2025-11-13 10:52:54,995 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-13 10:52:54,996 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:52:54,997 - INFO - Transforming 59801 rows...\n",
      "2025-11-13 10:52:55,373 - INFO - Transform complete.\n",
      "2025-11-13 10:52:55,375 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:52:55,613 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-13 10:52:55,666 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:52:59,819 - INFO - Epoch 1/10 [4.15s] | Train Loss: 0.4246 | Test Loss: 0.2757 | F1: 0.7018 | ROC-AUC: 0.9175\n",
      "2025-11-13 10:53:03,450 - INFO - Epoch 2/10 [3.63s] | Train Loss: 0.2832 | Test Loss: 0.2653 | F1: 0.7418 | ROC-AUC: 0.9265\n",
      "2025-11-13 10:53:07,990 - INFO - Epoch 3/10 [4.54s] | Train Loss: 0.2571 | Test Loss: 0.2579 | F1: 0.6874 | ROC-AUC: 0.9350\n",
      "2025-11-13 10:53:11,659 - INFO - Epoch 4/10 [3.67s] | Train Loss: 0.2450 | Test Loss: 0.2284 | F1: 0.7443 | ROC-AUC: 0.9431\n",
      "2025-11-13 10:53:15,276 - INFO - Epoch 5/10 [3.62s] | Train Loss: 0.2335 | Test Loss: 0.2724 | F1: 0.6751 | ROC-AUC: 0.9331\n",
      "2025-11-13 10:53:19,730 - INFO - Epoch 6/10 [4.45s] | Train Loss: 0.2254 | Test Loss: 0.2360 | F1: 0.7380 | ROC-AUC: 0.9412\n",
      "2025-11-13 10:53:24,214 - INFO - Epoch 7/10 [4.48s] | Train Loss: 0.2198 | Test Loss: 0.2212 | F1: 0.7729 | ROC-AUC: 0.9434\n",
      "2025-11-13 10:53:29,097 - INFO - Epoch 8/10 [4.88s] | Train Loss: 0.2162 | Test Loss: 0.2269 | F1: 0.7744 | ROC-AUC: 0.9400\n",
      "2025-11-13 10:53:34,272 - INFO - Epoch 9/10 [5.17s] | Train Loss: 0.2126 | Test Loss: 0.2560 | F1: 0.7197 | ROC-AUC: 0.9458\n",
      "2025-11-13 10:53:39,538 - INFO - Epoch 10/10 [5.26s] | Train Loss: 0.2121 | Test Loss: 0.2379 | F1: 0.7238 | ROC-AUC: 0.9403\n",
      "2025-11-13 10:53:39,539 - INFO - Training complete.\n",
      "2025-11-13 10:53:39,543 - INFO - {'loss': 0.227, 'accuracy': 0.917, 'f1': 0.774, 'roc_auc': 0.94, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 59801, 'test_size': 33464, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-13 10:53:39,558 - INFO - Yielding 50% split: 492 accounts, 75623 rows\n",
      "2025-11-13 10:53:39,561 - INFO - Embedding 75623 train texts...\n",
      "2025-11-13 10:53:41,564 - INFO - len(text_list)=75623 len(unique_texts)=46835 len(texts_to_embed)=0\n",
      "2025-11-13 10:53:43,052 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:53:43,356 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:53:44,114 - INFO - Fitting processor on 75623 rows...\n",
      "2025-11-13 10:53:44,115 - INFO - Fitting categorical amount features...\n",
      "2025-11-13 10:53:44,130 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:53:44,131 - INFO - Created 20 fallback bins.\n",
      "2025-11-13 10:53:44,131 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-13 10:53:44,132 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:53:44,133 - INFO - Transforming 75623 rows...\n",
      "2025-11-13 10:53:44,385 - INFO - Transform complete.\n",
      "2025-11-13 10:53:44,385 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:53:44,510 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-13 10:53:44,564 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:53:48,935 - INFO - Epoch 1/10 [4.37s] | Train Loss: 0.4005 | Test Loss: 0.2887 | F1: 0.7135 | ROC-AUC: 0.9099\n",
      "2025-11-13 10:53:53,626 - INFO - Epoch 2/10 [4.69s] | Train Loss: 0.2692 | Test Loss: 0.2697 | F1: 0.7574 | ROC-AUC: 0.9242\n",
      "2025-11-13 10:53:59,037 - INFO - Epoch 3/10 [5.41s] | Train Loss: 0.2474 | Test Loss: 0.2426 | F1: 0.7265 | ROC-AUC: 0.9428\n",
      "2025-11-13 10:54:04,102 - INFO - Epoch 4/10 [5.06s] | Train Loss: 0.2339 | Test Loss: 0.2394 | F1: 0.7513 | ROC-AUC: 0.9337\n",
      "2025-11-13 10:54:09,242 - INFO - Epoch 5/10 [5.14s] | Train Loss: 0.2255 | Test Loss: 0.2340 | F1: 0.7716 | ROC-AUC: 0.9376\n",
      "2025-11-13 10:54:13,961 - INFO - Epoch 6/10 [4.72s] | Train Loss: 0.2186 | Test Loss: 0.2128 | F1: 0.7756 | ROC-AUC: 0.9499\n",
      "2025-11-13 10:54:19,061 - INFO - Epoch 7/10 [5.10s] | Train Loss: 0.2137 | Test Loss: 0.2097 | F1: 0.7949 | ROC-AUC: 0.9501\n",
      "2025-11-13 10:54:35,899 - INFO - Epoch 8/10 [16.84s] | Train Loss: 0.2105 | Test Loss: 0.2373 | F1: 0.7274 | ROC-AUC: 0.9486\n",
      "2025-11-13 10:54:47,133 - INFO - Epoch 9/10 [11.23s] | Train Loss: 0.2076 | Test Loss: 0.2052 | F1: 0.7887 | ROC-AUC: 0.9530\n",
      "2025-11-13 10:55:02,620 - INFO - Epoch 10/10 [15.48s] | Train Loss: 0.2054 | Test Loss: 0.2104 | F1: 0.7779 | ROC-AUC: 0.9513\n",
      "2025-11-13 10:55:02,622 - INFO - Training complete.\n",
      "2025-11-13 10:55:02,648 - INFO - {'loss': 0.21, 'accuracy': 0.923, 'f1': 0.795, 'roc_auc': 0.95, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.5, 'train_size': 75623, 'test_size': 33464, 'train_accounts': 492, 'test_accounts': 246}\n",
      "2025-11-13 10:55:02,697 - INFO - Yielding 60% split: 590 accounts, 92281 rows\n",
      "2025-11-13 10:55:02,716 - INFO - Embedding 92281 train texts...\n",
      "2025-11-13 10:55:08,733 - INFO - len(text_list)=92281 len(unique_texts)=56280 len(texts_to_embed)=0\n",
      "2025-11-13 10:55:10,704 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:55:11,002 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:55:11,674 - INFO - Fitting processor on 92281 rows...\n",
      "2025-11-13 10:55:11,675 - INFO - Fitting categorical amount features...\n",
      "2025-11-13 10:55:11,694 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:55:11,695 - INFO - Created 20 fallback bins.\n",
      "2025-11-13 10:55:11,695 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-13 10:55:11,696 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:55:11,697 - INFO - Transforming 92281 rows...\n",
      "2025-11-13 10:55:12,091 - INFO - Transform complete.\n",
      "2025-11-13 10:55:12,092 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:55:12,239 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-13 10:55:12,304 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:55:20,325 - INFO - Epoch 1/10 [8.02s] | Train Loss: 0.3822 | Test Loss: 0.2588 | F1: 0.7008 | ROC-AUC: 0.9285\n",
      "2025-11-13 10:55:32,901 - INFO - Epoch 2/10 [12.57s] | Train Loss: 0.2648 | Test Loss: 0.2448 | F1: 0.7195 | ROC-AUC: 0.9416\n",
      "2025-11-13 10:55:48,781 - INFO - Epoch 3/10 [15.88s] | Train Loss: 0.2393 | Test Loss: 0.2260 | F1: 0.7693 | ROC-AUC: 0.9415\n",
      "2025-11-13 10:56:07,551 - INFO - Epoch 4/10 [18.77s] | Train Loss: 0.2310 | Test Loss: 0.2216 | F1: 0.7609 | ROC-AUC: 0.9473\n",
      "2025-11-13 10:56:18,351 - INFO - Epoch 5/10 [10.80s] | Train Loss: 0.2229 | Test Loss: 0.2822 | F1: 0.6314 | ROC-AUC: 0.9350\n",
      "2025-11-13 10:56:27,341 - INFO - Epoch 6/10 [8.99s] | Train Loss: 0.2195 | Test Loss: 0.2173 | F1: 0.7774 | ROC-AUC: 0.9495\n",
      "2025-11-13 10:56:36,475 - INFO - Epoch 7/10 [9.13s] | Train Loss: 0.2142 | Test Loss: 0.2157 | F1: 0.7682 | ROC-AUC: 0.9499\n",
      "2025-11-13 10:56:45,113 - INFO - Epoch 8/10 [8.64s] | Train Loss: 0.2090 | Test Loss: 0.1999 | F1: 0.8025 | ROC-AUC: 0.9559\n",
      "2025-11-13 10:56:54,059 - INFO - Epoch 9/10 [8.94s] | Train Loss: 0.2067 | Test Loss: 0.2086 | F1: 0.7814 | ROC-AUC: 0.9529\n",
      "2025-11-13 10:57:01,704 - INFO - Epoch 10/10 [7.64s] | Train Loss: 0.2036 | Test Loss: 0.2049 | F1: 0.7894 | ROC-AUC: 0.9528\n",
      "2025-11-13 10:57:01,705 - INFO - Training complete.\n",
      "2025-11-13 10:57:01,710 - INFO - {'loss': 0.2, 'accuracy': 0.926, 'f1': 0.803, 'roc_auc': 0.956, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.6, 'train_size': 92281, 'test_size': 33464, 'train_accounts': 590, 'test_accounts': 246}\n",
      "2025-11-13 10:57:01,725 - INFO - Yielding 70% split: 688 accounts, 108248 rows\n",
      "2025-11-13 10:57:01,729 - INFO - Embedding 108248 train texts...\n",
      "2025-11-13 10:57:04,083 - INFO - len(text_list)=108248 len(unique_texts)=65808 len(texts_to_embed)=0\n",
      "2025-11-13 10:57:06,911 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:57:07,214 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:57:08,074 - INFO - Fitting processor on 108248 rows...\n",
      "2025-11-13 10:57:08,075 - INFO - Fitting categorical amount features...\n",
      "2025-11-13 10:57:08,094 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:57:08,095 - INFO - Created 20 fallback bins.\n",
      "2025-11-13 10:57:08,095 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-13 10:57:08,096 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-13 10:57:08,097 - INFO - Transforming 108248 rows...\n",
      "2025-11-13 10:57:08,556 - INFO - Transform complete.\n",
      "2025-11-13 10:57:08,556 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:57:08,750 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-13 10:57:08,828 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:57:16,001 - INFO - Epoch 1/10 [7.17s] | Train Loss: 0.3619 | Test Loss: 0.2553 | F1: 0.6938 | ROC-AUC: 0.9334\n",
      "2025-11-13 10:57:22,696 - INFO - Epoch 2/10 [6.69s] | Train Loss: 0.2565 | Test Loss: 0.2278 | F1: 0.7695 | ROC-AUC: 0.9431\n",
      "2025-11-13 10:57:29,625 - INFO - Epoch 3/10 [6.93s] | Train Loss: 0.2371 | Test Loss: 0.2279 | F1: 0.7791 | ROC-AUC: 0.9438\n",
      "2025-11-13 10:57:37,138 - INFO - Epoch 4/10 [7.51s] | Train Loss: 0.2251 | Test Loss: 0.2165 | F1: 0.7798 | ROC-AUC: 0.9483\n",
      "2025-11-13 10:57:44,798 - INFO - Epoch 5/10 [7.66s] | Train Loss: 0.2185 | Test Loss: 0.2384 | F1: 0.7201 | ROC-AUC: 0.9448\n",
      "2025-11-13 10:57:52,592 - INFO - Epoch 6/10 [7.79s] | Train Loss: 0.2141 | Test Loss: 0.2232 | F1: 0.7861 | ROC-AUC: 0.9487\n",
      "2025-11-13 10:57:58,988 - INFO - Epoch 7/10 [6.39s] | Train Loss: 0.2079 | Test Loss: 0.2029 | F1: 0.8033 | ROC-AUC: 0.9551\n",
      "2025-11-13 10:58:04,893 - INFO - Epoch 8/10 [5.90s] | Train Loss: 0.2057 | Test Loss: 0.2166 | F1: 0.7922 | ROC-AUC: 0.9485\n",
      "2025-11-13 10:58:11,903 - INFO - Epoch 9/10 [7.01s] | Train Loss: 0.2032 | Test Loss: 0.2064 | F1: 0.7955 | ROC-AUC: 0.9530\n",
      "2025-11-13 10:58:17,507 - INFO - Epoch 10/10 [5.60s] | Train Loss: 0.2013 | Test Loss: 0.2013 | F1: 0.7930 | ROC-AUC: 0.9553\n",
      "2025-11-13 10:58:17,508 - INFO - Training complete.\n",
      "2025-11-13 10:58:17,514 - INFO - {'loss': 0.203, 'accuracy': 0.923, 'f1': 0.803, 'roc_auc': 0.955, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.7, 'train_size': 108248, 'test_size': 33464, 'train_accounts': 688, 'test_accounts': 246}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:58:17.585780Z",
     "start_time": "2025-11-13T08:58:17.580274Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "992cb940d94cdeb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16704: {'loss': 0.262,\n",
       "  'accuracy': 0.902,\n",
       "  'f1': 0.715,\n",
       "  'roc_auc': 0.926,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.1,\n",
       "  'train_size': 16704,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 98,\n",
       "  'test_accounts': 246},\n",
       " 29736: {'loss': 0.24,\n",
       "  'accuracy': 0.913,\n",
       "  'f1': 0.782,\n",
       "  'roc_auc': 0.941,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.2,\n",
       "  'train_size': 29736,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 196,\n",
       "  'test_accounts': 246},\n",
       " 42410: {'loss': 0.227,\n",
       "  'accuracy': 0.917,\n",
       "  'f1': 0.781,\n",
       "  'roc_auc': 0.94,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.3,\n",
       "  'train_size': 42410,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 295,\n",
       "  'test_accounts': 246},\n",
       " 59801: {'loss': 0.227,\n",
       "  'accuracy': 0.917,\n",
       "  'f1': 0.774,\n",
       "  'roc_auc': 0.94,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.4,\n",
       "  'train_size': 59801,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 393,\n",
       "  'test_accounts': 246},\n",
       " 75623: {'loss': 0.21,\n",
       "  'accuracy': 0.923,\n",
       "  'f1': 0.795,\n",
       "  'roc_auc': 0.95,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.5,\n",
       "  'train_size': 75623,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 492,\n",
       "  'test_accounts': 246},\n",
       " 92281: {'loss': 0.2,\n",
       "  'accuracy': 0.926,\n",
       "  'f1': 0.803,\n",
       "  'roc_auc': 0.956,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.6,\n",
       "  'train_size': 92281,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 590,\n",
       "  'test_accounts': 246},\n",
       " 108248: {'loss': 0.203,\n",
       "  'accuracy': 0.923,\n",
       "  'f1': 0.803,\n",
       "  'roc_auc': 0.955,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.7,\n",
       "  'train_size': 108248,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 688,\n",
       "  'test_accounts': 246}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-13T08:58:17.591904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "runner2 = ExpRunner.copy(runner1)\n",
    "runner2.feat_proc_params = feat_params_off\n",
    "results2 = runner2.run_torch(fracs)\n"
   ],
   "id": "96f96304f4d9f7cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 10:58:17,599 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-13 10:58:17,664 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-13 10:58:17,665 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-13 10:58:17,671 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-13 10:58:17,674 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-13 10:58:17,684 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-13 10:58:17,685 - INFO - Creating new EmbeddingService(model_name=albert-base-v2)\n",
      "2025-11-13 10:58:17,685 - INFO - Loading embedding model: albert-base-v2...\n",
      "2025-11-13 10:58:19,116 - INFO - Model albert-base-v2 loaded onto cpu. Cache at cache/albert-base-v2\n",
      "2025-11-13 10:58:19,117 - INFO - Embedding 16704 train texts...\n",
      "2025-11-13 10:58:19,231 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:19,464 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:58:19,711 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:20,265 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-13 10:58:20,266 - INFO - Transforming 16704 rows...\n",
      "2025-11-13 10:58:20,267 - INFO - Transform complete.\n",
      "2025-11-13 10:58:20,268 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:58:20,270 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 10:58:20,289 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:58:21,329 - INFO - Epoch 1/10 [1.04s] | Train Loss: 0.5519 | Test Loss: 0.7776 | F1: 0.4835 | ROC-AUC: 0.8445\n",
      "2025-11-13 10:58:22,333 - INFO - Epoch 2/10 [1.00s] | Train Loss: 0.3910 | Test Loss: 0.3685 | F1: 0.7084 | ROC-AUC: 0.8962\n",
      "2025-11-13 10:58:23,330 - INFO - Epoch 3/10 [1.00s] | Train Loss: 0.3041 | Test Loss: 0.4789 | F1: 0.6200 | ROC-AUC: 0.8811\n",
      "2025-11-13 10:58:24,299 - INFO - Epoch 4/10 [0.97s] | Train Loss: 0.2751 | Test Loss: 0.3085 | F1: 0.4591 | ROC-AUC: 0.9203\n",
      "2025-11-13 10:58:25,258 - INFO - Epoch 5/10 [0.96s] | Train Loss: 0.2601 | Test Loss: 0.2836 | F1: 0.7429 | ROC-AUC: 0.9081\n",
      "2025-11-13 10:58:26,296 - INFO - Epoch 6/10 [1.04s] | Train Loss: 0.2535 | Test Loss: 0.3462 | F1: 0.7172 | ROC-AUC: 0.9163\n",
      "2025-11-13 10:58:27,580 - INFO - Epoch 7/10 [1.28s] | Train Loss: 0.2381 | Test Loss: 0.3469 | F1: 0.6958 | ROC-AUC: 0.9082\n",
      "2025-11-13 10:58:28,752 - INFO - Epoch 8/10 [1.17s] | Train Loss: 0.2303 | Test Loss: 0.3447 | F1: 0.4125 | ROC-AUC: 0.9164\n",
      "2025-11-13 10:58:29,758 - INFO - Epoch 9/10 [1.01s] | Train Loss: 0.2273 | Test Loss: 0.2650 | F1: 0.7325 | ROC-AUC: 0.9246\n",
      "2025-11-13 10:58:30,771 - INFO - Epoch 10/10 [1.01s] | Train Loss: 0.2268 | Test Loss: 0.2573 | F1: 0.7564 | ROC-AUC: 0.9283\n",
      "2025-11-13 10:58:30,771 - INFO - Training complete.\n",
      "2025-11-13 10:58:30,777 - INFO - {'loss': 0.257, 'accuracy': 0.906, 'f1': 0.756, 'roc_auc': 0.928, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-13 10:58:30,785 - INFO - Yielding 20% split: 196 accounts, 29736 rows\n",
      "2025-11-13 10:58:30,786 - INFO - Embedding 29736 train texts...\n",
      "2025-11-13 10:58:30,949 - INFO - len(text_list)=29736 len(unique_texts)=19347 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:31,330 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:58:31,566 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:32,133 - INFO - Fitting processor on 29736 rows...\n",
      "2025-11-13 10:58:32,134 - INFO - Transforming 29736 rows...\n",
      "2025-11-13 10:58:32,135 - INFO - Transform complete.\n",
      "2025-11-13 10:58:32,136 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:58:32,136 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 10:58:32,160 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:58:33,713 - INFO - Epoch 1/10 [1.55s] | Train Loss: 0.4994 | Test Loss: 0.4231 | F1: 0.6875 | ROC-AUC: 0.8898\n",
      "2025-11-13 10:58:35,197 - INFO - Epoch 2/10 [1.48s] | Train Loss: 0.3209 | Test Loss: 0.3152 | F1: 0.6954 | ROC-AUC: 0.8854\n",
      "2025-11-13 10:58:36,882 - INFO - Epoch 3/10 [1.68s] | Train Loss: 0.2751 | Test Loss: 0.2986 | F1: 0.5519 | ROC-AUC: 0.9150\n",
      "2025-11-13 10:58:38,399 - INFO - Epoch 4/10 [1.52s] | Train Loss: 0.2553 | Test Loss: 0.2661 | F1: 0.7161 | ROC-AUC: 0.9180\n",
      "2025-11-13 10:58:39,881 - INFO - Epoch 5/10 [1.48s] | Train Loss: 0.2420 | Test Loss: 0.3732 | F1: 0.4033 | ROC-AUC: 0.9143\n",
      "2025-11-13 10:58:41,366 - INFO - Epoch 6/10 [1.48s] | Train Loss: 0.2310 | Test Loss: 0.2657 | F1: 0.6913 | ROC-AUC: 0.9176\n",
      "2025-11-13 10:58:42,913 - INFO - Epoch 7/10 [1.55s] | Train Loss: 0.2249 | Test Loss: 0.2583 | F1: 0.7319 | ROC-AUC: 0.9331\n",
      "2025-11-13 10:58:44,574 - INFO - Epoch 8/10 [1.66s] | Train Loss: 0.2238 | Test Loss: 0.4175 | F1: 0.4920 | ROC-AUC: 0.8932\n",
      "2025-11-13 10:58:46,162 - INFO - Epoch 9/10 [1.59s] | Train Loss: 0.2206 | Test Loss: 0.2711 | F1: 0.6887 | ROC-AUC: 0.9176\n",
      "2025-11-13 10:58:47,817 - INFO - Epoch 10/10 [1.65s] | Train Loss: 0.2074 | Test Loss: 0.2422 | F1: 0.7592 | ROC-AUC: 0.9336\n",
      "2025-11-13 10:58:47,817 - INFO - Training complete.\n",
      "2025-11-13 10:58:47,820 - INFO - {'loss': 0.242, 'accuracy': 0.909, 'f1': 0.759, 'roc_auc': 0.934, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 29736, 'test_size': 33464, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-13 10:58:47,828 - INFO - Yielding 30% split: 295 accounts, 42410 rows\n",
      "2025-11-13 10:58:47,829 - INFO - Embedding 42410 train texts...\n",
      "2025-11-13 10:58:48,066 - INFO - len(text_list)=42410 len(unique_texts)=26700 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:48,783 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:58:49,032 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:49,617 - INFO - Fitting processor on 42410 rows...\n",
      "2025-11-13 10:58:49,618 - INFO - Transforming 42410 rows...\n",
      "2025-11-13 10:58:49,619 - INFO - Transform complete.\n",
      "2025-11-13 10:58:49,620 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:58:49,621 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 10:58:49,657 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:58:51,726 - INFO - Epoch 1/10 [2.07s] | Train Loss: 0.4718 | Test Loss: 0.3596 | F1: 0.6551 | ROC-AUC: 0.8748\n",
      "2025-11-13 10:58:53,660 - INFO - Epoch 2/10 [1.93s] | Train Loss: 0.3049 | Test Loss: 0.3403 | F1: 0.7034 | ROC-AUC: 0.8995\n",
      "2025-11-13 10:58:55,473 - INFO - Epoch 3/10 [1.81s] | Train Loss: 0.2757 | Test Loss: 0.2483 | F1: 0.7505 | ROC-AUC: 0.9281\n",
      "2025-11-13 10:58:57,277 - INFO - Epoch 4/10 [1.80s] | Train Loss: 0.2567 | Test Loss: 0.2492 | F1: 0.7466 | ROC-AUC: 0.9265\n",
      "2025-11-13 10:58:59,688 - INFO - Epoch 5/10 [2.41s] | Train Loss: 0.2455 | Test Loss: 0.2340 | F1: 0.7635 | ROC-AUC: 0.9359\n",
      "2025-11-13 10:59:02,026 - INFO - Epoch 6/10 [2.34s] | Train Loss: 0.2376 | Test Loss: 0.2447 | F1: 0.7266 | ROC-AUC: 0.9357\n",
      "2025-11-13 10:59:04,117 - INFO - Epoch 7/10 [2.09s] | Train Loss: 0.2322 | Test Loss: 0.2953 | F1: 0.7274 | ROC-AUC: 0.8989\n",
      "2025-11-13 10:59:06,510 - INFO - Epoch 8/10 [2.39s] | Train Loss: 0.2243 | Test Loss: 0.3228 | F1: 0.5202 | ROC-AUC: 0.9249\n",
      "2025-11-13 10:59:08,543 - INFO - Epoch 9/10 [2.03s] | Train Loss: 0.2243 | Test Loss: 0.2387 | F1: 0.7597 | ROC-AUC: 0.9323\n",
      "2025-11-13 10:59:10,789 - INFO - Epoch 10/10 [2.25s] | Train Loss: 0.2187 | Test Loss: 0.2396 | F1: 0.7624 | ROC-AUC: 0.9365\n",
      "2025-11-13 10:59:10,790 - INFO - Training complete.\n",
      "2025-11-13 10:59:10,793 - INFO - {'loss': 0.234, 'accuracy': 0.914, 'f1': 0.764, 'roc_auc': 0.936, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 42410, 'test_size': 33464, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-13 10:59:10,804 - INFO - Yielding 40% split: 393 accounts, 59801 rows\n",
      "2025-11-13 10:59:10,805 - INFO - Embedding 59801 train texts...\n",
      "2025-11-13 10:59:11,198 - INFO - len(text_list)=59801 len(unique_texts)=37409 len(texts_to_embed)=0\n",
      "2025-11-13 10:59:12,188 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:59:12,434 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:59:13,099 - INFO - Fitting processor on 59801 rows...\n",
      "2025-11-13 10:59:13,100 - INFO - Transforming 59801 rows...\n",
      "2025-11-13 10:59:13,101 - INFO - Transform complete.\n",
      "2025-11-13 10:59:13,101 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:59:13,102 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 10:59:13,136 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:59:15,508 - INFO - Epoch 1/10 [2.37s] | Train Loss: 0.4281 | Test Loss: 0.2996 | F1: 0.5646 | ROC-AUC: 0.9043\n",
      "2025-11-13 10:59:17,930 - INFO - Epoch 2/10 [2.42s] | Train Loss: 0.2929 | Test Loss: 0.3358 | F1: 0.5530 | ROC-AUC: 0.9075\n",
      "2025-11-13 10:59:20,321 - INFO - Epoch 3/10 [2.39s] | Train Loss: 0.2660 | Test Loss: 0.2701 | F1: 0.6958 | ROC-AUC: 0.9238\n",
      "2025-11-13 10:59:23,136 - INFO - Epoch 4/10 [2.81s] | Train Loss: 0.2537 | Test Loss: 0.2823 | F1: 0.7408 | ROC-AUC: 0.9200\n",
      "2025-11-13 10:59:25,923 - INFO - Epoch 5/10 [2.79s] | Train Loss: 0.2446 | Test Loss: 0.2437 | F1: 0.7255 | ROC-AUC: 0.9357\n",
      "2025-11-13 10:59:29,316 - INFO - Epoch 6/10 [3.39s] | Train Loss: 0.2355 | Test Loss: 0.2316 | F1: 0.7697 | ROC-AUC: 0.9387\n",
      "2025-11-13 10:59:33,570 - INFO - Epoch 7/10 [4.25s] | Train Loss: 0.2284 | Test Loss: 0.2400 | F1: 0.7272 | ROC-AUC: 0.9399\n",
      "2025-11-13 10:59:36,547 - INFO - Epoch 8/10 [2.98s] | Train Loss: 0.2241 | Test Loss: 0.2481 | F1: 0.6922 | ROC-AUC: 0.9401\n",
      "2025-11-13 10:59:39,521 - INFO - Epoch 9/10 [2.97s] | Train Loss: 0.2216 | Test Loss: 0.2336 | F1: 0.7724 | ROC-AUC: 0.9400\n",
      "2025-11-13 10:59:42,507 - INFO - Epoch 10/10 [2.99s] | Train Loss: 0.2167 | Test Loss: 0.2221 | F1: 0.7853 | ROC-AUC: 0.9456\n",
      "2025-11-13 10:59:42,508 - INFO - Training complete.\n",
      "2025-11-13 10:59:42,512 - INFO - {'loss': 0.222, 'accuracy': 0.919, 'f1': 0.785, 'roc_auc': 0.946, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 59801, 'test_size': 33464, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-13 10:59:42,522 - INFO - Yielding 50% split: 492 accounts, 75623 rows\n",
      "2025-11-13 10:59:42,524 - INFO - Embedding 75623 train texts...\n",
      "2025-11-13 10:59:43,007 - INFO - len(text_list)=75623 len(unique_texts)=46835 len(texts_to_embed)=0\n",
      "2025-11-13 10:59:44,297 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:59:44,553 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:59:45,124 - INFO - Fitting processor on 75623 rows...\n",
      "2025-11-13 10:59:45,125 - INFO - Transforming 75623 rows...\n",
      "2025-11-13 10:59:45,126 - INFO - Transform complete.\n",
      "2025-11-13 10:59:45,127 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:59:45,129 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 10:59:45,173 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:59:48,310 - INFO - Epoch 1/10 [3.14s] | Train Loss: 0.4044 | Test Loss: 0.3549 | F1: 0.5354 | ROC-AUC: 0.8700\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:49.024097100Z",
     "start_time": "2025-11-12T19:13:48.322479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# runner3 = ExpRunner.copy(runner1)\n",
    "# runner3.feat_proc_params = FeatProcParams(n_bins=20, k_top=50)\n",
    "# runner3.run_torch([0.1])\n"
   ],
   "id": "a26f355bbf7e1888",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:29:49.024097100Z",
     "start_time": "2025-11-12T21:03:30.169793Z"
    }
   },
   "cell_type": "code",
   "source": "results2 = runner2.run_torch(fracs)\n",
   "id": "5b1cc1626d140f29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12 23:03:30,179 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-12 23:03:30,267 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-12 23:03:30,269 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-12 23:03:30,280 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-12 23:03:30,288 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-12 23:03:30,310 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-12 23:03:30,438 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:31,234 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:31,888 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-12 23:03:31,888 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:03:31,889 - INFO - Transforming 16704 rows...\n",
      "2025-11-12 23:03:31,891 - INFO - Transform complete.\n",
      "2025-11-12 23:03:31,892 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:03:31,893 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:03:31,913 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:03:33,113 - INFO - Epoch 1/10 [1.20s] | Train Loss: 0.5402 | Test Loss: 0.3942 | F1: 0.3268 | ROC-AUC: 0.8249\n",
      "2025-11-12 23:03:34,319 - INFO - Epoch 2/10 [1.21s] | Train Loss: 0.3881 | Test Loss: 0.3652 | F1: 0.7097 | ROC-AUC: 0.8953\n",
      "2025-11-12 23:03:35,496 - INFO - Epoch 3/10 [1.18s] | Train Loss: 0.3180 | Test Loss: 0.6218 | F1: 0.5331 | ROC-AUC: 0.8380\n",
      "2025-11-12 23:03:36,691 - INFO - Epoch 4/10 [1.19s] | Train Loss: 0.2786 | Test Loss: 0.2922 | F1: 0.5913 | ROC-AUC: 0.9210\n",
      "2025-11-12 23:03:37,814 - INFO - Epoch 5/10 [1.12s] | Train Loss: 0.2624 | Test Loss: 0.2841 | F1: 0.7387 | ROC-AUC: 0.9122\n",
      "2025-11-12 23:03:38,931 - INFO - Epoch 6/10 [1.12s] | Train Loss: 0.2503 | Test Loss: 0.3447 | F1: 0.6682 | ROC-AUC: 0.8837\n",
      "2025-11-12 23:03:40,104 - INFO - Epoch 7/10 [1.17s] | Train Loss: 0.2392 | Test Loss: 0.2801 | F1: 0.7168 | ROC-AUC: 0.9146\n",
      "2025-11-12 23:03:41,290 - INFO - Epoch 8/10 [1.19s] | Train Loss: 0.2363 | Test Loss: 0.2715 | F1: 0.7301 | ROC-AUC: 0.9194\n",
      "2025-11-12 23:03:42,452 - INFO - Epoch 9/10 [1.16s] | Train Loss: 0.2326 | Test Loss: 0.3754 | F1: 0.7030 | ROC-AUC: 0.9140\n",
      "2025-11-12 23:03:43,609 - INFO - Epoch 10/10 [1.16s] | Train Loss: 0.2237 | Test Loss: 0.2592 | F1: 0.7343 | ROC-AUC: 0.9295\n",
      "2025-11-12 23:03:43,610 - INFO - Training complete.\n",
      "2025-11-12 23:03:43,636 - INFO - {'loss': 0.284, 'accuracy': 0.902, 'f1': 0.739, 'roc_auc': 0.912, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-12 23:03:43,645 - INFO - Yielding 20% split: 196 accounts, 29736 rows\n",
      "2025-11-12 23:03:43,814 - INFO - len(text_list)=29736 len(unique_texts)=19347 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:44,452 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:45,001 - INFO - Fitting processor on 29736 rows...\n",
      "2025-11-12 23:03:45,002 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:03:45,003 - INFO - Transforming 29736 rows...\n",
      "2025-11-12 23:03:45,004 - INFO - Transform complete.\n",
      "2025-11-12 23:03:45,004 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:03:45,006 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:03:45,042 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:03:46,608 - INFO - Epoch 1/10 [1.56s] | Train Loss: 0.5137 | Test Loss: 0.4197 | F1: 0.6848 | ROC-AUC: 0.8764\n",
      "2025-11-12 23:03:48,170 - INFO - Epoch 2/10 [1.56s] | Train Loss: 0.3277 | Test Loss: 0.3060 | F1: 0.7422 | ROC-AUC: 0.9043\n",
      "2025-11-12 23:03:49,726 - INFO - Epoch 3/10 [1.55s] | Train Loss: 0.2764 | Test Loss: 0.3424 | F1: 0.6772 | ROC-AUC: 0.8781\n",
      "2025-11-12 23:03:51,266 - INFO - Epoch 4/10 [1.54s] | Train Loss: 0.2596 | Test Loss: 0.2621 | F1: 0.7282 | ROC-AUC: 0.9191\n",
      "2025-11-12 23:03:52,868 - INFO - Epoch 5/10 [1.60s] | Train Loss: 0.2402 | Test Loss: 0.2612 | F1: 0.7608 | ROC-AUC: 0.9209\n",
      "2025-11-12 23:03:54,500 - INFO - Epoch 6/10 [1.63s] | Train Loss: 0.2377 | Test Loss: 0.2826 | F1: 0.6204 | ROC-AUC: 0.9271\n",
      "2025-11-12 23:03:56,217 - INFO - Epoch 7/10 [1.72s] | Train Loss: 0.2333 | Test Loss: 0.2713 | F1: 0.7554 | ROC-AUC: 0.9110\n",
      "2025-11-12 23:03:57,998 - INFO - Epoch 8/10 [1.78s] | Train Loss: 0.2269 | Test Loss: 0.2600 | F1: 0.7304 | ROC-AUC: 0.9173\n",
      "2025-11-12 23:03:59,826 - INFO - Epoch 9/10 [1.83s] | Train Loss: 0.2189 | Test Loss: 0.2483 | F1: 0.7235 | ROC-AUC: 0.9292\n",
      "2025-11-12 23:04:01,685 - INFO - Epoch 10/10 [1.86s] | Train Loss: 0.2161 | Test Loss: 0.2551 | F1: 0.7225 | ROC-AUC: 0.9279\n",
      "2025-11-12 23:04:01,686 - INFO - Training complete.\n",
      "2025-11-12 23:04:01,688 - INFO - {'loss': 0.261, 'accuracy': 0.908, 'f1': 0.761, 'roc_auc': 0.921, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 29736, 'test_size': 33464, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-12 23:04:01,697 - INFO - Yielding 30% split: 295 accounts, 42410 rows\n",
      "2025-11-12 23:04:01,961 - INFO - len(text_list)=42410 len(unique_texts)=26700 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:03,766 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:04,861 - INFO - Fitting processor on 42410 rows...\n",
      "2025-11-12 23:04:04,862 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:04:04,863 - INFO - Transforming 42410 rows...\n",
      "2025-11-12 23:04:04,863 - INFO - Transform complete.\n",
      "2025-11-12 23:04:04,864 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:04:04,865 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:04:04,902 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:04:08,540 - INFO - Epoch 1/10 [3.64s] | Train Loss: 0.4850 | Test Loss: 0.3557 | F1: 0.6780 | ROC-AUC: 0.8853\n",
      "2025-11-12 23:04:12,099 - INFO - Epoch 2/10 [3.56s] | Train Loss: 0.3029 | Test Loss: 0.2977 | F1: 0.5623 | ROC-AUC: 0.9281\n",
      "2025-11-12 23:04:15,692 - INFO - Epoch 3/10 [3.59s] | Train Loss: 0.2740 | Test Loss: 0.6315 | F1: 0.5537 | ROC-AUC: 0.9015\n",
      "2025-11-12 23:04:19,180 - INFO - Epoch 4/10 [3.49s] | Train Loss: 0.2605 | Test Loss: 0.3186 | F1: 0.5851 | ROC-AUC: 0.9271\n",
      "2025-11-12 23:04:22,812 - INFO - Epoch 5/10 [3.63s] | Train Loss: 0.2490 | Test Loss: 0.2394 | F1: 0.7435 | ROC-AUC: 0.9330\n",
      "2025-11-12 23:04:26,639 - INFO - Epoch 6/10 [3.83s] | Train Loss: 0.2398 | Test Loss: 0.2460 | F1: 0.7331 | ROC-AUC: 0.9384\n",
      "2025-11-12 23:04:30,319 - INFO - Epoch 7/10 [3.68s] | Train Loss: 0.2326 | Test Loss: 0.2581 | F1: 0.7001 | ROC-AUC: 0.9257\n",
      "2025-11-12 23:04:34,047 - INFO - Epoch 8/10 [3.73s] | Train Loss: 0.2289 | Test Loss: 0.2550 | F1: 0.7125 | ROC-AUC: 0.9377\n",
      "2025-11-12 23:04:37,804 - INFO - Epoch 9/10 [3.76s] | Train Loss: 0.2219 | Test Loss: 0.2299 | F1: 0.7668 | ROC-AUC: 0.9390\n",
      "2025-11-12 23:04:41,580 - INFO - Epoch 10/10 [3.78s] | Train Loss: 0.2201 | Test Loss: 0.2259 | F1: 0.7727 | ROC-AUC: 0.9443\n",
      "2025-11-12 23:04:41,581 - INFO - Training complete.\n",
      "2025-11-12 23:04:41,585 - INFO - {'loss': 0.226, 'accuracy': 0.917, 'f1': 0.773, 'roc_auc': 0.944, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 42410, 'test_size': 33464, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-12 23:04:41,596 - INFO - Yielding 40% split: 393 accounts, 59801 rows\n",
      "2025-11-12 23:04:42,041 - INFO - len(text_list)=59801 len(unique_texts)=37409 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:43,698 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:44,494 - INFO - Fitting processor on 59801 rows...\n",
      "2025-11-12 23:04:44,495 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:04:44,496 - INFO - Transforming 59801 rows...\n",
      "2025-11-12 23:04:44,497 - INFO - Transform complete.\n",
      "2025-11-12 23:04:44,497 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:04:44,499 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:04:44,552 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:04:49,102 - INFO - Epoch 1/10 [4.55s] | Train Loss: 0.4326 | Test Loss: 0.3931 | F1: 0.3665 | ROC-AUC: 0.8420\n",
      "2025-11-12 23:04:53,774 - INFO - Epoch 2/10 [4.67s] | Train Loss: 0.2958 | Test Loss: 0.2713 | F1: 0.7420 | ROC-AUC: 0.9065\n",
      "2025-11-12 23:04:58,620 - INFO - Epoch 3/10 [4.84s] | Train Loss: 0.2667 | Test Loss: 0.2543 | F1: 0.7486 | ROC-AUC: 0.9231\n",
      "2025-11-12 23:05:03,570 - INFO - Epoch 4/10 [4.95s] | Train Loss: 0.2516 | Test Loss: 0.2458 | F1: 0.7537 | ROC-AUC: 0.9275\n",
      "2025-11-12 23:05:08,314 - INFO - Epoch 5/10 [4.74s] | Train Loss: 0.2435 | Test Loss: 0.2845 | F1: 0.7347 | ROC-AUC: 0.9094\n",
      "2025-11-12 23:05:13,131 - INFO - Epoch 6/10 [4.82s] | Train Loss: 0.2369 | Test Loss: 0.2336 | F1: 0.7699 | ROC-AUC: 0.9345\n",
      "2025-11-12 23:05:17,813 - INFO - Epoch 7/10 [4.68s] | Train Loss: 0.2291 | Test Loss: 0.2563 | F1: 0.7485 | ROC-AUC: 0.9255\n",
      "2025-11-12 23:05:22,407 - INFO - Epoch 8/10 [4.59s] | Train Loss: 0.2277 | Test Loss: 0.2225 | F1: 0.7806 | ROC-AUC: 0.9424\n",
      "2025-11-12 23:05:27,105 - INFO - Epoch 9/10 [4.70s] | Train Loss: 0.2233 | Test Loss: 0.2362 | F1: 0.7782 | ROC-AUC: 0.9400\n",
      "2025-11-12 23:05:32,033 - INFO - Epoch 10/10 [4.93s] | Train Loss: 0.2197 | Test Loss: 0.2244 | F1: 0.7513 | ROC-AUC: 0.9437\n",
      "2025-11-12 23:05:32,034 - INFO - Training complete.\n",
      "2025-11-12 23:05:32,041 - INFO - {'loss': 0.222, 'accuracy': 0.92, 'f1': 0.781, 'roc_auc': 0.942, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 59801, 'test_size': 33464, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-12 23:05:32,062 - INFO - Yielding 50% split: 492 accounts, 75623 rows\n",
      "2025-11-12 23:05:38,224 - INFO - len(text_list)=75623 len(unique_texts)=46835 len(texts_to_embed)=0\n",
      "2025-11-12 23:05:44,988 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:05:45,804 - INFO - Fitting processor on 75623 rows...\n",
      "2025-11-12 23:05:45,805 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:05:45,806 - INFO - Transforming 75623 rows...\n",
      "2025-11-12 23:05:45,807 - INFO - Transform complete.\n",
      "2025-11-12 23:05:45,807 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:05:45,808 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:05:45,861 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:05:51,115 - INFO - Epoch 1/10 [5.25s] | Train Loss: 0.4109 | Test Loss: 0.2848 | F1: 0.6521 | ROC-AUC: 0.9228\n",
      "2025-11-12 23:05:56,273 - INFO - Epoch 2/10 [5.16s] | Train Loss: 0.2780 | Test Loss: 0.2568 | F1: 0.7728 | ROC-AUC: 0.9215\n",
      "2025-11-12 23:06:01,806 - INFO - Epoch 3/10 [5.53s] | Train Loss: 0.2578 | Test Loss: 0.2307 | F1: 0.7689 | ROC-AUC: 0.9372\n",
      "2025-11-12 23:06:07,470 - INFO - Epoch 4/10 [5.66s] | Train Loss: 0.2456 | Test Loss: 0.2719 | F1: 0.7565 | ROC-AUC: 0.9296\n",
      "2025-11-12 23:06:13,066 - INFO - Epoch 5/10 [5.60s] | Train Loss: 0.2331 | Test Loss: 0.2173 | F1: 0.7790 | ROC-AUC: 0.9469\n",
      "2025-11-12 23:06:18,674 - INFO - Epoch 6/10 [5.61s] | Train Loss: 0.2290 | Test Loss: 0.2490 | F1: 0.7756 | ROC-AUC: 0.9269\n",
      "2025-11-12 23:06:24,550 - INFO - Epoch 7/10 [5.88s] | Train Loss: 0.2228 | Test Loss: 0.2804 | F1: 0.6344 | ROC-AUC: 0.9378\n",
      "2025-11-12 23:06:30,265 - INFO - Epoch 8/10 [5.71s] | Train Loss: 0.2197 | Test Loss: 0.2292 | F1: 0.7852 | ROC-AUC: 0.9349\n",
      "2025-11-12 23:06:35,929 - INFO - Epoch 9/10 [5.66s] | Train Loss: 0.2171 | Test Loss: 0.2127 | F1: 0.7905 | ROC-AUC: 0.9459\n",
      "2025-11-12 23:06:41,982 - INFO - Epoch 10/10 [6.05s] | Train Loss: 0.2135 | Test Loss: 0.2176 | F1: 0.7729 | ROC-AUC: 0.9513\n",
      "2025-11-12 23:06:41,983 - INFO - Training complete.\n",
      "2025-11-12 23:06:41,990 - INFO - {'loss': 0.213, 'accuracy': 0.923, 'f1': 0.79, 'roc_auc': 0.946, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.5, 'train_size': 75623, 'test_size': 33464, 'train_accounts': 492, 'test_accounts': 246}\n",
      "2025-11-12 23:06:42,003 - INFO - Yielding 60% split: 590 accounts, 92281 rows\n",
      "2025-11-12 23:06:43,983 - INFO - len(text_list)=92281 len(unique_texts)=56280 len(texts_to_embed)=0\n",
      "2025-11-12 23:06:46,428 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:06:47,082 - INFO - Fitting processor on 92281 rows...\n",
      "2025-11-12 23:06:47,082 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:06:47,083 - INFO - Transforming 92281 rows...\n",
      "2025-11-12 23:06:47,084 - INFO - Transform complete.\n",
      "2025-11-12 23:06:47,084 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:06:47,085 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:06:47,146 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:06:53,103 - INFO - Epoch 1/10 [5.96s] | Train Loss: 0.3903 | Test Loss: 0.3132 | F1: 0.6007 | ROC-AUC: 0.9043\n",
      "2025-11-12 23:06:59,219 - INFO - Epoch 2/10 [6.11s] | Train Loss: 0.2732 | Test Loss: 0.2501 | F1: 0.7334 | ROC-AUC: 0.9321\n",
      "2025-11-12 23:07:06,054 - INFO - Epoch 3/10 [6.83s] | Train Loss: 0.2520 | Test Loss: 0.2468 | F1: 0.7436 | ROC-AUC: 0.9314\n",
      "2025-11-12 23:07:12,663 - INFO - Epoch 4/10 [6.61s] | Train Loss: 0.2383 | Test Loss: 0.2255 | F1: 0.7777 | ROC-AUC: 0.9400\n",
      "2025-11-12 23:07:19,484 - INFO - Epoch 5/10 [6.82s] | Train Loss: 0.2307 | Test Loss: 0.2335 | F1: 0.7436 | ROC-AUC: 0.9449\n",
      "2025-11-12 23:07:26,178 - INFO - Epoch 6/10 [6.69s] | Train Loss: 0.2275 | Test Loss: 0.2238 | F1: 0.7817 | ROC-AUC: 0.9410\n",
      "2025-11-12 23:07:32,843 - INFO - Epoch 7/10 [6.66s] | Train Loss: 0.2239 | Test Loss: 0.2535 | F1: 0.7614 | ROC-AUC: 0.9257\n",
      "2025-11-12 23:07:39,629 - INFO - Epoch 8/10 [6.79s] | Train Loss: 0.2185 | Test Loss: 0.2216 | F1: 0.7895 | ROC-AUC: 0.9435\n",
      "2025-11-12 23:07:46,493 - INFO - Epoch 9/10 [6.86s] | Train Loss: 0.2190 | Test Loss: 0.2264 | F1: 0.7876 | ROC-AUC: 0.9442\n",
      "2025-11-12 23:07:53,347 - INFO - Epoch 10/10 [6.85s] | Train Loss: 0.2176 | Test Loss: 0.2244 | F1: 0.7880 | ROC-AUC: 0.9471\n",
      "2025-11-12 23:07:53,348 - INFO - Training complete.\n",
      "2025-11-12 23:07:53,352 - INFO - {'loss': 0.222, 'accuracy': 0.92, 'f1': 0.789, 'roc_auc': 0.944, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.6, 'train_size': 92281, 'test_size': 33464, 'train_accounts': 590, 'test_accounts': 246}\n",
      "2025-11-12 23:07:53,369 - INFO - Yielding 70% split: 688 accounts, 108248 rows\n",
      "2025-11-12 23:07:55,617 - INFO - len(text_list)=108248 len(unique_texts)=65808 len(texts_to_embed)=0\n",
      "2025-11-12 23:07:58,167 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:07:58,873 - INFO - Fitting processor on 108248 rows...\n",
      "2025-11-12 23:07:58,873 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:07:58,874 - INFO - Transforming 108248 rows...\n",
      "2025-11-12 23:07:58,875 - INFO - Transform complete.\n",
      "2025-11-12 23:07:58,876 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:07:58,876 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:07:58,930 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:08:05,961 - INFO - Epoch 1/10 [7.03s] | Train Loss: 0.3721 | Test Loss: 0.2713 | F1: 0.6816 | ROC-AUC: 0.9313\n",
      "2025-11-12 23:08:13,296 - INFO - Epoch 2/10 [7.33s] | Train Loss: 0.2660 | Test Loss: 0.2425 | F1: 0.7351 | ROC-AUC: 0.9400\n",
      "2025-11-12 23:08:20,981 - INFO - Epoch 3/10 [7.68s] | Train Loss: 0.2447 | Test Loss: 0.2411 | F1: 0.7811 | ROC-AUC: 0.9380\n",
      "2025-11-12 23:08:28,734 - INFO - Epoch 4/10 [7.75s] | Train Loss: 0.2353 | Test Loss: 0.2441 | F1: 0.7750 | ROC-AUC: 0.9336\n",
      "2025-11-12 23:08:36,473 - INFO - Epoch 5/10 [7.74s] | Train Loss: 0.2326 | Test Loss: 0.2413 | F1: 0.7459 | ROC-AUC: 0.9367\n",
      "2025-11-12 23:08:44,290 - INFO - Epoch 6/10 [7.82s] | Train Loss: 0.2253 | Test Loss: 0.2277 | F1: 0.7488 | ROC-AUC: 0.9467\n",
      "2025-11-12 23:08:52,268 - INFO - Epoch 7/10 [7.98s] | Train Loss: 0.2209 | Test Loss: 0.2335 | F1: 0.7512 | ROC-AUC: 0.9424\n",
      "2025-11-12 23:09:00,130 - INFO - Epoch 8/10 [7.86s] | Train Loss: 0.2179 | Test Loss: 0.2166 | F1: 0.7871 | ROC-AUC: 0.9489\n",
      "2025-11-12 23:09:08,070 - INFO - Epoch 9/10 [7.94s] | Train Loss: 0.2150 | Test Loss: 0.2288 | F1: 0.7693 | ROC-AUC: 0.9486\n",
      "2025-11-12 23:09:15,744 - INFO - Epoch 10/10 [7.67s] | Train Loss: 0.2137 | Test Loss: 0.2354 | F1: 0.7860 | ROC-AUC: 0.9440\n",
      "2025-11-12 23:09:15,745 - INFO - Training complete.\n",
      "2025-11-12 23:09:15,752 - INFO - {'loss': 0.217, 'accuracy': 0.92, 'f1': 0.787, 'roc_auc': 0.949, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.7, 'train_size': 108248, 'test_size': 33464, 'train_accounts': 688, 'test_accounts': 246}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e6aef1cf8d361873"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
