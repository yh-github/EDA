{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:40:26.632477Z",
     "start_time": "2025-11-16T13:40:26.625504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from log_utils import setup_logging\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "setup_logging(Path('../logs'))\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "e5ac7a92777dcdbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 15:40:26,630 - INFO - Logging to ..\\logs\\2025-11-16_15-40-26.log\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-16T13:40:28.137454Z",
     "start_time": "2025-11-16T13:40:26.647267Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from config import FieldConfig\n",
    "from pathlib import Path\n",
    "DATA_PATH = Path('C:/Work/Data/proc/')\n",
    "field_config = FieldConfig()\n",
    "\n",
    "df = pd.read_csv(DATA_PATH/'rec_data2.csv')\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "df_cleaned = df.dropna(subset=[field_config.date, field_config.amount, field_config.text, field_config.label])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "id                    0\n",
      "accountId             0\n",
      "date                  0\n",
      "amount                0\n",
      "bankRawDescription    0\n",
      "isRecurring           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:40:28.203807Z",
     "start_time": "2025-11-16T13:40:28.194120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from feature_processor import FeatProcParams\n",
    "from config import *\n",
    "\n",
    "feat_params = FeatProcParams(n_bins=20, k_top=50)\n",
    "feat_params_off = FeatProcParams.all_off()\n",
    "\n",
    "exp_config = ExperimentConfig(random_state=112025)\n",
    "\n",
    "fracs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]"
   ],
   "id": "64ad44e449b7478f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:40:31.953845Z",
     "start_time": "2025-11-16T13:40:29.891155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from classifier import HybridModel\n",
    "from runner import ExpRunner\n",
    "from config import EmbModel\n",
    "from embedder import EmbeddingService\n",
    "\n",
    "runner1 = ExpRunner.create(\n",
    "    exp_params=exp_config,\n",
    "    full_df=df_cleaned,\n",
    "    emb_params=EmbeddingService.Params(model_name=EmbModel.ALBERT),\n",
    "    feat_proc_params=feat_params,\n",
    "    model_params=HybridModel.MlpHyperParams(),\n",
    "    field_config=FieldConfig()\n",
    ")"
   ],
   "id": "d651a17edece86d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoav.haimovitch\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 15:40:31,944 - INFO - Setting global random seed to 112025\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:45:05.289158Z",
     "start_time": "2025-11-16T13:40:34.802762Z"
    }
   },
   "cell_type": "code",
   "source": "results = runner1.run_training_set_size(fracs)",
   "id": "97b17b877b9520a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 15:40:34,810 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-16 15:40:34,884 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-16 15:40:34,884 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-16 15:40:34,891 - INFO - Split complete. Train: len=150232 accounts=984, Test: len=39755 accounts=246.\n",
      "2025-11-16 15:40:34,895 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-16 15:40:34,909 - INFO - Yielding 10% split: 98 accounts, 15026 rows\n",
      "2025-11-16 15:40:34,911 - INFO - Creating new EmbeddingService(model_name=albert-base-v2)\n",
      "2025-11-16 15:40:34,912 - INFO - Loading embedding model: albert-base-v2...\n",
      "2025-11-16 15:40:36,137 - INFO - Model albert-base-v2 loaded onto cpu. Cache at cache/albert-base-v2\n",
      "2025-11-16 15:40:36,138 - INFO - Embedding 15026 train texts...\n",
      "2025-11-16 15:40:36,711 - INFO - len(text_list)=15026 len(unique_texts)=9841 len(texts_to_embed)=0\n",
      "2025-11-16 15:40:36,994 - INFO - Embedding 39755 test texts...\n",
      "2025-11-16 15:40:38,012 - INFO - len(text_list)=39755 len(unique_texts)=25274 len(texts_to_embed)=0\n",
      "2025-11-16 15:40:38,833 - INFO - Fitting processor on 15026 rows...\n",
      "2025-11-16 15:40:38,833 - INFO - Fitting categorical amount features...\n",
      "2025-11-16 15:40:38,840 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:40:38,841 - INFO - Created 20 fallback bins.\n",
      "2025-11-16 15:40:38,842 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-16 15:40:38,843 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:40:38,844 - INFO - Transforming 15026 rows...\n",
      "2025-11-16 15:40:38,903 - INFO - Transform complete.\n",
      "2025-11-16 15:40:38,904 - INFO - Transforming 39755 rows...\n",
      "2025-11-16 15:40:39,040 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-16 15:40:39,068 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-16 15:40:40,497 - INFO - Epoch 1/10 [1.43s] | Train Loss: 0.5481 | Test Loss: 0.3703 | F1: 0.5486 | ROC-AUC: 0.8438\n",
      "2025-11-16 15:40:41,799 - INFO - Epoch 2/10 [1.30s] | Train Loss: 0.3863 | Test Loss: 0.3394 | F1: 0.6043 | ROC-AUC: 0.8559\n",
      "2025-11-16 15:40:43,114 - INFO - Epoch 3/10 [1.31s] | Train Loss: 0.3212 | Test Loss: 0.3557 | F1: 0.6878 | ROC-AUC: 0.8858\n",
      "2025-11-16 15:40:44,430 - INFO - Epoch 4/10 [1.31s] | Train Loss: 0.2740 | Test Loss: 0.3134 | F1: 0.5529 | ROC-AUC: 0.8870\n",
      "2025-11-16 15:40:45,877 - INFO - Epoch 5/10 [1.45s] | Train Loss: 0.2552 | Test Loss: 0.3305 | F1: 0.5987 | ROC-AUC: 0.8504\n",
      "2025-11-16 15:40:47,616 - INFO - Epoch 6/10 [1.74s] | Train Loss: 0.2419 | Test Loss: 0.3446 | F1: 0.6245 | ROC-AUC: 0.8705\n",
      "2025-11-16 15:40:47,617 - INFO - Early stopping triggered at epoch 6. Best F1: 0.6878\n",
      "2025-11-16 15:40:47,618 - INFO - Training complete.\n",
      "2025-11-16 15:40:47,629 - INFO - {'loss': 0.356, 'accuracy': 0.884, 'f1': 0.688, 'roc_auc': 0.886, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 15026, 'test_size': 39755, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-16 15:40:47,644 - INFO - Yielding 20% split: 196 accounts, 30336 rows\n",
      "2025-11-16 15:40:47,646 - INFO - Embedding 30336 train texts...\n",
      "2025-11-16 15:40:48,538 - INFO - len(text_list)=30336 len(unique_texts)=18745 len(texts_to_embed)=0\n",
      "2025-11-16 15:40:49,737 - INFO - Embedding 39755 test texts...\n",
      "2025-11-16 15:40:50,389 - INFO - len(text_list)=39755 len(unique_texts)=25274 len(texts_to_embed)=0\n",
      "2025-11-16 15:40:52,007 - INFO - Fitting processor on 30336 rows...\n",
      "2025-11-16 15:40:52,008 - INFO - Fitting categorical amount features...\n",
      "2025-11-16 15:40:52,021 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:40:52,022 - INFO - Created 20 fallback bins.\n",
      "2025-11-16 15:40:52,022 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-16 15:40:52,023 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:40:52,024 - INFO - Transforming 30336 rows...\n",
      "2025-11-16 15:40:52,305 - INFO - Transform complete.\n",
      "2025-11-16 15:40:52,306 - INFO - Transforming 39755 rows...\n",
      "2025-11-16 15:40:52,510 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-16 15:40:52,553 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-16 15:40:54,629 - INFO - Epoch 1/10 [2.08s] | Train Loss: 0.5027 | Test Loss: 0.4255 | F1: 0.6187 | ROC-AUC: 0.8633\n",
      "2025-11-16 15:40:56,736 - INFO - Epoch 2/10 [2.11s] | Train Loss: 0.3176 | Test Loss: 0.3880 | F1: 0.6579 | ROC-AUC: 0.8917\n",
      "2025-11-16 15:40:58,849 - INFO - Epoch 3/10 [2.11s] | Train Loss: 0.2668 | Test Loss: 0.2716 | F1: 0.7088 | ROC-AUC: 0.9107\n",
      "2025-11-16 15:41:00,948 - INFO - Epoch 4/10 [2.10s] | Train Loss: 0.2450 | Test Loss: 0.3311 | F1: 0.4847 | ROC-AUC: 0.8997\n",
      "2025-11-16 15:41:03,570 - INFO - Epoch 5/10 [2.62s] | Train Loss: 0.2303 | Test Loss: 0.3574 | F1: 0.6924 | ROC-AUC: 0.9107\n",
      "2025-11-16 15:41:05,740 - INFO - Epoch 6/10 [2.17s] | Train Loss: 0.2237 | Test Loss: 0.2648 | F1: 0.7068 | ROC-AUC: 0.9119\n",
      "2025-11-16 15:41:05,741 - INFO - Early stopping triggered at epoch 6. Best F1: 0.7088\n",
      "2025-11-16 15:41:05,742 - INFO - Training complete.\n",
      "2025-11-16 15:41:05,758 - INFO - {'loss': 0.272, 'accuracy': 0.897, 'f1': 0.709, 'roc_auc': 0.911, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 30336, 'test_size': 39755, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-16 15:41:05,772 - INFO - Yielding 30% split: 295 accounts, 44605 rows\n",
      "2025-11-16 15:41:05,774 - INFO - Embedding 44605 train texts...\n",
      "2025-11-16 15:41:06,830 - INFO - len(text_list)=44605 len(unique_texts)=27455 len(texts_to_embed)=0\n",
      "2025-11-16 15:41:08,518 - INFO - Embedding 39755 test texts...\n",
      "2025-11-16 15:41:09,067 - INFO - len(text_list)=39755 len(unique_texts)=25274 len(texts_to_embed)=0\n",
      "2025-11-16 15:41:10,923 - INFO - Fitting processor on 44605 rows...\n",
      "2025-11-16 15:41:10,924 - INFO - Fitting categorical amount features...\n",
      "2025-11-16 15:41:10,940 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:41:10,942 - INFO - Created 20 fallback bins.\n",
      "2025-11-16 15:41:10,943 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-16 15:41:10,943 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:41:10,944 - INFO - Transforming 44605 rows...\n",
      "2025-11-16 15:41:11,408 - INFO - Transform complete.\n",
      "2025-11-16 15:41:11,408 - INFO - Transforming 39755 rows...\n",
      "2025-11-16 15:41:11,602 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-16 15:41:11,644 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-16 15:41:14,384 - INFO - Epoch 1/10 [2.74s] | Train Loss: 0.4616 | Test Loss: 0.3986 | F1: 0.6710 | ROC-AUC: 0.8878\n",
      "2025-11-16 15:41:17,100 - INFO - Epoch 2/10 [2.71s] | Train Loss: 0.2944 | Test Loss: 0.2708 | F1: 0.7077 | ROC-AUC: 0.9091\n",
      "2025-11-16 15:41:19,959 - INFO - Epoch 3/10 [2.86s] | Train Loss: 0.2580 | Test Loss: 0.2826 | F1: 0.7193 | ROC-AUC: 0.9132\n",
      "2025-11-16 15:41:22,714 - INFO - Epoch 4/10 [2.75s] | Train Loss: 0.2410 | Test Loss: 0.2503 | F1: 0.7036 | ROC-AUC: 0.9286\n",
      "2025-11-16 15:41:25,796 - INFO - Epoch 5/10 [3.08s] | Train Loss: 0.2311 | Test Loss: 0.2894 | F1: 0.6536 | ROC-AUC: 0.9217\n",
      "2025-11-16 15:41:28,660 - INFO - Epoch 6/10 [2.86s] | Train Loss: 0.2239 | Test Loss: 0.2655 | F1: 0.7281 | ROC-AUC: 0.9216\n",
      "2025-11-16 15:41:31,774 - INFO - Epoch 7/10 [3.11s] | Train Loss: 0.2180 | Test Loss: 0.2429 | F1: 0.7532 | ROC-AUC: 0.9272\n",
      "2025-11-16 15:41:34,740 - INFO - Epoch 8/10 [2.96s] | Train Loss: 0.2153 | Test Loss: 0.2549 | F1: 0.7260 | ROC-AUC: 0.9239\n",
      "2025-11-16 15:41:37,721 - INFO - Epoch 9/10 [2.98s] | Train Loss: 0.2100 | Test Loss: 0.2392 | F1: 0.7517 | ROC-AUC: 0.9315\n",
      "2025-11-16 15:41:40,583 - INFO - Epoch 10/10 [2.86s] | Train Loss: 0.2072 | Test Loss: 0.2350 | F1: 0.7461 | ROC-AUC: 0.9326\n",
      "2025-11-16 15:41:40,584 - INFO - Early stopping triggered at epoch 10. Best F1: 0.7532\n",
      "2025-11-16 15:41:40,585 - INFO - Training complete.\n",
      "2025-11-16 15:41:40,604 - INFO - {'loss': 0.243, 'accuracy': 0.911, 'f1': 0.753, 'roc_auc': 0.927, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 44605, 'test_size': 39755, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-16 15:41:40,622 - INFO - Yielding 40% split: 393 accounts, 58439 rows\n",
      "2025-11-16 15:41:40,625 - INFO - Embedding 58439 train texts...\n",
      "2025-11-16 15:41:42,010 - INFO - len(text_list)=58439 len(unique_texts)=35970 len(texts_to_embed)=0\n",
      "2025-11-16 15:41:44,537 - INFO - Embedding 39755 test texts...\n",
      "2025-11-16 15:41:45,262 - INFO - len(text_list)=39755 len(unique_texts)=25274 len(texts_to_embed)=0\n",
      "2025-11-16 15:41:47,258 - INFO - Fitting processor on 58439 rows...\n",
      "2025-11-16 15:41:47,260 - INFO - Fitting categorical amount features...\n",
      "2025-11-16 15:41:47,277 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:41:47,278 - INFO - Created 20 fallback bins.\n",
      "2025-11-16 15:41:47,279 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-16 15:41:47,280 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:41:47,280 - INFO - Transforming 58439 rows...\n",
      "2025-11-16 15:41:47,956 - INFO - Transform complete.\n",
      "2025-11-16 15:41:47,957 - INFO - Transforming 39755 rows...\n",
      "2025-11-16 15:41:48,417 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-16 15:41:48,470 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-16 15:41:51,921 - INFO - Epoch 1/10 [3.45s] | Train Loss: 0.4149 | Test Loss: 0.3739 | F1: 0.6135 | ROC-AUC: 0.8605\n",
      "2025-11-16 15:41:55,338 - INFO - Epoch 2/10 [3.42s] | Train Loss: 0.2762 | Test Loss: 0.3359 | F1: 0.6537 | ROC-AUC: 0.8899\n",
      "2025-11-16 15:41:58,684 - INFO - Epoch 3/10 [3.34s] | Train Loss: 0.2540 | Test Loss: 0.2897 | F1: 0.7198 | ROC-AUC: 0.9158\n",
      "2025-11-16 15:42:02,526 - INFO - Epoch 4/10 [3.84s] | Train Loss: 0.2378 | Test Loss: 0.2439 | F1: 0.7026 | ROC-AUC: 0.9332\n",
      "2025-11-16 15:42:05,882 - INFO - Epoch 5/10 [3.35s] | Train Loss: 0.2282 | Test Loss: 0.2380 | F1: 0.7353 | ROC-AUC: 0.9309\n",
      "2025-11-16 15:42:09,072 - INFO - Epoch 6/10 [3.19s] | Train Loss: 0.2214 | Test Loss: 0.2345 | F1: 0.7566 | ROC-AUC: 0.9374\n",
      "2025-11-16 15:42:12,226 - INFO - Epoch 7/10 [3.15s] | Train Loss: 0.2173 | Test Loss: 0.2622 | F1: 0.6191 | ROC-AUC: 0.9322\n",
      "2025-11-16 15:42:15,328 - INFO - Epoch 8/10 [3.10s] | Train Loss: 0.2129 | Test Loss: 0.2252 | F1: 0.7610 | ROC-AUC: 0.9386\n",
      "2025-11-16 15:42:18,548 - INFO - Epoch 9/10 [3.22s] | Train Loss: 0.2079 | Test Loss: 0.2385 | F1: 0.7206 | ROC-AUC: 0.9329\n",
      "2025-11-16 15:42:22,080 - INFO - Epoch 10/10 [3.53s] | Train Loss: 0.2074 | Test Loss: 0.2470 | F1: 0.7390 | ROC-AUC: 0.9327\n",
      "2025-11-16 15:42:22,081 - INFO - Training complete.\n",
      "2025-11-16 15:42:22,103 - INFO - {'loss': 0.225, 'accuracy': 0.915, 'f1': 0.761, 'roc_auc': 0.939, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 58439, 'test_size': 39755, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-16 15:42:22,139 - INFO - Yielding 50% split: 492 accounts, 74028 rows\n",
      "2025-11-16 15:42:22,142 - INFO - Embedding 74028 train texts...\n",
      "2025-11-16 15:42:24,643 - INFO - len(text_list)=74028 len(unique_texts)=45839 len(texts_to_embed)=0\n",
      "2025-11-16 15:42:28,621 - INFO - Embedding 39755 test texts...\n",
      "2025-11-16 15:42:29,433 - INFO - len(text_list)=39755 len(unique_texts)=25274 len(texts_to_embed)=0\n",
      "2025-11-16 15:42:31,191 - INFO - Fitting processor on 74028 rows...\n",
      "2025-11-16 15:42:31,192 - INFO - Fitting categorical amount features...\n",
      "2025-11-16 15:42:31,212 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:42:31,212 - INFO - Created 20 fallback bins.\n",
      "2025-11-16 15:42:31,213 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-16 15:42:31,214 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:42:31,214 - INFO - Transforming 74028 rows...\n",
      "2025-11-16 15:42:31,791 - INFO - Transform complete.\n",
      "2025-11-16 15:42:31,792 - INFO - Transforming 39755 rows...\n",
      "2025-11-16 15:42:32,132 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-16 15:42:32,193 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-16 15:42:36,347 - INFO - Epoch 1/10 [4.15s] | Train Loss: 0.3990 | Test Loss: 0.2797 | F1: 0.7107 | ROC-AUC: 0.9056\n",
      "2025-11-16 15:42:40,192 - INFO - Epoch 2/10 [3.84s] | Train Loss: 0.2657 | Test Loss: 0.2810 | F1: 0.7421 | ROC-AUC: 0.9164\n",
      "2025-11-16 15:42:44,126 - INFO - Epoch 3/10 [3.93s] | Train Loss: 0.2399 | Test Loss: 0.2549 | F1: 0.6886 | ROC-AUC: 0.9302\n",
      "2025-11-16 15:42:48,115 - INFO - Epoch 4/10 [3.99s] | Train Loss: 0.2290 | Test Loss: 0.2254 | F1: 0.7500 | ROC-AUC: 0.9391\n",
      "2025-11-16 15:42:52,224 - INFO - Epoch 5/10 [4.11s] | Train Loss: 0.2194 | Test Loss: 0.2305 | F1: 0.7535 | ROC-AUC: 0.9361\n",
      "2025-11-16 15:42:56,545 - INFO - Epoch 6/10 [4.32s] | Train Loss: 0.2169 | Test Loss: 0.3243 | F1: 0.7049 | ROC-AUC: 0.9192\n",
      "2025-11-16 15:43:00,639 - INFO - Epoch 7/10 [4.09s] | Train Loss: 0.2127 | Test Loss: 0.2244 | F1: 0.7547 | ROC-AUC: 0.9392\n",
      "2025-11-16 15:43:04,764 - INFO - Epoch 8/10 [4.12s] | Train Loss: 0.2106 | Test Loss: 0.2343 | F1: 0.7616 | ROC-AUC: 0.9395\n",
      "2025-11-16 15:43:09,027 - INFO - Epoch 9/10 [4.26s] | Train Loss: 0.2060 | Test Loss: 0.2354 | F1: 0.7191 | ROC-AUC: 0.9389\n",
      "2025-11-16 15:43:13,178 - INFO - Epoch 10/10 [4.15s] | Train Loss: 0.2035 | Test Loss: 0.2229 | F1: 0.7697 | ROC-AUC: 0.9420\n",
      "2025-11-16 15:43:13,180 - INFO - Training complete.\n",
      "2025-11-16 15:43:13,192 - INFO - {'loss': 0.223, 'accuracy': 0.916, 'f1': 0.77, 'roc_auc': 0.942, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.5, 'train_size': 74028, 'test_size': 39755, 'train_accounts': 492, 'test_accounts': 246}\n",
      "2025-11-16 15:43:13,210 - INFO - Yielding 60% split: 590 accounts, 88909 rows\n",
      "2025-11-16 15:43:13,213 - INFO - Embedding 88909 train texts...\n",
      "2025-11-16 15:43:14,838 - INFO - len(text_list)=88909 len(unique_texts)=55247 len(texts_to_embed)=0\n",
      "2025-11-16 15:43:18,754 - INFO - Embedding 39755 test texts...\n",
      "2025-11-16 15:43:19,495 - INFO - len(text_list)=39755 len(unique_texts)=25274 len(texts_to_embed)=0\n",
      "2025-11-16 15:43:21,417 - INFO - Fitting processor on 88909 rows...\n",
      "2025-11-16 15:43:21,419 - INFO - Fitting categorical amount features...\n",
      "2025-11-16 15:43:21,446 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:43:21,447 - INFO - Created 20 fallback bins.\n",
      "2025-11-16 15:43:21,448 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-16 15:43:21,448 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:43:21,449 - INFO - Transforming 88909 rows...\n",
      "2025-11-16 15:43:22,332 - INFO - Transform complete.\n",
      "2025-11-16 15:43:22,333 - INFO - Transforming 39755 rows...\n",
      "2025-11-16 15:43:22,683 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-16 15:43:22,756 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-16 15:43:27,438 - INFO - Epoch 1/10 [4.68s] | Train Loss: 0.3868 | Test Loss: 0.2762 | F1: 0.6147 | ROC-AUC: 0.9189\n",
      "2025-11-16 15:43:32,378 - INFO - Epoch 2/10 [4.94s] | Train Loss: 0.2569 | Test Loss: 0.2427 | F1: 0.7481 | ROC-AUC: 0.9277\n",
      "2025-11-16 15:43:37,586 - INFO - Epoch 3/10 [5.21s] | Train Loss: 0.2350 | Test Loss: 0.2326 | F1: 0.7411 | ROC-AUC: 0.9359\n",
      "2025-11-16 15:43:42,634 - INFO - Epoch 4/10 [5.05s] | Train Loss: 0.2232 | Test Loss: 0.2272 | F1: 0.7673 | ROC-AUC: 0.9372\n",
      "2025-11-16 15:43:47,724 - INFO - Epoch 5/10 [5.09s] | Train Loss: 0.2165 | Test Loss: 0.2575 | F1: 0.7581 | ROC-AUC: 0.9318\n",
      "2025-11-16 15:43:53,151 - INFO - Epoch 6/10 [5.43s] | Train Loss: 0.2105 | Test Loss: 0.2450 | F1: 0.7591 | ROC-AUC: 0.9325\n",
      "2025-11-16 15:43:58,485 - INFO - Epoch 7/10 [5.33s] | Train Loss: 0.2079 | Test Loss: 0.2248 | F1: 0.7541 | ROC-AUC: 0.9405\n",
      "2025-11-16 15:43:58,486 - INFO - Early stopping triggered at epoch 7. Best F1: 0.7673\n",
      "2025-11-16 15:43:58,487 - INFO - Training complete.\n",
      "2025-11-16 15:43:58,495 - INFO - {'loss': 0.227, 'accuracy': 0.918, 'f1': 0.767, 'roc_auc': 0.937, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.6, 'train_size': 88909, 'test_size': 39755, 'train_accounts': 590, 'test_accounts': 246}\n",
      "2025-11-16 15:43:58,516 - INFO - Yielding 70% split: 688 accounts, 105083 rows\n",
      "2025-11-16 15:43:58,519 - INFO - Embedding 105083 train texts...\n",
      "2025-11-16 15:44:01,244 - INFO - len(text_list)=105083 len(unique_texts)=65075 len(texts_to_embed)=0\n",
      "2025-11-16 15:44:07,096 - INFO - Embedding 39755 test texts...\n",
      "2025-11-16 15:44:07,927 - INFO - len(text_list)=39755 len(unique_texts)=25274 len(texts_to_embed)=0\n",
      "2025-11-16 15:44:09,875 - INFO - Fitting processor on 105083 rows...\n",
      "2025-11-16 15:44:09,877 - INFO - Fitting categorical amount features...\n",
      "2025-11-16 15:44:09,913 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:44:09,914 - INFO - Created 20 fallback bins.\n",
      "2025-11-16 15:44:09,915 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-16 15:44:09,915 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-16 15:44:09,916 - INFO - Transforming 105083 rows...\n",
      "2025-11-16 15:44:10,605 - INFO - Transform complete.\n",
      "2025-11-16 15:44:10,606 - INFO - Transforming 39755 rows...\n",
      "2025-11-16 15:44:10,749 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 889\n",
      "2025-11-16 15:44:10,820 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-16 15:44:15,855 - INFO - Epoch 1/10 [5.03s] | Train Loss: 0.3577 | Test Loss: 0.2643 | F1: 0.6858 | ROC-AUC: 0.9098\n",
      "2025-11-16 15:44:20,929 - INFO - Epoch 2/10 [5.07s] | Train Loss: 0.2466 | Test Loss: 0.2380 | F1: 0.7363 | ROC-AUC: 0.9317\n",
      "2025-11-16 15:44:25,868 - INFO - Epoch 3/10 [4.94s] | Train Loss: 0.2270 | Test Loss: 0.2281 | F1: 0.7200 | ROC-AUC: 0.9415\n",
      "2025-11-16 15:44:30,914 - INFO - Epoch 4/10 [5.05s] | Train Loss: 0.2166 | Test Loss: 0.2213 | F1: 0.7614 | ROC-AUC: 0.9410\n",
      "2025-11-16 15:44:36,729 - INFO - Epoch 5/10 [5.81s] | Train Loss: 0.2082 | Test Loss: 0.2129 | F1: 0.7713 | ROC-AUC: 0.9458\n",
      "2025-11-16 15:44:42,371 - INFO - Epoch 6/10 [5.64s] | Train Loss: 0.2054 | Test Loss: 0.2193 | F1: 0.7490 | ROC-AUC: 0.9435\n",
      "2025-11-16 15:44:48,399 - INFO - Epoch 7/10 [6.03s] | Train Loss: 0.2030 | Test Loss: 0.2172 | F1: 0.7741 | ROC-AUC: 0.9464\n",
      "2025-11-16 15:44:53,945 - INFO - Epoch 8/10 [5.55s] | Train Loss: 0.1998 | Test Loss: 0.2148 | F1: 0.7642 | ROC-AUC: 0.9438\n",
      "2025-11-16 15:44:59,398 - INFO - Epoch 9/10 [5.45s] | Train Loss: 0.1972 | Test Loss: 0.2327 | F1: 0.7351 | ROC-AUC: 0.9422\n",
      "2025-11-16 15:45:05,227 - INFO - Epoch 10/10 [5.83s] | Train Loss: 0.1958 | Test Loss: 0.2052 | F1: 0.7782 | ROC-AUC: 0.9492\n",
      "2025-11-16 15:45:05,228 - INFO - Training complete.\n",
      "2025-11-16 15:45:05,237 - INFO - {'loss': 0.205, 'accuracy': 0.923, 'f1': 0.778, 'roc_auc': 0.949, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.7, 'train_size': 105083, 'test_size': 39755, 'train_accounts': 688, 'test_accounts': 246}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:45:05.376999Z",
     "start_time": "2025-11-16T13:45:05.367943Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "d0c438cefd39b695",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15026: {'loss': 0.356,\n",
       "  'accuracy': 0.884,\n",
       "  'f1': 0.688,\n",
       "  'roc_auc': 0.886,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.1,\n",
       "  'train_size': 15026,\n",
       "  'test_size': 39755,\n",
       "  'train_accounts': 98,\n",
       "  'test_accounts': 246},\n",
       " 30336: {'loss': 0.272,\n",
       "  'accuracy': 0.897,\n",
       "  'f1': 0.709,\n",
       "  'roc_auc': 0.911,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.2,\n",
       "  'train_size': 30336,\n",
       "  'test_size': 39755,\n",
       "  'train_accounts': 196,\n",
       "  'test_accounts': 246},\n",
       " 44605: {'loss': 0.243,\n",
       "  'accuracy': 0.911,\n",
       "  'f1': 0.753,\n",
       "  'roc_auc': 0.927,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.3,\n",
       "  'train_size': 44605,\n",
       "  'test_size': 39755,\n",
       "  'train_accounts': 295,\n",
       "  'test_accounts': 246},\n",
       " 58439: {'loss': 0.225,\n",
       "  'accuracy': 0.915,\n",
       "  'f1': 0.761,\n",
       "  'roc_auc': 0.939,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.4,\n",
       "  'train_size': 58439,\n",
       "  'test_size': 39755,\n",
       "  'train_accounts': 393,\n",
       "  'test_accounts': 246},\n",
       " 74028: {'loss': 0.223,\n",
       "  'accuracy': 0.916,\n",
       "  'f1': 0.77,\n",
       "  'roc_auc': 0.942,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.5,\n",
       "  'train_size': 74028,\n",
       "  'test_size': 39755,\n",
       "  'train_accounts': 492,\n",
       "  'test_accounts': 246},\n",
       " 88909: {'loss': 0.227,\n",
       "  'accuracy': 0.918,\n",
       "  'f1': 0.767,\n",
       "  'roc_auc': 0.937,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.6,\n",
       "  'train_size': 88909,\n",
       "  'test_size': 39755,\n",
       "  'train_accounts': 590,\n",
       "  'test_accounts': 246},\n",
       " 105083: {'loss': 0.205,\n",
       "  'accuracy': 0.923,\n",
       "  'f1': 0.778,\n",
       "  'roc_auc': 0.949,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.7,\n",
       "  'train_size': 105083,\n",
       "  'test_size': 39755,\n",
       "  'train_accounts': 688,\n",
       "  'test_accounts': 246}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:45:05.405173Z",
     "start_time": "2025-11-16T13:45:05.400633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for k,vs in results.items():\n",
    "    print(k, vs['f1'])"
   ],
   "id": "e6e3668d76ab5be3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15026 0.688\n",
      "30336 0.709\n",
      "44605 0.753\n",
      "58439 0.761\n",
      "74028 0.77\n",
      "88909 0.767\n",
      "105083 0.778\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:40:20.323594700Z",
     "start_time": "2025-11-13T08:58:17.580274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # OLD results\n",
    "# {16704: {'loss': 0.262,\n",
    "#   'accuracy': 0.902,\n",
    "#   'f1': 0.715,\n",
    "#   'roc_auc': 0.926,\n",
    "#   'embedder.model_name': 'albert-base-v2',\n",
    "#   'train_frac': 0.1,\n",
    "#   'train_size': 16704,\n",
    "#   'test_size': 33464,\n",
    "#   'train_accounts': 98,\n",
    "#   'test_accounts': 246},\n",
    "#  29736: {'loss': 0.24,\n",
    "#   'accuracy': 0.913,\n",
    "#   'f1': 0.782,\n",
    "#   'roc_auc': 0.941,\n",
    "#   'embedder.model_name': 'albert-base-v2',\n",
    "#   'train_frac': 0.2,\n",
    "#   'train_size': 29736,\n",
    "#   'test_size': 33464,\n",
    "#   'train_accounts': 196,\n",
    "#   'test_accounts': 246},\n",
    "#  42410: {'loss': 0.227,\n",
    "#   'accuracy': 0.917,\n",
    "#   'f1': 0.781,\n",
    "#   'roc_auc': 0.94,\n",
    "#   'embedder.model_name': 'albert-base-v2',\n",
    "#   'train_frac': 0.3,\n",
    "#   'train_size': 42410,\n",
    "#   'test_size': 33464,\n",
    "#   'train_accounts': 295,\n",
    "#   'test_accounts': 246},\n",
    "#  59801: {'loss': 0.227,\n",
    "#   'accuracy': 0.917,\n",
    "#   'f1': 0.774,\n",
    "#   'roc_auc': 0.94,\n",
    "#   'embedder.model_name': 'albert-base-v2',\n",
    "#   'train_frac': 0.4,\n",
    "#   'train_size': 59801,\n",
    "#   'test_size': 33464,\n",
    "#   'train_accounts': 393,\n",
    "#   'test_accounts': 246},\n",
    "#  75623: {'loss': 0.21,\n",
    "#   'accuracy': 0.923,\n",
    "#   'f1': 0.795,\n",
    "#   'roc_auc': 0.95,\n",
    "#   'embedder.model_name': 'albert-base-v2',\n",
    "#   'train_frac': 0.5,\n",
    "#   'train_size': 75623,\n",
    "#   'test_size': 33464,\n",
    "#   'train_accounts': 492,\n",
    "#   'test_accounts': 246},\n",
    "#  92281: {'loss': 0.2,\n",
    "#   'accuracy': 0.926,\n",
    "#   'f1': 0.803,\n",
    "#   'roc_auc': 0.956,\n",
    "#   'embedder.model_name': 'albert-base-v2',\n",
    "#   'train_frac': 0.6,\n",
    "#   'train_size': 92281,\n",
    "#   'test_size': 33464,\n",
    "#   'train_accounts': 590,\n",
    "#   'test_accounts': 246},\n",
    "#  108248: {'loss': 0.203,\n",
    "#   'accuracy': 0.923,\n",
    "#   'f1': 0.803,\n",
    "#   'roc_auc': 0.955,\n",
    "#   'embedder.model_name': 'albert-base-v2',\n",
    "#   'train_frac': 0.7,\n",
    "#   'train_size': 108248,\n",
    "#   'test_size': 33464,\n",
    "#   'train_accounts': 688,\n",
    "#   'test_accounts': 246}}"
   ],
   "id": "992cb940d94cdeb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16704: {'loss': 0.262,\n",
       "  'accuracy': 0.902,\n",
       "  'f1': 0.715,\n",
       "  'roc_auc': 0.926,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.1,\n",
       "  'train_size': 16704,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 98,\n",
       "  'test_accounts': 246},\n",
       " 29736: {'loss': 0.24,\n",
       "  'accuracy': 0.913,\n",
       "  'f1': 0.782,\n",
       "  'roc_auc': 0.941,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.2,\n",
       "  'train_size': 29736,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 196,\n",
       "  'test_accounts': 246},\n",
       " 42410: {'loss': 0.227,\n",
       "  'accuracy': 0.917,\n",
       "  'f1': 0.781,\n",
       "  'roc_auc': 0.94,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.3,\n",
       "  'train_size': 42410,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 295,\n",
       "  'test_accounts': 246},\n",
       " 59801: {'loss': 0.227,\n",
       "  'accuracy': 0.917,\n",
       "  'f1': 0.774,\n",
       "  'roc_auc': 0.94,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.4,\n",
       "  'train_size': 59801,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 393,\n",
       "  'test_accounts': 246},\n",
       " 75623: {'loss': 0.21,\n",
       "  'accuracy': 0.923,\n",
       "  'f1': 0.795,\n",
       "  'roc_auc': 0.95,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.5,\n",
       "  'train_size': 75623,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 492,\n",
       "  'test_accounts': 246},\n",
       " 92281: {'loss': 0.2,\n",
       "  'accuracy': 0.926,\n",
       "  'f1': 0.803,\n",
       "  'roc_auc': 0.956,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.6,\n",
       "  'train_size': 92281,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 590,\n",
       "  'test_accounts': 246},\n",
       " 108248: {'loss': 0.203,\n",
       "  'accuracy': 0.923,\n",
       "  'f1': 0.803,\n",
       "  'roc_auc': 0.955,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.7,\n",
       "  'train_size': 108248,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 688,\n",
       "  'test_accounts': 246}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:40:20.323594700Z",
     "start_time": "2025-11-13T08:58:17.591904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "runner2 = ExpRunner.copy(runner1)\n",
    "runner2.feat_proc_params = feat_params_off\n",
    "results2 = runner2.run_training_set_size(fracs)\n"
   ],
   "id": "96f96304f4d9f7cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 10:58:17,599 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-13 10:58:17,664 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-13 10:58:17,665 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-13 10:58:17,671 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-13 10:58:17,674 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-13 10:58:17,684 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-13 10:58:17,685 - INFO - Creating new EmbeddingService(model_name=albert-base-v2)\n",
      "2025-11-13 10:58:17,685 - INFO - Loading embedding model: albert-base-v2...\n",
      "2025-11-13 10:58:19,116 - INFO - Model albert-base-v2 loaded onto cpu. Cache at cache/albert-base-v2\n",
      "2025-11-13 10:58:19,117 - INFO - Embedding 16704 train texts...\n",
      "2025-11-13 10:58:19,231 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:19,464 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:58:19,711 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:20,265 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-13 10:58:20,266 - INFO - Transforming 16704 rows...\n",
      "2025-11-13 10:58:20,267 - INFO - Transform complete.\n",
      "2025-11-13 10:58:20,268 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:58:20,270 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 10:58:20,289 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:58:21,329 - INFO - Epoch 1/10 [1.04s] | Train Loss: 0.5519 | Test Loss: 0.7776 | F1: 0.4835 | ROC-AUC: 0.8445\n",
      "2025-11-13 10:58:22,333 - INFO - Epoch 2/10 [1.00s] | Train Loss: 0.3910 | Test Loss: 0.3685 | F1: 0.7084 | ROC-AUC: 0.8962\n",
      "2025-11-13 10:58:23,330 - INFO - Epoch 3/10 [1.00s] | Train Loss: 0.3041 | Test Loss: 0.4789 | F1: 0.6200 | ROC-AUC: 0.8811\n",
      "2025-11-13 10:58:24,299 - INFO - Epoch 4/10 [0.97s] | Train Loss: 0.2751 | Test Loss: 0.3085 | F1: 0.4591 | ROC-AUC: 0.9203\n",
      "2025-11-13 10:58:25,258 - INFO - Epoch 5/10 [0.96s] | Train Loss: 0.2601 | Test Loss: 0.2836 | F1: 0.7429 | ROC-AUC: 0.9081\n",
      "2025-11-13 10:58:26,296 - INFO - Epoch 6/10 [1.04s] | Train Loss: 0.2535 | Test Loss: 0.3462 | F1: 0.7172 | ROC-AUC: 0.9163\n",
      "2025-11-13 10:58:27,580 - INFO - Epoch 7/10 [1.28s] | Train Loss: 0.2381 | Test Loss: 0.3469 | F1: 0.6958 | ROC-AUC: 0.9082\n",
      "2025-11-13 10:58:28,752 - INFO - Epoch 8/10 [1.17s] | Train Loss: 0.2303 | Test Loss: 0.3447 | F1: 0.4125 | ROC-AUC: 0.9164\n",
      "2025-11-13 10:58:29,758 - INFO - Epoch 9/10 [1.01s] | Train Loss: 0.2273 | Test Loss: 0.2650 | F1: 0.7325 | ROC-AUC: 0.9246\n",
      "2025-11-13 10:58:30,771 - INFO - Epoch 10/10 [1.01s] | Train Loss: 0.2268 | Test Loss: 0.2573 | F1: 0.7564 | ROC-AUC: 0.9283\n",
      "2025-11-13 10:58:30,771 - INFO - Training complete.\n",
      "2025-11-13 10:58:30,777 - INFO - {'loss': 0.257, 'accuracy': 0.906, 'f1': 0.756, 'roc_auc': 0.928, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-13 10:58:30,785 - INFO - Yielding 20% split: 196 accounts, 29736 rows\n",
      "2025-11-13 10:58:30,786 - INFO - Embedding 29736 train texts...\n",
      "2025-11-13 10:58:30,949 - INFO - len(text_list)=29736 len(unique_texts)=19347 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:31,330 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:58:31,566 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:32,133 - INFO - Fitting processor on 29736 rows...\n",
      "2025-11-13 10:58:32,134 - INFO - Transforming 29736 rows...\n",
      "2025-11-13 10:58:32,135 - INFO - Transform complete.\n",
      "2025-11-13 10:58:32,136 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:58:32,136 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 10:58:32,160 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:58:33,713 - INFO - Epoch 1/10 [1.55s] | Train Loss: 0.4994 | Test Loss: 0.4231 | F1: 0.6875 | ROC-AUC: 0.8898\n",
      "2025-11-13 10:58:35,197 - INFO - Epoch 2/10 [1.48s] | Train Loss: 0.3209 | Test Loss: 0.3152 | F1: 0.6954 | ROC-AUC: 0.8854\n",
      "2025-11-13 10:58:36,882 - INFO - Epoch 3/10 [1.68s] | Train Loss: 0.2751 | Test Loss: 0.2986 | F1: 0.5519 | ROC-AUC: 0.9150\n",
      "2025-11-13 10:58:38,399 - INFO - Epoch 4/10 [1.52s] | Train Loss: 0.2553 | Test Loss: 0.2661 | F1: 0.7161 | ROC-AUC: 0.9180\n",
      "2025-11-13 10:58:39,881 - INFO - Epoch 5/10 [1.48s] | Train Loss: 0.2420 | Test Loss: 0.3732 | F1: 0.4033 | ROC-AUC: 0.9143\n",
      "2025-11-13 10:58:41,366 - INFO - Epoch 6/10 [1.48s] | Train Loss: 0.2310 | Test Loss: 0.2657 | F1: 0.6913 | ROC-AUC: 0.9176\n",
      "2025-11-13 10:58:42,913 - INFO - Epoch 7/10 [1.55s] | Train Loss: 0.2249 | Test Loss: 0.2583 | F1: 0.7319 | ROC-AUC: 0.9331\n",
      "2025-11-13 10:58:44,574 - INFO - Epoch 8/10 [1.66s] | Train Loss: 0.2238 | Test Loss: 0.4175 | F1: 0.4920 | ROC-AUC: 0.8932\n",
      "2025-11-13 10:58:46,162 - INFO - Epoch 9/10 [1.59s] | Train Loss: 0.2206 | Test Loss: 0.2711 | F1: 0.6887 | ROC-AUC: 0.9176\n",
      "2025-11-13 10:58:47,817 - INFO - Epoch 10/10 [1.65s] | Train Loss: 0.2074 | Test Loss: 0.2422 | F1: 0.7592 | ROC-AUC: 0.9336\n",
      "2025-11-13 10:58:47,817 - INFO - Training complete.\n",
      "2025-11-13 10:58:47,820 - INFO - {'loss': 0.242, 'accuracy': 0.909, 'f1': 0.759, 'roc_auc': 0.934, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 29736, 'test_size': 33464, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-13 10:58:47,828 - INFO - Yielding 30% split: 295 accounts, 42410 rows\n",
      "2025-11-13 10:58:47,829 - INFO - Embedding 42410 train texts...\n",
      "2025-11-13 10:58:48,066 - INFO - len(text_list)=42410 len(unique_texts)=26700 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:48,783 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:58:49,032 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:58:49,617 - INFO - Fitting processor on 42410 rows...\n",
      "2025-11-13 10:58:49,618 - INFO - Transforming 42410 rows...\n",
      "2025-11-13 10:58:49,619 - INFO - Transform complete.\n",
      "2025-11-13 10:58:49,620 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:58:49,621 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 10:58:49,657 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:58:51,726 - INFO - Epoch 1/10 [2.07s] | Train Loss: 0.4718 | Test Loss: 0.3596 | F1: 0.6551 | ROC-AUC: 0.8748\n",
      "2025-11-13 10:58:53,660 - INFO - Epoch 2/10 [1.93s] | Train Loss: 0.3049 | Test Loss: 0.3403 | F1: 0.7034 | ROC-AUC: 0.8995\n",
      "2025-11-13 10:58:55,473 - INFO - Epoch 3/10 [1.81s] | Train Loss: 0.2757 | Test Loss: 0.2483 | F1: 0.7505 | ROC-AUC: 0.9281\n",
      "2025-11-13 10:58:57,277 - INFO - Epoch 4/10 [1.80s] | Train Loss: 0.2567 | Test Loss: 0.2492 | F1: 0.7466 | ROC-AUC: 0.9265\n",
      "2025-11-13 10:58:59,688 - INFO - Epoch 5/10 [2.41s] | Train Loss: 0.2455 | Test Loss: 0.2340 | F1: 0.7635 | ROC-AUC: 0.9359\n",
      "2025-11-13 10:59:02,026 - INFO - Epoch 6/10 [2.34s] | Train Loss: 0.2376 | Test Loss: 0.2447 | F1: 0.7266 | ROC-AUC: 0.9357\n",
      "2025-11-13 10:59:04,117 - INFO - Epoch 7/10 [2.09s] | Train Loss: 0.2322 | Test Loss: 0.2953 | F1: 0.7274 | ROC-AUC: 0.8989\n",
      "2025-11-13 10:59:06,510 - INFO - Epoch 8/10 [2.39s] | Train Loss: 0.2243 | Test Loss: 0.3228 | F1: 0.5202 | ROC-AUC: 0.9249\n",
      "2025-11-13 10:59:08,543 - INFO - Epoch 9/10 [2.03s] | Train Loss: 0.2243 | Test Loss: 0.2387 | F1: 0.7597 | ROC-AUC: 0.9323\n",
      "2025-11-13 10:59:10,789 - INFO - Epoch 10/10 [2.25s] | Train Loss: 0.2187 | Test Loss: 0.2396 | F1: 0.7624 | ROC-AUC: 0.9365\n",
      "2025-11-13 10:59:10,790 - INFO - Training complete.\n",
      "2025-11-13 10:59:10,793 - INFO - {'loss': 0.234, 'accuracy': 0.914, 'f1': 0.764, 'roc_auc': 0.936, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 42410, 'test_size': 33464, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-13 10:59:10,804 - INFO - Yielding 40% split: 393 accounts, 59801 rows\n",
      "2025-11-13 10:59:10,805 - INFO - Embedding 59801 train texts...\n",
      "2025-11-13 10:59:11,198 - INFO - len(text_list)=59801 len(unique_texts)=37409 len(texts_to_embed)=0\n",
      "2025-11-13 10:59:12,188 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:59:12,434 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:59:13,099 - INFO - Fitting processor on 59801 rows...\n",
      "2025-11-13 10:59:13,100 - INFO - Transforming 59801 rows...\n",
      "2025-11-13 10:59:13,101 - INFO - Transform complete.\n",
      "2025-11-13 10:59:13,101 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:59:13,102 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 10:59:13,136 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:59:15,508 - INFO - Epoch 1/10 [2.37s] | Train Loss: 0.4281 | Test Loss: 0.2996 | F1: 0.5646 | ROC-AUC: 0.9043\n",
      "2025-11-13 10:59:17,930 - INFO - Epoch 2/10 [2.42s] | Train Loss: 0.2929 | Test Loss: 0.3358 | F1: 0.5530 | ROC-AUC: 0.9075\n",
      "2025-11-13 10:59:20,321 - INFO - Epoch 3/10 [2.39s] | Train Loss: 0.2660 | Test Loss: 0.2701 | F1: 0.6958 | ROC-AUC: 0.9238\n",
      "2025-11-13 10:59:23,136 - INFO - Epoch 4/10 [2.81s] | Train Loss: 0.2537 | Test Loss: 0.2823 | F1: 0.7408 | ROC-AUC: 0.9200\n",
      "2025-11-13 10:59:25,923 - INFO - Epoch 5/10 [2.79s] | Train Loss: 0.2446 | Test Loss: 0.2437 | F1: 0.7255 | ROC-AUC: 0.9357\n",
      "2025-11-13 10:59:29,316 - INFO - Epoch 6/10 [3.39s] | Train Loss: 0.2355 | Test Loss: 0.2316 | F1: 0.7697 | ROC-AUC: 0.9387\n",
      "2025-11-13 10:59:33,570 - INFO - Epoch 7/10 [4.25s] | Train Loss: 0.2284 | Test Loss: 0.2400 | F1: 0.7272 | ROC-AUC: 0.9399\n",
      "2025-11-13 10:59:36,547 - INFO - Epoch 8/10 [2.98s] | Train Loss: 0.2241 | Test Loss: 0.2481 | F1: 0.6922 | ROC-AUC: 0.9401\n",
      "2025-11-13 10:59:39,521 - INFO - Epoch 9/10 [2.97s] | Train Loss: 0.2216 | Test Loss: 0.2336 | F1: 0.7724 | ROC-AUC: 0.9400\n",
      "2025-11-13 10:59:42,507 - INFO - Epoch 10/10 [2.99s] | Train Loss: 0.2167 | Test Loss: 0.2221 | F1: 0.7853 | ROC-AUC: 0.9456\n",
      "2025-11-13 10:59:42,508 - INFO - Training complete.\n",
      "2025-11-13 10:59:42,512 - INFO - {'loss': 0.222, 'accuracy': 0.919, 'f1': 0.785, 'roc_auc': 0.946, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 59801, 'test_size': 33464, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-13 10:59:42,522 - INFO - Yielding 50% split: 492 accounts, 75623 rows\n",
      "2025-11-13 10:59:42,524 - INFO - Embedding 75623 train texts...\n",
      "2025-11-13 10:59:43,007 - INFO - len(text_list)=75623 len(unique_texts)=46835 len(texts_to_embed)=0\n",
      "2025-11-13 10:59:44,297 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 10:59:44,553 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 10:59:45,124 - INFO - Fitting processor on 75623 rows...\n",
      "2025-11-13 10:59:45,125 - INFO - Transforming 75623 rows...\n",
      "2025-11-13 10:59:45,126 - INFO - Transform complete.\n",
      "2025-11-13 10:59:45,127 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 10:59:45,129 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 10:59:45,173 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 10:59:48,310 - INFO - Epoch 1/10 [3.14s] | Train Loss: 0.4044 | Test Loss: 0.3549 | F1: 0.5354 | ROC-AUC: 0.8700\n",
      "2025-11-13 10:59:51,506 - INFO - Epoch 2/10 [3.20s] | Train Loss: 0.2764 | Test Loss: 0.2933 | F1: 0.7370 | ROC-AUC: 0.9212\n",
      "2025-11-13 10:59:54,708 - INFO - Epoch 3/10 [3.20s] | Train Loss: 0.2516 | Test Loss: 0.2418 | F1: 0.7317 | ROC-AUC: 0.9347\n",
      "2025-11-13 10:59:58,422 - INFO - Epoch 4/10 [3.71s] | Train Loss: 0.2408 | Test Loss: 0.2620 | F1: 0.6396 | ROC-AUC: 0.9376\n",
      "2025-11-13 11:00:02,429 - INFO - Epoch 5/10 [4.01s] | Train Loss: 0.2339 | Test Loss: 0.2778 | F1: 0.6590 | ROC-AUC: 0.9332\n",
      "2025-11-13 11:00:07,464 - INFO - Epoch 6/10 [5.03s] | Train Loss: 0.2286 | Test Loss: 0.2470 | F1: 0.7216 | ROC-AUC: 0.9401\n",
      "2025-11-13 11:00:14,160 - INFO - Epoch 7/10 [6.69s] | Train Loss: 0.2232 | Test Loss: 0.2936 | F1: 0.7363 | ROC-AUC: 0.9161\n",
      "2025-11-13 11:00:21,012 - INFO - Epoch 8/10 [6.85s] | Train Loss: 0.2190 | Test Loss: 0.2213 | F1: 0.7815 | ROC-AUC: 0.9458\n",
      "2025-11-13 11:00:27,910 - INFO - Epoch 9/10 [6.90s] | Train Loss: 0.2158 | Test Loss: 0.2235 | F1: 0.7849 | ROC-AUC: 0.9462\n",
      "2025-11-13 11:00:33,126 - INFO - Epoch 10/10 [5.22s] | Train Loss: 0.2125 | Test Loss: 0.2303 | F1: 0.7525 | ROC-AUC: 0.9448\n",
      "2025-11-13 11:00:33,127 - INFO - Training complete.\n",
      "2025-11-13 11:00:33,132 - INFO - {'loss': 0.224, 'accuracy': 0.917, 'f1': 0.785, 'roc_auc': 0.946, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.5, 'train_size': 75623, 'test_size': 33464, 'train_accounts': 492, 'test_accounts': 246}\n",
      "2025-11-13 11:00:33,144 - INFO - Yielding 60% split: 590 accounts, 92281 rows\n",
      "2025-11-13 11:00:33,146 - INFO - Embedding 92281 train texts...\n",
      "2025-11-13 11:00:33,921 - INFO - len(text_list)=92281 len(unique_texts)=56280 len(texts_to_embed)=0\n",
      "2025-11-13 11:00:36,343 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 11:00:36,968 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 11:00:37,568 - INFO - Fitting processor on 92281 rows...\n",
      "2025-11-13 11:00:37,569 - INFO - Transforming 92281 rows...\n",
      "2025-11-13 11:00:37,570 - INFO - Transform complete.\n",
      "2025-11-13 11:00:37,571 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 11:00:37,572 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 11:00:37,623 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 11:00:44,295 - INFO - Epoch 1/10 [6.67s] | Train Loss: 0.3847 | Test Loss: 0.2477 | F1: 0.7456 | ROC-AUC: 0.9296\n",
      "2025-11-13 11:00:49,927 - INFO - Epoch 2/10 [5.63s] | Train Loss: 0.2721 | Test Loss: 0.2618 | F1: 0.6703 | ROC-AUC: 0.9237\n",
      "2025-11-13 11:00:55,697 - INFO - Epoch 3/10 [5.77s] | Train Loss: 0.2495 | Test Loss: 0.2544 | F1: 0.7756 | ROC-AUC: 0.9205\n",
      "2025-11-13 11:01:01,332 - INFO - Epoch 4/10 [5.63s] | Train Loss: 0.2394 | Test Loss: 0.2620 | F1: 0.6628 | ROC-AUC: 0.9362\n",
      "2025-11-13 11:01:07,512 - INFO - Epoch 5/10 [6.18s] | Train Loss: 0.2313 | Test Loss: 0.2257 | F1: 0.7882 | ROC-AUC: 0.9429\n",
      "2025-11-13 11:01:13,774 - INFO - Epoch 6/10 [6.26s] | Train Loss: 0.2268 | Test Loss: 0.2155 | F1: 0.7884 | ROC-AUC: 0.9465\n",
      "2025-11-13 11:01:19,062 - INFO - Epoch 7/10 [5.29s] | Train Loss: 0.2239 | Test Loss: 0.2299 | F1: 0.7604 | ROC-AUC: 0.9434\n",
      "2025-11-13 11:01:24,299 - INFO - Epoch 8/10 [5.24s] | Train Loss: 0.2194 | Test Loss: 0.2408 | F1: 0.7214 | ROC-AUC: 0.9367\n",
      "2025-11-13 11:01:29,505 - INFO - Epoch 9/10 [5.21s] | Train Loss: 0.2178 | Test Loss: 0.2151 | F1: 0.7951 | ROC-AUC: 0.9464\n",
      "2025-11-13 11:01:34,945 - INFO - Epoch 10/10 [5.44s] | Train Loss: 0.2150 | Test Loss: 0.4414 | F1: 0.6477 | ROC-AUC: 0.9197\n",
      "2025-11-13 11:01:34,946 - INFO - Training complete.\n",
      "2025-11-13 11:01:34,952 - INFO - {'loss': 0.215, 'accuracy': 0.924, 'f1': 0.795, 'roc_auc': 0.946, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.6, 'train_size': 92281, 'test_size': 33464, 'train_accounts': 590, 'test_accounts': 246}\n",
      "2025-11-13 11:01:34,966 - INFO - Yielding 70% split: 688 accounts, 108248 rows\n",
      "2025-11-13 11:01:34,968 - INFO - Embedding 108248 train texts...\n",
      "2025-11-13 11:01:35,863 - INFO - len(text_list)=108248 len(unique_texts)=65808 len(texts_to_embed)=0\n",
      "2025-11-13 11:01:38,133 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 11:01:38,363 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 11:01:38,986 - INFO - Fitting processor on 108248 rows...\n",
      "2025-11-13 11:01:38,987 - INFO - Transforming 108248 rows...\n",
      "2025-11-13 11:01:38,988 - INFO - Transform complete.\n",
      "2025-11-13 11:01:38,989 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 11:01:38,990 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 11:01:39,040 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 11:01:44,480 - INFO - Epoch 1/10 [5.44s] | Train Loss: 0.3689 | Test Loss: 0.2704 | F1: 0.7505 | ROC-AUC: 0.9055\n",
      "2025-11-13 11:01:49,833 - INFO - Epoch 2/10 [5.35s] | Train Loss: 0.2659 | Test Loss: 0.2843 | F1: 0.7481 | ROC-AUC: 0.9157\n",
      "2025-11-13 11:01:55,718 - INFO - Epoch 3/10 [5.88s] | Train Loss: 0.2449 | Test Loss: 0.2479 | F1: 0.7728 | ROC-AUC: 0.9368\n",
      "2025-11-13 11:02:01,673 - INFO - Epoch 4/10 [5.95s] | Train Loss: 0.2353 | Test Loss: 0.6923 | F1: 0.5421 | ROC-AUC: 0.8731\n",
      "2025-11-13 11:02:08,115 - INFO - Epoch 5/10 [6.44s] | Train Loss: 0.2268 | Test Loss: 0.2249 | F1: 0.7888 | ROC-AUC: 0.9426\n",
      "2025-11-13 11:02:14,202 - INFO - Epoch 6/10 [6.09s] | Train Loss: 0.2224 | Test Loss: 0.2778 | F1: 0.7422 | ROC-AUC: 0.9232\n",
      "2025-11-13 11:02:20,159 - INFO - Epoch 7/10 [5.96s] | Train Loss: 0.2198 | Test Loss: 0.2164 | F1: 0.7876 | ROC-AUC: 0.9452\n",
      "2025-11-13 11:02:26,110 - INFO - Epoch 8/10 [5.95s] | Train Loss: 0.2159 | Test Loss: 0.2119 | F1: 0.7822 | ROC-AUC: 0.9496\n",
      "2025-11-13 11:02:31,979 - INFO - Epoch 9/10 [5.87s] | Train Loss: 0.2117 | Test Loss: 0.2202 | F1: 0.7880 | ROC-AUC: 0.9477\n",
      "2025-11-13 11:02:38,372 - INFO - Epoch 10/10 [6.39s] | Train Loss: 0.2123 | Test Loss: 0.2286 | F1: 0.7836 | ROC-AUC: 0.9402\n",
      "2025-11-13 11:02:38,373 - INFO - Training complete.\n",
      "2025-11-13 11:02:38,379 - INFO - {'loss': 0.225, 'accuracy': 0.922, 'f1': 0.789, 'roc_auc': 0.943, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.7, 'train_size': 108248, 'test_size': 33464, 'train_accounts': 688, 'test_accounts': 246}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:40:20.323594700Z",
     "start_time": "2025-11-13T09:02:38.460750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# runner3 = ExpRunner.copy(runner1)\n",
    "# runner3.feat_proc_params = FeatProcParams(n_bins=20, k_top=50)\n",
    "# runner3.run_training_set_size([0.1])\n"
   ],
   "id": "a26f355bbf7e1888",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:40:20.323594700Z",
     "start_time": "2025-11-13T09:02:38.472669Z"
    }
   },
   "cell_type": "code",
   "source": "# results2 = runner2.run_training_set_size(fracs)\n",
   "id": "5b1cc1626d140f29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 11:02:38,482 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-13 11:02:38,590 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-13 11:02:38,591 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-13 11:02:38,600 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-13 11:02:38,604 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-13 11:02:38,623 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-13 11:02:38,625 - INFO - Embedding 16704 train texts...\n",
      "2025-11-13 11:02:38,733 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-13 11:02:38,993 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 11:02:39,312 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 11:02:39,959 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-13 11:02:39,959 - INFO - Transforming 16704 rows...\n",
      "2025-11-13 11:02:39,961 - INFO - Transform complete.\n",
      "2025-11-13 11:02:39,962 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 11:02:39,962 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 11:02:39,978 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 11:02:41,439 - INFO - Epoch 1/10 [1.46s] | Train Loss: 0.5708 | Test Loss: 0.3556 | F1: 0.5547 | ROC-AUC: 0.8603\n",
      "2025-11-13 11:02:42,786 - INFO - Epoch 2/10 [1.35s] | Train Loss: 0.4010 | Test Loss: 0.3121 | F1: 0.5623 | ROC-AUC: 0.9067\n",
      "2025-11-13 11:02:44,163 - INFO - Epoch 3/10 [1.38s] | Train Loss: 0.3129 | Test Loss: 0.3160 | F1: 0.6104 | ROC-AUC: 0.9060\n",
      "2025-11-13 11:02:46,242 - INFO - Epoch 4/10 [2.08s] | Train Loss: 0.2808 | Test Loss: 0.3521 | F1: 0.5347 | ROC-AUC: 0.8816\n",
      "2025-11-13 11:02:47,901 - INFO - Epoch 5/10 [1.66s] | Train Loss: 0.2688 | Test Loss: 0.4034 | F1: 0.6166 | ROC-AUC: 0.8600\n",
      "2025-11-13 11:02:49,722 - INFO - Epoch 6/10 [1.82s] | Train Loss: 0.2533 | Test Loss: 0.3691 | F1: 0.4167 | ROC-AUC: 0.9130\n",
      "2025-11-13 11:02:51,125 - INFO - Epoch 7/10 [1.40s] | Train Loss: 0.2423 | Test Loss: 0.3550 | F1: 0.5196 | ROC-AUC: 0.9240\n",
      "2025-11-13 11:02:52,691 - INFO - Epoch 8/10 [1.57s] | Train Loss: 0.2369 | Test Loss: 0.3674 | F1: 0.4905 | ROC-AUC: 0.9060\n",
      "2025-11-13 11:02:54,287 - INFO - Epoch 9/10 [1.59s] | Train Loss: 0.2284 | Test Loss: 0.2967 | F1: 0.5940 | ROC-AUC: 0.9211\n",
      "2025-11-13 11:02:55,629 - INFO - Epoch 10/10 [1.34s] | Train Loss: 0.2293 | Test Loss: 0.3218 | F1: 0.5582 | ROC-AUC: 0.9223\n",
      "2025-11-13 11:02:55,630 - INFO - Training complete.\n",
      "2025-11-13 11:02:55,653 - INFO - {'loss': 0.403, 'accuracy': 0.837, 'f1': 0.617, 'roc_auc': 0.86, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-13 11:02:55,666 - INFO - Yielding 20% split: 196 accounts, 29736 rows\n",
      "2025-11-13 11:02:55,667 - INFO - Embedding 29736 train texts...\n",
      "2025-11-13 11:02:55,815 - INFO - len(text_list)=29736 len(unique_texts)=19347 len(texts_to_embed)=0\n",
      "2025-11-13 11:02:56,229 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 11:02:56,465 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 11:02:57,067 - INFO - Fitting processor on 29736 rows...\n",
      "2025-11-13 11:02:57,068 - INFO - Transforming 29736 rows...\n",
      "2025-11-13 11:02:57,069 - INFO - Transform complete.\n",
      "2025-11-13 11:02:57,070 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 11:02:57,071 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 11:02:57,095 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 11:02:59,067 - INFO - Epoch 1/10 [1.97s] | Train Loss: 0.5145 | Test Loss: 0.3729 | F1: 0.7036 | ROC-AUC: 0.8844\n",
      "2025-11-13 11:03:01,018 - INFO - Epoch 2/10 [1.95s] | Train Loss: 0.3266 | Test Loss: 0.3683 | F1: 0.3295 | ROC-AUC: 0.9103\n",
      "2025-11-13 11:03:02,971 - INFO - Epoch 3/10 [1.95s] | Train Loss: 0.2775 | Test Loss: 0.3826 | F1: 0.6853 | ROC-AUC: 0.8933\n",
      "2025-11-13 11:03:04,907 - INFO - Epoch 4/10 [1.94s] | Train Loss: 0.2604 | Test Loss: 0.2470 | F1: 0.7298 | ROC-AUC: 0.9306\n",
      "2025-11-13 11:03:06,992 - INFO - Epoch 5/10 [2.08s] | Train Loss: 0.2469 | Test Loss: 0.2616 | F1: 0.6976 | ROC-AUC: 0.9200\n",
      "2025-11-13 11:03:08,889 - INFO - Epoch 6/10 [1.90s] | Train Loss: 0.2412 | Test Loss: 0.2707 | F1: 0.7441 | ROC-AUC: 0.9115\n",
      "2025-11-13 11:03:10,896 - INFO - Epoch 7/10 [2.01s] | Train Loss: 0.2282 | Test Loss: 0.2649 | F1: 0.6617 | ROC-AUC: 0.9356\n",
      "2025-11-13 11:03:13,016 - INFO - Epoch 8/10 [2.12s] | Train Loss: 0.2252 | Test Loss: 0.2498 | F1: 0.7680 | ROC-AUC: 0.9224\n",
      "2025-11-13 11:03:15,112 - INFO - Epoch 9/10 [2.10s] | Train Loss: 0.2225 | Test Loss: 0.2351 | F1: 0.7680 | ROC-AUC: 0.9385\n",
      "2025-11-13 11:03:17,182 - INFO - Epoch 10/10 [2.07s] | Train Loss: 0.2151 | Test Loss: 0.2338 | F1: 0.7630 | ROC-AUC: 0.9367\n",
      "2025-11-13 11:03:17,183 - INFO - Training complete.\n",
      "2025-11-13 11:03:17,186 - INFO - {'loss': 0.235, 'accuracy': 0.912, 'f1': 0.768, 'roc_auc': 0.938, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 29736, 'test_size': 33464, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-13 11:03:17,193 - INFO - Yielding 30% split: 295 accounts, 42410 rows\n",
      "2025-11-13 11:03:17,194 - INFO - Embedding 42410 train texts...\n",
      "2025-11-13 11:03:17,470 - INFO - len(text_list)=42410 len(unique_texts)=26700 len(texts_to_embed)=0\n",
      "2025-11-13 11:03:18,170 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 11:03:18,399 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 11:03:18,915 - INFO - Fitting processor on 42410 rows...\n",
      "2025-11-13 11:03:18,915 - INFO - Transforming 42410 rows...\n",
      "2025-11-13 11:03:18,916 - INFO - Transform complete.\n",
      "2025-11-13 11:03:18,917 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 11:03:18,918 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 11:03:18,946 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 11:03:21,389 - INFO - Epoch 1/10 [2.44s] | Train Loss: 0.4637 | Test Loss: 0.3525 | F1: 0.7204 | ROC-AUC: 0.9023\n",
      "2025-11-13 11:03:23,802 - INFO - Epoch 2/10 [2.41s] | Train Loss: 0.3001 | Test Loss: 0.2827 | F1: 0.6648 | ROC-AUC: 0.9099\n",
      "2025-11-13 11:03:26,311 - INFO - Epoch 3/10 [2.51s] | Train Loss: 0.2657 | Test Loss: 0.2880 | F1: 0.7394 | ROC-AUC: 0.9034\n",
      "2025-11-13 11:03:28,827 - INFO - Epoch 4/10 [2.52s] | Train Loss: 0.2512 | Test Loss: 0.2565 | F1: 0.6918 | ROC-AUC: 0.9342\n",
      "2025-11-13 11:03:31,530 - INFO - Epoch 5/10 [2.70s] | Train Loss: 0.2411 | Test Loss: 0.2689 | F1: 0.6742 | ROC-AUC: 0.9358\n",
      "2025-11-13 11:03:34,272 - INFO - Epoch 6/10 [2.74s] | Train Loss: 0.2375 | Test Loss: 0.2642 | F1: 0.7582 | ROC-AUC: 0.9193\n",
      "2025-11-13 11:03:37,076 - INFO - Epoch 7/10 [2.80s] | Train Loss: 0.2339 | Test Loss: 0.2463 | F1: 0.7395 | ROC-AUC: 0.9407\n",
      "2025-11-13 11:03:39,834 - INFO - Epoch 8/10 [2.76s] | Train Loss: 0.2272 | Test Loss: 0.2491 | F1: 0.7654 | ROC-AUC: 0.9292\n",
      "2025-11-13 11:03:42,573 - INFO - Epoch 9/10 [2.74s] | Train Loss: 0.2215 | Test Loss: 0.2438 | F1: 0.7489 | ROC-AUC: 0.9412\n",
      "2025-11-13 11:03:45,260 - INFO - Epoch 10/10 [2.69s] | Train Loss: 0.2182 | Test Loss: 0.2487 | F1: 0.7533 | ROC-AUC: 0.9268\n",
      "2025-11-13 11:03:45,261 - INFO - Training complete.\n",
      "2025-11-13 11:03:45,264 - INFO - {'loss': 0.249, 'accuracy': 0.91, 'f1': 0.765, 'roc_auc': 0.929, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 42410, 'test_size': 33464, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-13 11:03:45,274 - INFO - Yielding 40% split: 393 accounts, 59801 rows\n",
      "2025-11-13 11:03:45,275 - INFO - Embedding 59801 train texts...\n",
      "2025-11-13 11:03:45,622 - INFO - len(text_list)=59801 len(unique_texts)=37409 len(texts_to_embed)=0\n",
      "2025-11-13 11:03:46,536 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 11:03:46,779 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 11:03:47,577 - INFO - Fitting processor on 59801 rows...\n",
      "2025-11-13 11:03:47,578 - INFO - Transforming 59801 rows...\n",
      "2025-11-13 11:03:47,579 - INFO - Transform complete.\n",
      "2025-11-13 11:03:47,579 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 11:03:47,581 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 11:03:47,619 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 11:03:51,278 - INFO - Epoch 1/10 [3.66s] | Train Loss: 0.4382 | Test Loss: 0.4402 | F1: 0.1552 | ROC-AUC: 0.8973\n",
      "2025-11-13 11:03:54,543 - INFO - Epoch 2/10 [3.26s] | Train Loss: 0.2893 | Test Loss: 0.2586 | F1: 0.7353 | ROC-AUC: 0.9266\n",
      "2025-11-13 11:03:57,773 - INFO - Epoch 3/10 [3.23s] | Train Loss: 0.2675 | Test Loss: 0.2443 | F1: 0.7603 | ROC-AUC: 0.9269\n",
      "2025-11-13 11:04:01,414 - INFO - Epoch 4/10 [3.64s] | Train Loss: 0.2518 | Test Loss: 0.2337 | F1: 0.7709 | ROC-AUC: 0.9354\n",
      "2025-11-13 11:04:04,891 - INFO - Epoch 5/10 [3.48s] | Train Loss: 0.2440 | Test Loss: 0.2882 | F1: 0.7377 | ROC-AUC: 0.9102\n",
      "2025-11-13 11:04:08,736 - INFO - Epoch 6/10 [3.84s] | Train Loss: 0.2374 | Test Loss: 0.2412 | F1: 0.7595 | ROC-AUC: 0.9335\n",
      "2025-11-13 11:04:12,348 - INFO - Epoch 7/10 [3.61s] | Train Loss: 0.2306 | Test Loss: 0.2193 | F1: 0.7851 | ROC-AUC: 0.9440\n",
      "2025-11-13 11:04:15,817 - INFO - Epoch 8/10 [3.47s] | Train Loss: 0.2246 | Test Loss: 0.2261 | F1: 0.7731 | ROC-AUC: 0.9413\n",
      "2025-11-13 11:04:19,426 - INFO - Epoch 9/10 [3.61s] | Train Loss: 0.2217 | Test Loss: 0.2262 | F1: 0.7850 | ROC-AUC: 0.9430\n",
      "2025-11-13 11:04:22,927 - INFO - Epoch 10/10 [3.50s] | Train Loss: 0.2169 | Test Loss: 0.2151 | F1: 0.7848 | ROC-AUC: 0.9474\n",
      "2025-11-13 11:04:22,928 - INFO - Training complete.\n",
      "2025-11-13 11:04:22,933 - INFO - {'loss': 0.219, 'accuracy': 0.922, 'f1': 0.785, 'roc_auc': 0.944, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 59801, 'test_size': 33464, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-13 11:04:22,943 - INFO - Yielding 50% split: 492 accounts, 75623 rows\n",
      "2025-11-13 11:04:22,944 - INFO - Embedding 75623 train texts...\n",
      "2025-11-13 11:04:23,404 - INFO - len(text_list)=75623 len(unique_texts)=46835 len(texts_to_embed)=0\n",
      "2025-11-13 11:04:24,597 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 11:04:24,829 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 11:04:25,356 - INFO - Fitting processor on 75623 rows...\n",
      "2025-11-13 11:04:25,357 - INFO - Transforming 75623 rows...\n",
      "2025-11-13 11:04:25,357 - INFO - Transform complete.\n",
      "2025-11-13 11:04:25,358 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 11:04:25,359 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 11:04:25,396 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 11:04:29,447 - INFO - Epoch 1/10 [4.05s] | Train Loss: 0.4077 | Test Loss: 0.3378 | F1: 0.7045 | ROC-AUC: 0.9065\n",
      "2025-11-13 11:04:33,812 - INFO - Epoch 2/10 [4.36s] | Train Loss: 0.2776 | Test Loss: 0.2398 | F1: 0.7761 | ROC-AUC: 0.9348\n",
      "2025-11-13 11:04:37,902 - INFO - Epoch 3/10 [4.09s] | Train Loss: 0.2523 | Test Loss: 0.2617 | F1: 0.7655 | ROC-AUC: 0.9289\n",
      "2025-11-13 11:04:42,098 - INFO - Epoch 4/10 [4.20s] | Train Loss: 0.2403 | Test Loss: 0.2357 | F1: 0.7650 | ROC-AUC: 0.9358\n",
      "2025-11-13 11:04:46,332 - INFO - Epoch 5/10 [4.23s] | Train Loss: 0.2344 | Test Loss: 0.2501 | F1: 0.7771 | ROC-AUC: 0.9328\n",
      "2025-11-13 11:04:50,669 - INFO - Epoch 6/10 [4.34s] | Train Loss: 0.2266 | Test Loss: 0.2279 | F1: 0.7697 | ROC-AUC: 0.9447\n",
      "2025-11-13 11:04:55,311 - INFO - Epoch 7/10 [4.64s] | Train Loss: 0.2228 | Test Loss: 0.2522 | F1: 0.6969 | ROC-AUC: 0.9373\n",
      "2025-11-13 11:04:59,619 - INFO - Epoch 8/10 [4.31s] | Train Loss: 0.2177 | Test Loss: 0.2267 | F1: 0.7821 | ROC-AUC: 0.9422\n",
      "2025-11-13 11:05:04,009 - INFO - Epoch 9/10 [4.39s] | Train Loss: 0.2154 | Test Loss: 0.2232 | F1: 0.7790 | ROC-AUC: 0.9409\n",
      "2025-11-13 11:05:08,798 - INFO - Epoch 10/10 [4.79s] | Train Loss: 0.2102 | Test Loss: 0.2211 | F1: 0.7789 | ROC-AUC: 0.9490\n",
      "2025-11-13 11:05:08,799 - INFO - Training complete.\n",
      "2025-11-13 11:05:08,805 - INFO - {'loss': 0.227, 'accuracy': 0.917, 'f1': 0.782, 'roc_auc': 0.942, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.5, 'train_size': 75623, 'test_size': 33464, 'train_accounts': 492, 'test_accounts': 246}\n",
      "2025-11-13 11:05:08,818 - INFO - Yielding 60% split: 590 accounts, 92281 rows\n",
      "2025-11-13 11:05:08,820 - INFO - Embedding 92281 train texts...\n",
      "2025-11-13 11:05:09,563 - INFO - len(text_list)=92281 len(unique_texts)=56280 len(texts_to_embed)=0\n",
      "2025-11-13 11:05:11,442 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 11:05:11,744 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 11:05:12,365 - INFO - Fitting processor on 92281 rows...\n",
      "2025-11-13 11:05:12,366 - INFO - Transforming 92281 rows...\n",
      "2025-11-13 11:05:12,367 - INFO - Transform complete.\n",
      "2025-11-13 11:05:12,368 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 11:05:12,369 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 11:05:12,416 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 11:05:18,980 - INFO - Epoch 1/10 [6.56s] | Train Loss: 0.3840 | Test Loss: 0.4623 | F1: 0.2140 | ROC-AUC: 0.9022\n",
      "2025-11-13 11:05:25,488 - INFO - Epoch 2/10 [6.51s] | Train Loss: 0.2702 | Test Loss: 0.2595 | F1: 0.6872 | ROC-AUC: 0.9306\n",
      "2025-11-13 11:05:30,470 - INFO - Epoch 3/10 [4.98s] | Train Loss: 0.2498 | Test Loss: 0.2507 | F1: 0.7515 | ROC-AUC: 0.9282\n",
      "2025-11-13 11:05:35,444 - INFO - Epoch 4/10 [4.97s] | Train Loss: 0.2410 | Test Loss: 0.2277 | F1: 0.7794 | ROC-AUC: 0.9379\n",
      "2025-11-13 11:05:40,531 - INFO - Epoch 5/10 [5.09s] | Train Loss: 0.2318 | Test Loss: 0.2559 | F1: 0.7468 | ROC-AUC: 0.9275\n",
      "2025-11-13 11:05:45,775 - INFO - Epoch 6/10 [5.24s] | Train Loss: 0.2270 | Test Loss: 0.2567 | F1: 0.6867 | ROC-AUC: 0.9265\n",
      "2025-11-13 11:05:50,962 - INFO - Epoch 7/10 [5.18s] | Train Loss: 0.2235 | Test Loss: 0.2245 | F1: 0.7518 | ROC-AUC: 0.9453\n",
      "2025-11-13 11:05:56,424 - INFO - Epoch 8/10 [5.46s] | Train Loss: 0.2190 | Test Loss: 0.2099 | F1: 0.7821 | ROC-AUC: 0.9501\n",
      "2025-11-13 11:06:01,711 - INFO - Epoch 9/10 [5.29s] | Train Loss: 0.2171 | Test Loss: 0.2191 | F1: 0.7809 | ROC-AUC: 0.9452\n",
      "2025-11-13 11:06:07,430 - INFO - Epoch 10/10 [5.72s] | Train Loss: 0.2147 | Test Loss: 0.2219 | F1: 0.7833 | ROC-AUC: 0.9412\n",
      "2025-11-13 11:06:07,431 - INFO - Training complete.\n",
      "2025-11-13 11:06:07,436 - INFO - {'loss': 0.222, 'accuracy': 0.921, 'f1': 0.783, 'roc_auc': 0.941, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.6, 'train_size': 92281, 'test_size': 33464, 'train_accounts': 590, 'test_accounts': 246}\n",
      "2025-11-13 11:06:07,454 - INFO - Yielding 70% split: 688 accounts, 108248 rows\n",
      "2025-11-13 11:06:07,457 - INFO - Embedding 108248 train texts...\n",
      "2025-11-13 11:06:08,125 - INFO - len(text_list)=108248 len(unique_texts)=65808 len(texts_to_embed)=0\n",
      "2025-11-13 11:06:10,030 - INFO - Embedding 33464 test texts...\n",
      "2025-11-13 11:06:10,355 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-13 11:06:10,962 - INFO - Fitting processor on 108248 rows...\n",
      "2025-11-13 11:06:10,963 - INFO - Transforming 108248 rows...\n",
      "2025-11-13 11:06:10,963 - INFO - Transform complete.\n",
      "2025-11-13 11:06:10,964 - INFO - Transforming 33464 rows...\n",
      "2025-11-13 11:06:10,966 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-13 11:06:11,011 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-13 11:06:16,202 - INFO - Epoch 1/10 [5.19s] | Train Loss: 0.3692 | Test Loss: 0.2668 | F1: 0.7633 | ROC-AUC: 0.9240\n",
      "2025-11-13 11:06:21,663 - INFO - Epoch 2/10 [5.46s] | Train Loss: 0.2626 | Test Loss: 0.2453 | F1: 0.7590 | ROC-AUC: 0.9349\n",
      "2025-11-13 11:06:27,980 - INFO - Epoch 3/10 [6.31s] | Train Loss: 0.2455 | Test Loss: 0.3051 | F1: 0.6297 | ROC-AUC: 0.9200\n",
      "2025-11-13 11:06:34,103 - INFO - Epoch 4/10 [6.12s] | Train Loss: 0.2315 | Test Loss: 0.2403 | F1: 0.7794 | ROC-AUC: 0.9350\n",
      "2025-11-13 11:06:40,060 - INFO - Epoch 5/10 [5.96s] | Train Loss: 0.2267 | Test Loss: 0.2229 | F1: 0.7801 | ROC-AUC: 0.9456\n",
      "2025-11-13 11:06:46,104 - INFO - Epoch 6/10 [6.04s] | Train Loss: 0.2221 | Test Loss: 0.2224 | F1: 0.7878 | ROC-AUC: 0.9464\n",
      "2025-11-13 11:06:52,264 - INFO - Epoch 7/10 [6.16s] | Train Loss: 0.2178 | Test Loss: 0.2091 | F1: 0.7839 | ROC-AUC: 0.9509\n",
      "2025-11-13 11:06:58,541 - INFO - Epoch 8/10 [6.28s] | Train Loss: 0.2166 | Test Loss: 0.2718 | F1: 0.7202 | ROC-AUC: 0.9411\n",
      "2025-11-13 11:07:04,706 - INFO - Epoch 9/10 [6.16s] | Train Loss: 0.2123 | Test Loss: 0.2319 | F1: 0.7704 | ROC-AUC: 0.9478\n",
      "2025-11-13 11:07:11,155 - INFO - Epoch 10/10 [6.45s] | Train Loss: 0.2091 | Test Loss: 0.2303 | F1: 0.7785 | ROC-AUC: 0.9444\n",
      "2025-11-13 11:07:11,156 - INFO - Training complete.\n",
      "2025-11-13 11:07:11,163 - INFO - {'loss': 0.222, 'accuracy': 0.919, 'f1': 0.788, 'roc_auc': 0.946, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.7, 'train_size': 108248, 'test_size': 33464, 'train_accounts': 688, 'test_accounts': 246}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T13:40:20.323594700Z",
     "start_time": "2025-11-13T12:38:07.164447Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Import named \"numpy\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"numpy\" was resolved to \"numpy:2.3.4\" package (https://pypi.org/project/numpy/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "WARNING: Import named \"pandas\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"pandas\" was resolved to \"pandas:2.3.3\" package (https://pypi.org/project/pandas/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "WARNING: Import named \"scikit_learn\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"scikit_learn\" was resolved to \"scikit-learn:1.7.2\" package (https://pypi.org/project/scikit-learn/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "INFO: Successfully saved requirements file in .\\requirements.txt\n"
     ]
    }
   ],
   "execution_count": 12,
   "source": "#!pipreqs  .",
   "id": "5806038350ac3907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "496d0af5c8a03e63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
