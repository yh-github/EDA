{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:38:59.719928Z",
     "start_time": "2025-11-12T14:38:59.716573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure logging\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[stdout_handler]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def flush_logger():\n",
    "    stdout_handler.flush()\n",
    "\n"
   ],
   "id": "e5ac7a92777dcdbf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-12T14:39:01.147010Z",
     "start_time": "2025-11-12T14:38:59.727199Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from config import FieldConfig, ExperimentConfig\n",
    "from pathlib import Path\n",
    "DATA_PATH = Path('C:/Work/Data/proc/')\n",
    "field_config = FieldConfig()\n",
    "\n",
    "df = pd.read_csv(DATA_PATH/'rec_data2.csv')\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "df_cleaned = df.dropna(subset=[field_config.date, field_config.amount, field_config.text, field_config.label])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "id                    0\n",
      "accountId             0\n",
      "date                  0\n",
      "amount                0\n",
      "bankRawDescription    0\n",
      "isRecurring           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:39:01.213582Z",
     "start_time": "2025-11-12T14:39:01.209026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from feature_processor import FeatProcParams\n",
    "from config import *\n",
    "\n",
    "feat_params = FeatProcParams()\n",
    "feat_params_off = FeatProcParams(\n",
    "    use_cyclical_dates=False,\n",
    "    use_categorical_dates=False,\n",
    "    use_continuous_amount=False,\n",
    "    use_categorical_amount=False\n",
    ")\n",
    "\n",
    "exp_config = ExperimentConfig()"
   ],
   "id": "64ad44e449b7478f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:39:03.260479Z",
     "start_time": "2025-11-12T14:39:01.220003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from runner import ExpRunner\n",
    "from config import EmbModel\n",
    "from embedder import EmbeddingService\n",
    "\n",
    "runner1 = ExpRunner.create(\n",
    "    exp_params=exp_config,\n",
    "    full_df=df_cleaned,\n",
    "    emb_params=EmbeddingService.Params(model_name=EmbModel.ALBERT),\n",
    "    feat_proc_params=feat_params,\n",
    "    field_config=FieldConfig()\n",
    ")"
   ],
   "id": "d651a17edece86d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoav.haimovitch\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:40:46.969785Z",
     "start_time": "2025-11-12T14:39:03.319638Z"
    }
   },
   "cell_type": "code",
   "source": "runner1.run_torch([0.1])",
   "id": "97b17b877b9520a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12 16:39:03,326 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-12 16:39:03,415 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-12 16:39:03,416 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-12 16:39:03,426 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-12 16:39:03,431 - INFO - Preparing to create 1 training set fractions...\n",
      "2025-11-12 16:39:03,452 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-12 16:39:03,453 - INFO - Creating new EmbeddingService(model_name=albert-base-v2)\n",
      "2025-11-12 16:39:03,454 - INFO - Loading embedding model: albert-base-v2...\n",
      "2025-11-12 16:39:05,117 - INFO - Model albert-base-v2 loaded onto cpu. Cache at cache/albert-base-v2\n",
      "2025-11-12 16:39:06,293 - INFO - len(text_list)=156523 len(unique_texts)=94627 len(texts_to_embed)=0\n",
      "2025-11-12 16:39:09,769 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 16:39:10,448 - INFO - Fitting processor on 156523 rows...\n",
      "2025-11-12 16:39:10,449 - INFO - Fitting categorical amount features...\n",
      "2025-11-12 16:39:10,460 - INFO - --- Magic Number Cents Analysis (Top 10) ---\n",
      "2025-11-12 16:39:10,462 - INFO -   0: 291\n",
      "2025-11-12 16:39:10,463 - INFO -   95: 48\n",
      "2025-11-12 16:39:10,464 - INFO -   99: 32\n",
      "2025-11-12 16:39:10,465 - INFO -   50: 23\n",
      "2025-11-12 16:39:10,465 - INFO -   25: 9\n",
      "2025-11-12 16:39:10,466 - INFO -   75: 5\n",
      "2025-11-12 16:39:10,466 - INFO -   10: 5\n",
      "2025-11-12 16:39:10,467 - INFO -   29: 5\n",
      "2025-11-12 16:39:10,468 - INFO -   98: 5\n",
      "2025-11-12 16:39:10,469 - INFO -   49: 5\n",
      "2025-11-12 16:39:10,494 - INFO - Fit complete. Found 500 magic numbers.\n",
      "2025-11-12 16:39:10,494 - INFO - Created 100 fallback bins.\n",
      "2025-11-12 16:39:10,496 - INFO - Total Amount Vocabulary size: 602\n",
      "2025-11-12 16:39:10,497 - INFO - Transforming 156523 rows...\n",
      "2025-11-12 16:39:11,054 - INFO - Transform complete.\n",
      "2025-11-12 16:39:11,055 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 16:39:11,177 - INFO - Transform complete.\n",
      "Created 3 embedding layers. Total categorical embed dim: 112\n",
      "Total input dim to MLP: 888\n",
      "2025-11-12 16:39:11,257 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 16:39:19,552 - INFO - Epoch 1/10 [8.29s] | Train Loss: 0.3326 | Test Loss: 0.2425 | F1: 0.7571 | ROC-AUC: 0.9319\n",
      "2025-11-12 16:39:28,224 - INFO - Epoch 2/10 [8.67s] | Train Loss: 0.2390 | Test Loss: 0.2442 | F1: 0.6962 | ROC-AUC: 0.9407\n",
      "2025-11-12 16:39:37,134 - INFO - Epoch 3/10 [8.91s] | Train Loss: 0.2225 | Test Loss: 0.2202 | F1: 0.7635 | ROC-AUC: 0.9445\n",
      "2025-11-12 16:39:46,240 - INFO - Epoch 4/10 [9.11s] | Train Loss: 0.2140 | Test Loss: 0.2204 | F1: 0.7547 | ROC-AUC: 0.9474\n",
      "2025-11-12 16:39:55,466 - INFO - Epoch 5/10 [9.22s] | Train Loss: 0.2079 | Test Loss: 0.2038 | F1: 0.7781 | ROC-AUC: 0.9555\n",
      "2025-11-12 16:40:05,617 - INFO - Epoch 6/10 [10.15s] | Train Loss: 0.2031 | Test Loss: 0.2346 | F1: 0.7409 | ROC-AUC: 0.9482\n",
      "2025-11-12 16:40:15,706 - INFO - Epoch 7/10 [10.09s] | Train Loss: 0.1990 | Test Loss: 0.2075 | F1: 0.7855 | ROC-AUC: 0.9548\n",
      "2025-11-12 16:40:26,583 - INFO - Epoch 8/10 [10.88s] | Train Loss: 0.1961 | Test Loss: 0.2031 | F1: 0.8042 | ROC-AUC: 0.9536\n",
      "2025-11-12 16:40:36,958 - INFO - Epoch 9/10 [10.37s] | Train Loss: 0.1942 | Test Loss: 0.2217 | F1: 0.7553 | ROC-AUC: 0.9469\n",
      "2025-11-12 16:40:46,928 - INFO - Epoch 10/10 [9.97s] | Train Loss: 0.1915 | Test Loss: 0.2199 | F1: 0.7760 | ROC-AUC: 0.9499\n",
      "2025-11-12 16:40:46,929 - INFO - Training complete.\n",
      "2025-11-12 16:40:46,932 - INFO - {'loss': 0.203, 'accuracy': 0.926, 'f1': 0.804, 'roc_auc': 0.954, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{16704: {'loss': 0.203,\n",
       "  'accuracy': 0.926,\n",
       "  'f1': 0.804,\n",
       "  'roc_auc': 0.954,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.1,\n",
       "  'train_size': 16704,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 98,\n",
       "  'test_accounts': 246}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:42:22.182431Z",
     "start_time": "2025-11-12T14:42:15.844391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "runner2 = ExpRunner.copy(runner1)\n",
    "runner2.feat_proc_params = feat_params_off\n",
    "runner2.run_torch([0.1])\n"
   ],
   "id": "96f96304f4d9f7cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12 16:42:15,853 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-12 16:42:15,927 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-12 16:42:15,928 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-12 16:42:15,934 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-12 16:42:15,939 - INFO - Preparing to create 1 training set fractions...\n",
      "2025-11-12 16:42:15,952 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-12 16:42:15,953 - INFO - Creating new EmbeddingService(model_name=albert-base-v2)\n",
      "2025-11-12 16:42:15,954 - INFO - Loading embedding model: albert-base-v2...\n",
      "2025-11-12 16:42:16,760 - INFO - Model albert-base-v2 loaded onto cpu. Cache at cache/albert-base-v2\n",
      "2025-11-12 16:42:17,866 - INFO - len(text_list)=156523 len(unique_texts)=94627 len(texts_to_embed)=0\n",
      "2025-11-12 16:42:21,157 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 16:42:21,752 - INFO - Fitting processor on 156523 rows...\n",
      "2025-11-12 16:42:21,753 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 16:42:21,753 - INFO - Transforming 156523 rows...\n",
      "2025-11-12 16:42:21,754 - INFO - Transform complete.\n",
      "2025-11-12 16:42:21,754 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 16:42:21,755 - INFO - Transform complete.\n",
      "Created 3 embedding layers. Total categorical embed dim: 112\n",
      "Total input dim to MLP: 880\n",
      "2025-11-12 16:42:21,832 - INFO - Starting PyTorch training on cpu for 10 epochs...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m runner2 = ExpRunner.copy(runner1)\n\u001B[32m      2\u001B[39m runner2.feat_proc_params = feat_params_off\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mrunner2\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_torch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\src\\runner.py:443\u001B[39m, in \u001B[36mExpRunner.run_torch\u001B[39m\u001B[34m(self, fractions)\u001B[39m\n\u001B[32m    441\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m frac, sub_train_df \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.create_learning_curve_splits(df_train, fractions):\n\u001B[32m    442\u001B[39m     train_feature_set, test_feature_set, processor = \u001B[38;5;28mself\u001B[39m.build_data_for_pytorch(df_train, df_test)\n\u001B[32m--> \u001B[39m\u001B[32m443\u001B[39m     res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_experiment_pytorch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_feature_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_feature_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    444\u001B[39m     d = {\n\u001B[32m    445\u001B[39m         **res,\n\u001B[32m    446\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mtrain_frac\u001B[39m\u001B[33m\"\u001B[39m: r(frac),\n\u001B[32m   (...)\u001B[39m\u001B[32m    450\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mtest_accounts\u001B[39m\u001B[33m\"\u001B[39m: df_test[\u001B[38;5;28mself\u001B[39m.field_config.accountId].nunique()\n\u001B[32m    451\u001B[39m     }\n\u001B[32m    452\u001B[39m     logger.info(d)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\src\\runner.py:413\u001B[39m, in \u001B[36mExpRunner.run_experiment_pytorch\u001B[39m\u001B[34m(self, train_features, test_features, processor)\u001B[39m\n\u001B[32m    410\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m, NUM_EPOCHS + \u001B[32m1\u001B[39m):\n\u001B[32m    411\u001B[39m     start_time = time.time()\n\u001B[32m--> \u001B[39m\u001B[32m413\u001B[39m     train_loss = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    414\u001B[39m     metrics = evaluate_model(model, test_loader, criterion, DEVICE)\n\u001B[32m    416\u001B[39m     epoch_time = time.time() - start_time\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\src\\classifier.py:125\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, optimizer, criterion, device)\u001B[39m\n\u001B[32m    121\u001B[39m y_true = batch.y.to(device)\n\u001B[32m    122\u001B[39m \u001B[38;5;66;03m# -----------------------------\u001B[39;00m\n\u001B[32m    123\u001B[39m \n\u001B[32m    124\u001B[39m \u001B[38;5;66;03m# 1. Forward pass\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m logits = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_continuous\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_categorical\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    127\u001B[39m \u001B[38;5;66;03m# 2. Calculate loss\u001B[39;00m\n\u001B[32m    128\u001B[39m loss = criterion(logits, y_true)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\src\\classifier.py:82\u001B[39m, in \u001B[36mHybridModel.forward\u001B[39m\u001B[34m(self, x_text, x_continuous, x_categorical)\u001B[39m\n\u001B[32m     79\u001B[39m embedded_cats = []\n\u001B[32m     80\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.categorical_feature_names):\n\u001B[32m     81\u001B[39m     \u001B[38;5;66;03m# Get the i-th column of categorical IDs\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m     cat_ids = \u001B[43mx_categorical\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     83\u001B[39m     embedded_cats.append(\u001B[38;5;28mself\u001B[39m.embedding_layers[name](cat_ids))\n\u001B[32m     85\u001B[39m \u001B[38;5;66;03m# Concatenate all looked-up embeddings\u001B[39;00m\n",
      "\u001B[31mIndexError\u001B[39m: index 0 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a26f355bbf7e1888"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
