{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T19:04:46.996813Z",
     "start_time": "2025-11-12T19:04:46.991032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure logging\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate timestamped filename\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_filename = f\"{timestamp}.log\"\n",
    "\n",
    "file_handler = logging.FileHandler(log_filename)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[stdout_handler, file_handler]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def flush_logger():\n",
    "    stdout_handler.flush()\n",
    "\n"
   ],
   "id": "e5ac7a92777dcdbf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-12T19:04:48.864270Z",
     "start_time": "2025-11-12T19:04:47.006929Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from config import FieldConfig, ExperimentConfig\n",
    "from pathlib import Path\n",
    "DATA_PATH = Path('C:/Work/Data/proc/')\n",
    "field_config = FieldConfig()\n",
    "\n",
    "df = pd.read_csv(DATA_PATH/'rec_data2.csv')\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "df_cleaned = df.dropna(subset=[field_config.date, field_config.amount, field_config.text, field_config.label])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "id                    0\n",
      "accountId             0\n",
      "date                  0\n",
      "amount                0\n",
      "bankRawDescription    0\n",
      "isRecurring           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T19:04:49.162855Z",
     "start_time": "2025-11-12T19:04:48.947418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from feature_processor import FeatProcParams\n",
    "from config import *\n",
    "\n",
    "feat_params = FeatProcParams(n_bins=20, k_top=50)\n",
    "feat_params_off = FeatProcParams.NOP()\n",
    "\n",
    "exp_config = ExperimentConfig()\n",
    "\n",
    "fracs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]"
   ],
   "id": "64ad44e449b7478f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T19:04:51.639193Z",
     "start_time": "2025-11-12T19:04:49.173371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from runner import ExpRunner\n",
    "from config import EmbModel\n",
    "from embedder import EmbeddingService\n",
    "\n",
    "runner1 = ExpRunner.create(\n",
    "    exp_params=exp_config,\n",
    "    full_df=df_cleaned,\n",
    "    emb_params=EmbeddingService.Params(model_name=EmbModel.ALBERT),\n",
    "    feat_proc_params=feat_params,\n",
    "    field_config=FieldConfig()\n",
    ")"
   ],
   "id": "d651a17edece86d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoav.haimovitch\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T19:09:20.259207Z",
     "start_time": "2025-11-12T19:04:51.663726Z"
    }
   },
   "cell_type": "code",
   "source": "results = runner1.run_torch(fracs)",
   "id": "97b17b877b9520a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12 21:04:51,672 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-12 21:04:51,745 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-12 21:04:51,746 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-12 21:04:51,752 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-12 21:04:51,756 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-12 21:04:51,767 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-12 21:04:51,768 - INFO - Creating new EmbeddingService(model_name=albert-base-v2)\n",
      "2025-11-12 21:04:51,771 - INFO - Loading embedding model: albert-base-v2...\n",
      "2025-11-12 21:04:53,655 - INFO - Model albert-base-v2 loaded onto cpu. Cache at cache/albert-base-v2\n",
      "2025-11-12 21:04:54,328 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-12 21:04:56,882 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:04:57,483 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-12 21:04:57,483 - INFO - Fitting categorical amount features...\n",
      "2025-11-12 21:04:57,486 - INFO - --- Magic Number Cents Analysis (Top 10) ---\n",
      "2025-11-12 21:04:57,487 - INFO -   0: 39\n",
      "2025-11-12 21:04:57,488 - INFO -   99: 9\n",
      "2025-11-12 21:04:57,488 - INFO -   79: 1\n",
      "2025-11-12 21:04:57,489 - INFO -   35: 1\n",
      "2025-11-12 21:04:57,494 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-12 21:04:57,495 - INFO - Created 20 fallback bins.\n",
      "2025-11-12 21:04:57,496 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-12 21:04:57,497 - INFO - Transforming 16704 rows...\n",
      "2025-11-12 21:04:57,556 - INFO - Transform complete.\n",
      "2025-11-12 21:04:57,556 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:04:57,677 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 888\n",
      "2025-11-12 21:04:57,711 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:04:58,793 - INFO - Epoch 1/10 [1.08s] | Train Loss: 0.5480 | Test Loss: 0.5590 | F1: 0.6005 | ROC-AUC: 0.8719\n",
      "2025-11-12 21:04:59,826 - INFO - Epoch 2/10 [1.03s] | Train Loss: 0.3854 | Test Loss: 0.3385 | F1: 0.6987 | ROC-AUC: 0.8934\n",
      "2025-11-12 21:05:00,889 - INFO - Epoch 3/10 [1.06s] | Train Loss: 0.3086 | Test Loss: 0.3032 | F1: 0.6934 | ROC-AUC: 0.8936\n",
      "2025-11-12 21:05:01,965 - INFO - Epoch 4/10 [1.08s] | Train Loss: 0.2759 | Test Loss: 0.3544 | F1: 0.6998 | ROC-AUC: 0.9065\n",
      "2025-11-12 21:05:03,076 - INFO - Epoch 5/10 [1.11s] | Train Loss: 0.2566 | Test Loss: 0.3418 | F1: 0.6894 | ROC-AUC: 0.8883\n",
      "2025-11-12 21:05:04,246 - INFO - Epoch 6/10 [1.17s] | Train Loss: 0.2345 | Test Loss: 0.5408 | F1: 0.6196 | ROC-AUC: 0.8958\n",
      "2025-11-12 21:05:05,352 - INFO - Epoch 7/10 [1.10s] | Train Loss: 0.2252 | Test Loss: 0.2869 | F1: 0.6422 | ROC-AUC: 0.9289\n",
      "2025-11-12 21:05:06,422 - INFO - Epoch 8/10 [1.07s] | Train Loss: 0.2218 | Test Loss: 0.3112 | F1: 0.6860 | ROC-AUC: 0.9000\n",
      "2025-11-12 21:05:07,462 - INFO - Epoch 9/10 [1.04s] | Train Loss: 0.2083 | Test Loss: 0.2764 | F1: 0.7056 | ROC-AUC: 0.9198\n",
      "2025-11-12 21:05:08,524 - INFO - Epoch 10/10 [1.06s] | Train Loss: 0.2073 | Test Loss: 0.3168 | F1: 0.7125 | ROC-AUC: 0.9131\n",
      "2025-11-12 21:05:08,525 - INFO - Training complete.\n",
      "2025-11-12 21:05:08,528 - INFO - {'loss': 0.317, 'accuracy': 0.877, 'f1': 0.712, 'roc_auc': 0.913, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-12 21:05:08,537 - INFO - Yielding 20% split: 196 accounts, 29736 rows\n",
      "2025-11-12 21:05:08,984 - INFO - len(text_list)=29736 len(unique_texts)=19347 len(texts_to_embed)=0\n",
      "2025-11-12 21:05:09,624 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:05:10,200 - INFO - Fitting processor on 29736 rows...\n",
      "2025-11-12 21:05:10,201 - INFO - Fitting categorical amount features...\n",
      "2025-11-12 21:05:10,204 - INFO - --- Magic Number Cents Analysis (Top 10) ---\n",
      "2025-11-12 21:05:10,205 - INFO -   0: 42\n",
      "2025-11-12 21:05:10,205 - INFO -   99: 6\n",
      "2025-11-12 21:05:10,206 - INFO -   50: 1\n",
      "2025-11-12 21:05:10,207 - INFO -   95: 1\n",
      "2025-11-12 21:05:10,215 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-12 21:05:10,215 - INFO - Created 20 fallback bins.\n",
      "2025-11-12 21:05:10,216 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-12 21:05:10,217 - INFO - Transforming 29736 rows...\n",
      "2025-11-12 21:05:10,317 - INFO - Transform complete.\n",
      "2025-11-12 21:05:10,317 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:05:10,427 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 888\n",
      "2025-11-12 21:05:10,466 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:05:12,088 - INFO - Epoch 1/10 [1.62s] | Train Loss: 0.5160 | Test Loss: 0.3685 | F1: 0.6959 | ROC-AUC: 0.8987\n",
      "2025-11-12 21:05:13,555 - INFO - Epoch 2/10 [1.47s] | Train Loss: 0.3234 | Test Loss: 0.3129 | F1: 0.5826 | ROC-AUC: 0.8951\n",
      "2025-11-12 21:05:15,037 - INFO - Epoch 3/10 [1.48s] | Train Loss: 0.2624 | Test Loss: 0.3491 | F1: 0.5463 | ROC-AUC: 0.8829\n",
      "2025-11-12 21:05:16,563 - INFO - Epoch 4/10 [1.53s] | Train Loss: 0.2453 | Test Loss: 0.2611 | F1: 0.7048 | ROC-AUC: 0.9268\n",
      "2025-11-12 21:05:18,082 - INFO - Epoch 5/10 [1.52s] | Train Loss: 0.2315 | Test Loss: 0.2693 | F1: 0.6945 | ROC-AUC: 0.9264\n",
      "2025-11-12 21:05:19,669 - INFO - Epoch 6/10 [1.59s] | Train Loss: 0.2235 | Test Loss: 0.2812 | F1: 0.6483 | ROC-AUC: 0.9249\n",
      "2025-11-12 21:05:21,228 - INFO - Epoch 7/10 [1.56s] | Train Loss: 0.2102 | Test Loss: 0.2975 | F1: 0.5937 | ROC-AUC: 0.9212\n",
      "2025-11-12 21:05:22,797 - INFO - Epoch 8/10 [1.57s] | Train Loss: 0.2127 | Test Loss: 0.2591 | F1: 0.6996 | ROC-AUC: 0.9376\n",
      "2025-11-12 21:05:24,394 - INFO - Epoch 9/10 [1.60s] | Train Loss: 0.2032 | Test Loss: 0.2575 | F1: 0.6788 | ROC-AUC: 0.9408\n",
      "2025-11-12 21:05:25,997 - INFO - Epoch 10/10 [1.60s] | Train Loss: 0.1993 | Test Loss: 0.2977 | F1: 0.6720 | ROC-AUC: 0.9202\n",
      "2025-11-12 21:05:25,998 - INFO - Training complete.\n",
      "2025-11-12 21:05:26,001 - INFO - {'loss': 0.261, 'accuracy': 0.901, 'f1': 0.705, 'roc_auc': 0.927, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 29736, 'test_size': 33464, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-12 21:05:26,009 - INFO - Yielding 30% split: 295 accounts, 42410 rows\n",
      "2025-11-12 21:05:26,717 - INFO - len(text_list)=42410 len(unique_texts)=26700 len(texts_to_embed)=0\n",
      "2025-11-12 21:05:27,709 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:05:28,269 - INFO - Fitting processor on 42410 rows...\n",
      "2025-11-12 21:05:28,270 - INFO - Fitting categorical amount features...\n",
      "2025-11-12 21:05:28,275 - INFO - --- Magic Number Cents Analysis (Top 10) ---\n",
      "2025-11-12 21:05:28,276 - INFO -   0: 42\n",
      "2025-11-12 21:05:28,276 - INFO -   99: 6\n",
      "2025-11-12 21:05:28,277 - INFO -   50: 1\n",
      "2025-11-12 21:05:28,278 - INFO -   95: 1\n",
      "2025-11-12 21:05:28,286 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-12 21:05:28,287 - INFO - Created 20 fallback bins.\n",
      "2025-11-12 21:05:28,288 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-12 21:05:28,289 - INFO - Transforming 42410 rows...\n",
      "2025-11-12 21:05:28,427 - INFO - Transform complete.\n",
      "2025-11-12 21:05:28,428 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:05:28,545 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 888\n",
      "2025-11-12 21:05:28,591 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:05:30,621 - INFO - Epoch 1/10 [2.03s] | Train Loss: 0.4712 | Test Loss: 0.3121 | F1: 0.6995 | ROC-AUC: 0.9006\n",
      "2025-11-12 21:05:32,524 - INFO - Epoch 2/10 [1.90s] | Train Loss: 0.3002 | Test Loss: 0.2783 | F1: 0.6854 | ROC-AUC: 0.9145\n",
      "2025-11-12 21:05:34,434 - INFO - Epoch 3/10 [1.91s] | Train Loss: 0.2632 | Test Loss: 0.2553 | F1: 0.7506 | ROC-AUC: 0.9268\n",
      "2025-11-12 21:05:36,495 - INFO - Epoch 4/10 [2.06s] | Train Loss: 0.2444 | Test Loss: 0.2399 | F1: 0.7586 | ROC-AUC: 0.9343\n",
      "2025-11-12 21:05:38,566 - INFO - Epoch 5/10 [2.07s] | Train Loss: 0.2337 | Test Loss: 0.2335 | F1: 0.7539 | ROC-AUC: 0.9380\n",
      "2025-11-12 21:05:40,682 - INFO - Epoch 6/10 [2.12s] | Train Loss: 0.2311 | Test Loss: 0.2289 | F1: 0.7637 | ROC-AUC: 0.9412\n",
      "2025-11-12 21:05:42,795 - INFO - Epoch 7/10 [2.11s] | Train Loss: 0.2186 | Test Loss: 0.2371 | F1: 0.7523 | ROC-AUC: 0.9370\n",
      "2025-11-12 21:05:45,145 - INFO - Epoch 8/10 [2.35s] | Train Loss: 0.2159 | Test Loss: 0.2324 | F1: 0.7693 | ROC-AUC: 0.9406\n",
      "2025-11-12 21:05:47,545 - INFO - Epoch 9/10 [2.40s] | Train Loss: 0.2089 | Test Loss: 0.2378 | F1: 0.7410 | ROC-AUC: 0.9423\n",
      "2025-11-12 21:05:49,731 - INFO - Epoch 10/10 [2.19s] | Train Loss: 0.2085 | Test Loss: 0.2356 | F1: 0.7548 | ROC-AUC: 0.9390\n",
      "2025-11-12 21:05:49,731 - INFO - Training complete.\n",
      "2025-11-12 21:05:49,735 - INFO - {'loss': 0.232, 'accuracy': 0.915, 'f1': 0.769, 'roc_auc': 0.941, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 42410, 'test_size': 33464, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-12 21:05:49,744 - INFO - Yielding 40% split: 393 accounts, 59801 rows\n",
      "2025-11-12 21:05:51,151 - INFO - len(text_list)=59801 len(unique_texts)=37409 len(texts_to_embed)=0\n",
      "2025-11-12 21:05:52,335 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:05:53,008 - INFO - Fitting processor on 59801 rows...\n",
      "2025-11-12 21:05:53,009 - INFO - Fitting categorical amount features...\n",
      "2025-11-12 21:05:53,013 - INFO - --- Magic Number Cents Analysis (Top 10) ---\n",
      "2025-11-12 21:05:53,014 - INFO -   0: 39\n",
      "2025-11-12 21:05:53,015 - INFO -   99: 9\n",
      "2025-11-12 21:05:53,016 - INFO -   50: 1\n",
      "2025-11-12 21:05:53,017 - INFO -   95: 1\n",
      "2025-11-12 21:05:53,025 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-12 21:05:53,026 - INFO - Created 20 fallback bins.\n",
      "2025-11-12 21:05:53,027 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-12 21:05:53,028 - INFO - Transforming 59801 rows...\n",
      "2025-11-12 21:05:53,222 - INFO - Transform complete.\n",
      "2025-11-12 21:05:53,223 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:05:53,335 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 888\n",
      "2025-11-12 21:05:53,394 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:05:56,103 - INFO - Epoch 1/10 [2.71s] | Train Loss: 0.4354 | Test Loss: 0.3049 | F1: 0.7214 | ROC-AUC: 0.9076\n",
      "2025-11-12 21:05:58,618 - INFO - Epoch 2/10 [2.51s] | Train Loss: 0.2884 | Test Loss: 0.2679 | F1: 0.7184 | ROC-AUC: 0.9186\n",
      "2025-11-12 21:06:01,133 - INFO - Epoch 3/10 [2.51s] | Train Loss: 0.2584 | Test Loss: 0.2490 | F1: 0.7194 | ROC-AUC: 0.9357\n",
      "2025-11-12 21:06:04,038 - INFO - Epoch 4/10 [2.91s] | Train Loss: 0.2434 | Test Loss: 0.2266 | F1: 0.7501 | ROC-AUC: 0.9446\n",
      "2025-11-12 21:06:07,132 - INFO - Epoch 5/10 [3.09s] | Train Loss: 0.2333 | Test Loss: 0.2654 | F1: 0.6803 | ROC-AUC: 0.9440\n",
      "2025-11-12 21:06:09,911 - INFO - Epoch 6/10 [2.78s] | Train Loss: 0.2254 | Test Loss: 0.2224 | F1: 0.7800 | ROC-AUC: 0.9437\n",
      "2025-11-12 21:06:12,949 - INFO - Epoch 7/10 [3.04s] | Train Loss: 0.2227 | Test Loss: 0.2361 | F1: 0.7743 | ROC-AUC: 0.9395\n",
      "2025-11-12 21:06:16,103 - INFO - Epoch 8/10 [3.15s] | Train Loss: 0.2184 | Test Loss: 0.2206 | F1: 0.7655 | ROC-AUC: 0.9473\n",
      "2025-11-12 21:06:18,976 - INFO - Epoch 9/10 [2.87s] | Train Loss: 0.2136 | Test Loss: 0.2132 | F1: 0.7830 | ROC-AUC: 0.9491\n",
      "2025-11-12 21:06:21,887 - INFO - Epoch 10/10 [2.91s] | Train Loss: 0.2101 | Test Loss: 0.2538 | F1: 0.6743 | ROC-AUC: 0.9395\n",
      "2025-11-12 21:06:21,888 - INFO - Training complete.\n",
      "2025-11-12 21:06:21,893 - INFO - {'loss': 0.213, 'accuracy': 0.92, 'f1': 0.783, 'roc_auc': 0.949, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 59801, 'test_size': 33464, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-12 21:06:21,905 - INFO - Yielding 50% split: 492 accounts, 75623 rows\n",
      "2025-11-12 21:06:23,378 - INFO - len(text_list)=75623 len(unique_texts)=46835 len(texts_to_embed)=0\n",
      "2025-11-12 21:06:24,833 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:06:25,417 - INFO - Fitting processor on 75623 rows...\n",
      "2025-11-12 21:06:25,418 - INFO - Fitting categorical amount features...\n",
      "2025-11-12 21:06:25,423 - INFO - --- Magic Number Cents Analysis (Top 10) ---\n",
      "2025-11-12 21:06:25,424 - INFO -   0: 40\n",
      "2025-11-12 21:06:25,425 - INFO -   99: 8\n",
      "2025-11-12 21:06:25,426 - INFO -   50: 1\n",
      "2025-11-12 21:06:25,427 - INFO -   95: 1\n",
      "2025-11-12 21:06:25,442 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-12 21:06:25,443 - INFO - Created 20 fallback bins.\n",
      "2025-11-12 21:06:25,443 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-12 21:06:25,445 - INFO - Transforming 75623 rows...\n",
      "2025-11-12 21:06:25,724 - INFO - Transform complete.\n",
      "2025-11-12 21:06:25,725 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:06:25,851 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 888\n",
      "2025-11-12 21:06:25,917 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:06:29,160 - INFO - Epoch 1/10 [3.24s] | Train Loss: 0.4040 | Test Loss: 0.3712 | F1: 0.6916 | ROC-AUC: 0.9058\n",
      "2025-11-12 21:06:32,263 - INFO - Epoch 2/10 [3.10s] | Train Loss: 0.2719 | Test Loss: 0.2503 | F1: 0.7616 | ROC-AUC: 0.9343\n",
      "2025-11-12 21:06:35,626 - INFO - Epoch 3/10 [3.36s] | Train Loss: 0.2448 | Test Loss: 0.2728 | F1: 0.7558 | ROC-AUC: 0.9349\n",
      "2025-11-12 21:06:39,046 - INFO - Epoch 4/10 [3.42s] | Train Loss: 0.2344 | Test Loss: 0.2367 | F1: 0.7382 | ROC-AUC: 0.9408\n",
      "2025-11-12 21:06:42,441 - INFO - Epoch 5/10 [3.39s] | Train Loss: 0.2249 | Test Loss: 0.2199 | F1: 0.7558 | ROC-AUC: 0.9486\n",
      "2025-11-12 21:06:45,872 - INFO - Epoch 6/10 [3.43s] | Train Loss: 0.2191 | Test Loss: 0.2232 | F1: 0.7674 | ROC-AUC: 0.9448\n",
      "2025-11-12 21:06:52,050 - INFO - Epoch 7/10 [6.18s] | Train Loss: 0.2160 | Test Loss: 0.2220 | F1: 0.7796 | ROC-AUC: 0.9454\n",
      "2025-11-12 21:06:56,852 - INFO - Epoch 8/10 [4.80s] | Train Loss: 0.2104 | Test Loss: 0.2426 | F1: 0.7696 | ROC-AUC: 0.9425\n",
      "2025-11-12 21:07:01,696 - INFO - Epoch 9/10 [4.84s] | Train Loss: 0.2065 | Test Loss: 0.2325 | F1: 0.7303 | ROC-AUC: 0.9476\n",
      "2025-11-12 21:07:07,457 - INFO - Epoch 10/10 [5.76s] | Train Loss: 0.2028 | Test Loss: 0.2373 | F1: 0.7312 | ROC-AUC: 0.9447\n",
      "2025-11-12 21:07:07,458 - INFO - Training complete.\n",
      "2025-11-12 21:07:07,467 - INFO - {'loss': 0.222, 'accuracy': 0.917, 'f1': 0.78, 'roc_auc': 0.945, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.5, 'train_size': 75623, 'test_size': 33464, 'train_accounts': 492, 'test_accounts': 246}\n",
      "2025-11-12 21:07:07,481 - INFO - Yielding 60% split: 590 accounts, 92281 rows\n",
      "2025-11-12 21:07:09,120 - INFO - len(text_list)=92281 len(unique_texts)=56280 len(texts_to_embed)=0\n",
      "2025-11-12 21:07:10,996 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:07:11,534 - INFO - Fitting processor on 92281 rows...\n",
      "2025-11-12 21:07:11,535 - INFO - Fitting categorical amount features...\n",
      "2025-11-12 21:07:11,539 - INFO - --- Magic Number Cents Analysis (Top 10) ---\n",
      "2025-11-12 21:07:11,540 - INFO -   0: 39\n",
      "2025-11-12 21:07:11,541 - INFO -   99: 9\n",
      "2025-11-12 21:07:11,542 - INFO -   50: 1\n",
      "2025-11-12 21:07:11,542 - INFO -   95: 1\n",
      "2025-11-12 21:07:11,559 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-12 21:07:11,560 - INFO - Created 20 fallback bins.\n",
      "2025-11-12 21:07:11,561 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-12 21:07:11,561 - INFO - Transforming 92281 rows...\n",
      "2025-11-12 21:07:11,873 - INFO - Transform complete.\n",
      "2025-11-12 21:07:11,873 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:07:12,016 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 888\n",
      "2025-11-12 21:07:12,081 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:07:18,785 - INFO - Epoch 1/10 [6.70s] | Train Loss: 0.3831 | Test Loss: 0.2823 | F1: 0.6510 | ROC-AUC: 0.9264\n",
      "2025-11-12 21:07:23,716 - INFO - Epoch 2/10 [4.93s] | Train Loss: 0.2650 | Test Loss: 0.2715 | F1: 0.7635 | ROC-AUC: 0.9337\n",
      "2025-11-12 21:07:29,069 - INFO - Epoch 3/10 [5.35s] | Train Loss: 0.2416 | Test Loss: 0.2309 | F1: 0.7356 | ROC-AUC: 0.9441\n",
      "2025-11-12 21:07:34,895 - INFO - Epoch 4/10 [5.82s] | Train Loss: 0.2317 | Test Loss: 0.2378 | F1: 0.7761 | ROC-AUC: 0.9435\n",
      "2025-11-12 21:07:40,823 - INFO - Epoch 5/10 [5.93s] | Train Loss: 0.2252 | Test Loss: 0.2398 | F1: 0.7669 | ROC-AUC: 0.9406\n",
      "2025-11-12 21:07:47,106 - INFO - Epoch 6/10 [6.28s] | Train Loss: 0.2193 | Test Loss: 0.2109 | F1: 0.7820 | ROC-AUC: 0.9492\n",
      "2025-11-12 21:07:53,457 - INFO - Epoch 7/10 [6.35s] | Train Loss: 0.2145 | Test Loss: 0.2246 | F1: 0.7782 | ROC-AUC: 0.9459\n",
      "2025-11-12 21:07:58,966 - INFO - Epoch 8/10 [5.51s] | Train Loss: 0.2124 | Test Loss: 0.2041 | F1: 0.7858 | ROC-AUC: 0.9535\n",
      "2025-11-12 21:08:05,007 - INFO - Epoch 9/10 [6.04s] | Train Loss: 0.2080 | Test Loss: 0.2165 | F1: 0.7904 | ROC-AUC: 0.9480\n",
      "2025-11-12 21:08:10,707 - INFO - Epoch 10/10 [5.70s] | Train Loss: 0.2054 | Test Loss: 0.2052 | F1: 0.7991 | ROC-AUC: 0.9532\n",
      "2025-11-12 21:08:10,708 - INFO - Training complete.\n",
      "2025-11-12 21:08:10,714 - INFO - {'loss': 0.205, 'accuracy': 0.924, 'f1': 0.799, 'roc_auc': 0.953, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.6, 'train_size': 92281, 'test_size': 33464, 'train_accounts': 590, 'test_accounts': 246}\n",
      "2025-11-12 21:08:10,727 - INFO - Yielding 70% split: 688 accounts, 108248 rows\n",
      "2025-11-12 21:08:12,563 - INFO - len(text_list)=108248 len(unique_texts)=65808 len(texts_to_embed)=0\n",
      "2025-11-12 21:08:14,647 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:08:15,301 - INFO - Fitting processor on 108248 rows...\n",
      "2025-11-12 21:08:15,302 - INFO - Fitting categorical amount features...\n",
      "2025-11-12 21:08:15,308 - INFO - --- Magic Number Cents Analysis (Top 10) ---\n",
      "2025-11-12 21:08:15,309 - INFO -   0: 39\n",
      "2025-11-12 21:08:15,310 - INFO -   99: 9\n",
      "2025-11-12 21:08:15,311 - INFO -   50: 1\n",
      "2025-11-12 21:08:15,312 - INFO -   95: 1\n",
      "2025-11-12 21:08:15,328 - INFO - Fit complete. Found 50 magic numbers.\n",
      "2025-11-12 21:08:15,329 - INFO - Created 20 fallback bins.\n",
      "2025-11-12 21:08:15,329 - INFO - Total Amount Vocabulary size: 72\n",
      "2025-11-12 21:08:15,330 - INFO - Transforming 108248 rows...\n",
      "2025-11-12 21:08:15,729 - INFO - Transform complete.\n",
      "2025-11-12 21:08:15,730 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:08:15,844 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=True, use_categorical=True\n",
      "Total input dim to MLP: 888\n",
      "2025-11-12 21:08:15,915 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:08:21,935 - INFO - Epoch 1/10 [6.02s] | Train Loss: 0.3655 | Test Loss: 0.2452 | F1: 0.7482 | ROC-AUC: 0.9340\n",
      "2025-11-12 21:08:27,638 - INFO - Epoch 2/10 [5.70s] | Train Loss: 0.2564 | Test Loss: 0.2367 | F1: 0.7653 | ROC-AUC: 0.9353\n",
      "2025-11-12 21:08:34,175 - INFO - Epoch 3/10 [6.54s] | Train Loss: 0.2365 | Test Loss: 0.2351 | F1: 0.7329 | ROC-AUC: 0.9432\n",
      "2025-11-12 21:08:40,350 - INFO - Epoch 4/10 [6.17s] | Train Loss: 0.2266 | Test Loss: 0.2114 | F1: 0.7775 | ROC-AUC: 0.9516\n",
      "2025-11-12 21:08:46,786 - INFO - Epoch 5/10 [6.44s] | Train Loss: 0.2196 | Test Loss: 0.2153 | F1: 0.7670 | ROC-AUC: 0.9501\n",
      "2025-11-12 21:08:53,595 - INFO - Epoch 6/10 [6.81s] | Train Loss: 0.2137 | Test Loss: 0.2177 | F1: 0.7670 | ROC-AUC: 0.9506\n",
      "2025-11-12 21:08:59,998 - INFO - Epoch 7/10 [6.40s] | Train Loss: 0.2098 | Test Loss: 0.2442 | F1: 0.7776 | ROC-AUC: 0.9438\n",
      "2025-11-12 21:09:06,964 - INFO - Epoch 8/10 [6.96s] | Train Loss: 0.2073 | Test Loss: 0.2241 | F1: 0.7652 | ROC-AUC: 0.9509\n",
      "2025-11-12 21:09:13,432 - INFO - Epoch 9/10 [6.47s] | Train Loss: 0.2037 | Test Loss: 0.2036 | F1: 0.7990 | ROC-AUC: 0.9540\n",
      "2025-11-12 21:09:20,218 - INFO - Epoch 10/10 [6.79s] | Train Loss: 0.2019 | Test Loss: 0.2005 | F1: 0.7970 | ROC-AUC: 0.9564\n",
      "2025-11-12 21:09:20,218 - INFO - Training complete.\n",
      "2025-11-12 21:09:20,223 - INFO - {'loss': 0.204, 'accuracy': 0.924, 'f1': 0.799, 'roc_auc': 0.954, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.7, 'train_size': 108248, 'test_size': 33464, 'train_accounts': 688, 'test_accounts': 246}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T19:09:20.369388Z",
     "start_time": "2025-11-12T19:09:20.363958Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "992cb940d94cdeb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16704: {'loss': 0.317,\n",
       "  'accuracy': 0.877,\n",
       "  'f1': 0.712,\n",
       "  'roc_auc': 0.913,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.1,\n",
       "  'train_size': 16704,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 98,\n",
       "  'test_accounts': 246},\n",
       " 29736: {'loss': 0.261,\n",
       "  'accuracy': 0.901,\n",
       "  'f1': 0.705,\n",
       "  'roc_auc': 0.927,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.2,\n",
       "  'train_size': 29736,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 196,\n",
       "  'test_accounts': 246},\n",
       " 42410: {'loss': 0.232,\n",
       "  'accuracy': 0.915,\n",
       "  'f1': 0.769,\n",
       "  'roc_auc': 0.941,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.3,\n",
       "  'train_size': 42410,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 295,\n",
       "  'test_accounts': 246},\n",
       " 59801: {'loss': 0.213,\n",
       "  'accuracy': 0.92,\n",
       "  'f1': 0.783,\n",
       "  'roc_auc': 0.949,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.4,\n",
       "  'train_size': 59801,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 393,\n",
       "  'test_accounts': 246},\n",
       " 75623: {'loss': 0.222,\n",
       "  'accuracy': 0.917,\n",
       "  'f1': 0.78,\n",
       "  'roc_auc': 0.945,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.5,\n",
       "  'train_size': 75623,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 492,\n",
       "  'test_accounts': 246},\n",
       " 92281: {'loss': 0.205,\n",
       "  'accuracy': 0.924,\n",
       "  'f1': 0.799,\n",
       "  'roc_auc': 0.953,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.6,\n",
       "  'train_size': 92281,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 590,\n",
       "  'test_accounts': 246},\n",
       " 108248: {'loss': 0.204,\n",
       "  'accuracy': 0.924,\n",
       "  'f1': 0.799,\n",
       "  'roc_auc': 0.954,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.7,\n",
       "  'train_size': 108248,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 688,\n",
       "  'test_accounts': 246}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T19:13:48.030087Z",
     "start_time": "2025-11-12T19:09:20.411471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "runner2 = ExpRunner.copy(runner1)\n",
    "runner2.feat_proc_params = feat_params_off\n",
    "results2 = runner2.run_torch(fracs)\n"
   ],
   "id": "96f96304f4d9f7cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12 21:09:20,419 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-12 21:09:20,520 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-12 21:09:20,521 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-12 21:09:20,528 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-12 21:09:20,535 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-12 21:09:20,548 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-12 21:09:20,549 - INFO - Creating new EmbeddingService(model_name=albert-base-v2)\n",
      "2025-11-12 21:09:20,550 - INFO - Loading embedding model: albert-base-v2...\n",
      "2025-11-12 21:09:22,093 - INFO - Model albert-base-v2 loaded onto cpu. Cache at cache/albert-base-v2\n",
      "2025-11-12 21:09:22,195 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-12 21:09:22,719 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:09:23,272 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-12 21:09:23,273 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:09:23,273 - INFO - Transforming 16704 rows...\n",
      "2025-11-12 21:09:23,274 - INFO - Transform complete.\n",
      "2025-11-12 21:09:23,275 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:09:23,276 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:09:23,293 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:09:24,581 - INFO - Epoch 1/10 [1.29s] | Train Loss: 0.5879 | Test Loss: 0.4392 | F1: 0.6652 | ROC-AUC: 0.8812\n",
      "2025-11-12 21:09:26,047 - INFO - Epoch 2/10 [1.47s] | Train Loss: 0.4066 | Test Loss: 1.3574 | F1: 0.3921 | ROC-AUC: 0.7680\n",
      "2025-11-12 21:09:27,294 - INFO - Epoch 3/10 [1.25s] | Train Loss: 0.3228 | Test Loss: 0.3234 | F1: 0.5573 | ROC-AUC: 0.8683\n",
      "2025-11-12 21:09:28,618 - INFO - Epoch 4/10 [1.32s] | Train Loss: 0.2807 | Test Loss: 0.2990 | F1: 0.6646 | ROC-AUC: 0.8999\n",
      "2025-11-12 21:09:29,985 - INFO - Epoch 5/10 [1.37s] | Train Loss: 0.2606 | Test Loss: 0.3402 | F1: 0.5993 | ROC-AUC: 0.8831\n",
      "2025-11-12 21:09:31,291 - INFO - Epoch 6/10 [1.31s] | Train Loss: 0.2516 | Test Loss: 0.3102 | F1: 0.7457 | ROC-AUC: 0.9089\n",
      "2025-11-12 21:09:32,875 - INFO - Epoch 7/10 [1.58s] | Train Loss: 0.2458 | Test Loss: 0.2681 | F1: 0.7221 | ROC-AUC: 0.9216\n",
      "2025-11-12 21:09:34,251 - INFO - Epoch 8/10 [1.37s] | Train Loss: 0.2384 | Test Loss: 0.2649 | F1: 0.7182 | ROC-AUC: 0.9196\n",
      "2025-11-12 21:09:35,571 - INFO - Epoch 9/10 [1.32s] | Train Loss: 0.2299 | Test Loss: 0.2618 | F1: 0.7318 | ROC-AUC: 0.9235\n",
      "2025-11-12 21:09:36,946 - INFO - Epoch 10/10 [1.37s] | Train Loss: 0.2289 | Test Loss: 0.2964 | F1: 0.5529 | ROC-AUC: 0.9202\n",
      "2025-11-12 21:09:36,946 - INFO - Training complete.\n",
      "2025-11-12 21:09:36,952 - INFO - {'loss': 0.31, 'accuracy': 0.899, 'f1': 0.746, 'roc_auc': 0.909, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-12 21:09:36,961 - INFO - Yielding 20% split: 196 accounts, 29736 rows\n",
      "2025-11-12 21:09:37,122 - INFO - len(text_list)=29736 len(unique_texts)=19347 len(texts_to_embed)=0\n",
      "2025-11-12 21:09:37,735 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:09:38,255 - INFO - Fitting processor on 29736 rows...\n",
      "2025-11-12 21:09:38,256 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:09:38,256 - INFO - Transforming 29736 rows...\n",
      "2025-11-12 21:09:38,258 - INFO - Transform complete.\n",
      "2025-11-12 21:09:38,259 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:09:38,260 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:09:38,282 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:09:40,136 - INFO - Epoch 1/10 [1.85s] | Train Loss: 0.4982 | Test Loss: 0.3814 | F1: 0.3863 | ROC-AUC: 0.8279\n",
      "2025-11-12 21:09:42,026 - INFO - Epoch 2/10 [1.89s] | Train Loss: 0.3169 | Test Loss: 0.3001 | F1: 0.7260 | ROC-AUC: 0.8923\n",
      "2025-11-12 21:09:43,855 - INFO - Epoch 3/10 [1.83s] | Train Loss: 0.2724 | Test Loss: 0.2615 | F1: 0.7353 | ROC-AUC: 0.9163\n",
      "2025-11-12 21:09:45,812 - INFO - Epoch 4/10 [1.96s] | Train Loss: 0.2548 | Test Loss: 0.2938 | F1: 0.5841 | ROC-AUC: 0.9253\n",
      "2025-11-12 21:09:48,118 - INFO - Epoch 5/10 [2.31s] | Train Loss: 0.2408 | Test Loss: 0.2943 | F1: 0.7355 | ROC-AUC: 0.9147\n",
      "2025-11-12 21:09:50,006 - INFO - Epoch 6/10 [1.89s] | Train Loss: 0.2328 | Test Loss: 0.3181 | F1: 0.5879 | ROC-AUC: 0.9125\n",
      "2025-11-12 21:09:52,036 - INFO - Epoch 7/10 [2.03s] | Train Loss: 0.2259 | Test Loss: 0.2572 | F1: 0.7416 | ROC-AUC: 0.9162\n",
      "2025-11-12 21:09:54,053 - INFO - Epoch 8/10 [2.02s] | Train Loss: 0.2262 | Test Loss: 0.3589 | F1: 0.6472 | ROC-AUC: 0.8951\n",
      "2025-11-12 21:09:56,196 - INFO - Epoch 9/10 [2.14s] | Train Loss: 0.2197 | Test Loss: 0.3344 | F1: 0.7258 | ROC-AUC: 0.9205\n",
      "2025-11-12 21:09:58,332 - INFO - Epoch 10/10 [2.14s] | Train Loss: 0.2166 | Test Loss: 0.3044 | F1: 0.5662 | ROC-AUC: 0.9284\n",
      "2025-11-12 21:09:58,333 - INFO - Training complete.\n",
      "2025-11-12 21:09:58,338 - INFO - {'loss': 0.257, 'accuracy': 0.907, 'f1': 0.742, 'roc_auc': 0.916, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 29736, 'test_size': 33464, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-12 21:09:58,347 - INFO - Yielding 30% split: 295 accounts, 42410 rows\n",
      "2025-11-12 21:09:58,628 - INFO - len(text_list)=42410 len(unique_texts)=26700 len(texts_to_embed)=0\n",
      "2025-11-12 21:09:59,714 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:10:00,249 - INFO - Fitting processor on 42410 rows...\n",
      "2025-11-12 21:10:00,249 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:10:00,250 - INFO - Transforming 42410 rows...\n",
      "2025-11-12 21:10:00,251 - INFO - Transform complete.\n",
      "2025-11-12 21:10:00,251 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:10:00,252 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:10:00,283 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:10:02,915 - INFO - Epoch 1/10 [2.63s] | Train Loss: 0.4736 | Test Loss: 0.2969 | F1: 0.7117 | ROC-AUC: 0.8977\n",
      "2025-11-12 21:10:05,378 - INFO - Epoch 2/10 [2.46s] | Train Loss: 0.3052 | Test Loss: 0.3007 | F1: 0.7379 | ROC-AUC: 0.9045\n",
      "2025-11-12 21:10:07,742 - INFO - Epoch 3/10 [2.36s] | Train Loss: 0.2753 | Test Loss: 0.3257 | F1: 0.5370 | ROC-AUC: 0.9139\n",
      "2025-11-12 21:10:10,047 - INFO - Epoch 4/10 [2.30s] | Train Loss: 0.2565 | Test Loss: 0.3488 | F1: 0.5328 | ROC-AUC: 0.9284\n",
      "2025-11-12 21:10:12,552 - INFO - Epoch 5/10 [2.50s] | Train Loss: 0.2437 | Test Loss: 0.2516 | F1: 0.7491 | ROC-AUC: 0.9265\n",
      "2025-11-12 21:10:15,208 - INFO - Epoch 6/10 [2.65s] | Train Loss: 0.2386 | Test Loss: 0.2296 | F1: 0.7730 | ROC-AUC: 0.9389\n",
      "2025-11-12 21:10:18,239 - INFO - Epoch 7/10 [3.03s] | Train Loss: 0.2325 | Test Loss: 0.2324 | F1: 0.7677 | ROC-AUC: 0.9386\n",
      "2025-11-12 21:10:20,918 - INFO - Epoch 8/10 [2.68s] | Train Loss: 0.2283 | Test Loss: 0.2498 | F1: 0.7238 | ROC-AUC: 0.9387\n",
      "2025-11-12 21:10:23,531 - INFO - Epoch 9/10 [2.61s] | Train Loss: 0.2234 | Test Loss: 0.3730 | F1: 0.6871 | ROC-AUC: 0.9123\n",
      "2025-11-12 21:10:26,208 - INFO - Epoch 10/10 [2.68s] | Train Loss: 0.2187 | Test Loss: 0.2318 | F1: 0.7756 | ROC-AUC: 0.9381\n",
      "2025-11-12 21:10:26,209 - INFO - Training complete.\n",
      "2025-11-12 21:10:26,212 - INFO - {'loss': 0.232, 'accuracy': 0.916, 'f1': 0.776, 'roc_auc': 0.938, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 42410, 'test_size': 33464, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-12 21:10:26,222 - INFO - Yielding 40% split: 393 accounts, 59801 rows\n",
      "2025-11-12 21:10:26,573 - INFO - len(text_list)=59801 len(unique_texts)=37409 len(texts_to_embed)=0\n",
      "2025-11-12 21:10:27,738 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:10:28,381 - INFO - Fitting processor on 59801 rows...\n",
      "2025-11-12 21:10:28,382 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:10:28,382 - INFO - Transforming 59801 rows...\n",
      "2025-11-12 21:10:28,383 - INFO - Transform complete.\n",
      "2025-11-12 21:10:28,384 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:10:28,385 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:10:28,415 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:10:31,751 - INFO - Epoch 1/10 [3.33s] | Train Loss: 0.4346 | Test Loss: 0.3561 | F1: 0.7241 | ROC-AUC: 0.8964\n",
      "2025-11-12 21:10:35,058 - INFO - Epoch 2/10 [3.31s] | Train Loss: 0.2954 | Test Loss: 0.3950 | F1: 0.6590 | ROC-AUC: 0.8940\n",
      "2025-11-12 21:10:38,088 - INFO - Epoch 3/10 [3.03s] | Train Loss: 0.2652 | Test Loss: 0.2551 | F1: 0.7585 | ROC-AUC: 0.9196\n",
      "2025-11-12 21:10:41,427 - INFO - Epoch 4/10 [3.34s] | Train Loss: 0.2518 | Test Loss: 0.2421 | F1: 0.7633 | ROC-AUC: 0.9291\n",
      "2025-11-12 21:10:44,750 - INFO - Epoch 5/10 [3.32s] | Train Loss: 0.2434 | Test Loss: 0.2888 | F1: 0.7442 | ROC-AUC: 0.9270\n",
      "2025-11-12 21:10:48,489 - INFO - Epoch 6/10 [3.74s] | Train Loss: 0.2354 | Test Loss: 0.2874 | F1: 0.6357 | ROC-AUC: 0.9340\n",
      "2025-11-12 21:10:51,884 - INFO - Epoch 7/10 [3.39s] | Train Loss: 0.2283 | Test Loss: 0.2336 | F1: 0.7791 | ROC-AUC: 0.9375\n",
      "2025-11-12 21:10:55,269 - INFO - Epoch 8/10 [3.38s] | Train Loss: 0.2283 | Test Loss: 0.2280 | F1: 0.7665 | ROC-AUC: 0.9423\n",
      "2025-11-12 21:10:58,480 - INFO - Epoch 9/10 [3.21s] | Train Loss: 0.2186 | Test Loss: 0.3159 | F1: 0.7078 | ROC-AUC: 0.9205\n",
      "2025-11-12 21:11:01,731 - INFO - Epoch 10/10 [3.25s] | Train Loss: 0.2197 | Test Loss: 0.2380 | F1: 0.7675 | ROC-AUC: 0.9405\n",
      "2025-11-12 21:11:01,732 - INFO - Training complete.\n",
      "2025-11-12 21:11:01,737 - INFO - {'loss': 0.234, 'accuracy': 0.92, 'f1': 0.779, 'roc_auc': 0.937, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 59801, 'test_size': 33464, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-12 21:11:01,746 - INFO - Yielding 50% split: 492 accounts, 75623 rows\n",
      "2025-11-12 21:11:02,260 - INFO - len(text_list)=75623 len(unique_texts)=46835 len(texts_to_embed)=0\n",
      "2025-11-12 21:11:04,176 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:11:04,775 - INFO - Fitting processor on 75623 rows...\n",
      "2025-11-12 21:11:04,776 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:11:04,777 - INFO - Transforming 75623 rows...\n",
      "2025-11-12 21:11:04,778 - INFO - Transform complete.\n",
      "2025-11-12 21:11:04,779 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:11:04,780 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:11:04,820 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:11:08,529 - INFO - Epoch 1/10 [3.71s] | Train Loss: 0.3927 | Test Loss: 0.2711 | F1: 0.7097 | ROC-AUC: 0.9143\n",
      "2025-11-12 21:11:12,414 - INFO - Epoch 2/10 [3.88s] | Train Loss: 0.2774 | Test Loss: 0.2331 | F1: 0.7715 | ROC-AUC: 0.9360\n",
      "2025-11-12 21:11:16,264 - INFO - Epoch 3/10 [3.85s] | Train Loss: 0.2543 | Test Loss: 0.2531 | F1: 0.7342 | ROC-AUC: 0.9346\n",
      "2025-11-12 21:11:20,579 - INFO - Epoch 4/10 [4.31s] | Train Loss: 0.2412 | Test Loss: 0.2273 | F1: 0.7906 | ROC-AUC: 0.9384\n",
      "2025-11-12 21:11:24,637 - INFO - Epoch 5/10 [4.06s] | Train Loss: 0.2351 | Test Loss: 0.3092 | F1: 0.7250 | ROC-AUC: 0.9299\n",
      "2025-11-12 21:11:28,670 - INFO - Epoch 6/10 [4.03s] | Train Loss: 0.2289 | Test Loss: 0.2472 | F1: 0.7091 | ROC-AUC: 0.9402\n",
      "2025-11-12 21:11:33,107 - INFO - Epoch 7/10 [4.44s] | Train Loss: 0.2261 | Test Loss: 0.2180 | F1: 0.7668 | ROC-AUC: 0.9491\n",
      "2025-11-12 21:11:37,381 - INFO - Epoch 8/10 [4.27s] | Train Loss: 0.2226 | Test Loss: 0.2313 | F1: 0.7348 | ROC-AUC: 0.9469\n",
      "2025-11-12 21:11:41,532 - INFO - Epoch 9/10 [4.15s] | Train Loss: 0.2175 | Test Loss: 0.2161 | F1: 0.7918 | ROC-AUC: 0.9450\n",
      "2025-11-12 21:11:45,674 - INFO - Epoch 10/10 [4.14s] | Train Loss: 0.2147 | Test Loss: 0.2171 | F1: 0.7857 | ROC-AUC: 0.9433\n",
      "2025-11-12 21:11:45,675 - INFO - Training complete.\n",
      "2025-11-12 21:11:45,680 - INFO - {'loss': 0.216, 'accuracy': 0.924, 'f1': 0.792, 'roc_auc': 0.945, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.5, 'train_size': 75623, 'test_size': 33464, 'train_accounts': 492, 'test_accounts': 246}\n",
      "2025-11-12 21:11:45,692 - INFO - Yielding 60% split: 590 accounts, 92281 rows\n",
      "2025-11-12 21:11:46,249 - INFO - len(text_list)=92281 len(unique_texts)=56280 len(texts_to_embed)=0\n",
      "2025-11-12 21:11:48,577 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:11:49,165 - INFO - Fitting processor on 92281 rows...\n",
      "2025-11-12 21:11:49,166 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:11:49,167 - INFO - Transforming 92281 rows...\n",
      "2025-11-12 21:11:49,168 - INFO - Transform complete.\n",
      "2025-11-12 21:11:49,169 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:11:49,170 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:11:49,212 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:11:53,673 - INFO - Epoch 1/10 [4.46s] | Train Loss: 0.3826 | Test Loss: 0.2747 | F1: 0.7241 | ROC-AUC: 0.9118\n",
      "2025-11-12 21:11:58,234 - INFO - Epoch 2/10 [4.56s] | Train Loss: 0.2695 | Test Loss: 0.2385 | F1: 0.7585 | ROC-AUC: 0.9378\n",
      "2025-11-12 21:12:03,739 - INFO - Epoch 3/10 [5.50s] | Train Loss: 0.2476 | Test Loss: 0.2404 | F1: 0.7677 | ROC-AUC: 0.9397\n",
      "2025-11-12 21:12:09,030 - INFO - Epoch 4/10 [5.29s] | Train Loss: 0.2372 | Test Loss: 0.2334 | F1: 0.7702 | ROC-AUC: 0.9348\n",
      "2025-11-12 21:12:14,959 - INFO - Epoch 5/10 [5.93s] | Train Loss: 0.2307 | Test Loss: 0.2150 | F1: 0.7899 | ROC-AUC: 0.9462\n",
      "2025-11-12 21:12:20,961 - INFO - Epoch 6/10 [6.00s] | Train Loss: 0.2241 | Test Loss: 0.2136 | F1: 0.7933 | ROC-AUC: 0.9482\n",
      "2025-11-12 21:12:25,944 - INFO - Epoch 7/10 [4.98s] | Train Loss: 0.2205 | Test Loss: 0.2109 | F1: 0.7854 | ROC-AUC: 0.9502\n",
      "2025-11-12 21:12:30,862 - INFO - Epoch 8/10 [4.92s] | Train Loss: 0.2176 | Test Loss: 0.2173 | F1: 0.7752 | ROC-AUC: 0.9483\n",
      "2025-11-12 21:12:36,370 - INFO - Epoch 9/10 [5.51s] | Train Loss: 0.2157 | Test Loss: 0.2203 | F1: 0.7797 | ROC-AUC: 0.9482\n",
      "2025-11-12 21:12:41,328 - INFO - Epoch 10/10 [4.96s] | Train Loss: 0.2137 | Test Loss: 0.2202 | F1: 0.7745 | ROC-AUC: 0.9464\n",
      "2025-11-12 21:12:41,329 - INFO - Training complete.\n",
      "2025-11-12 21:12:41,334 - INFO - {'loss': 0.214, 'accuracy': 0.922, 'f1': 0.793, 'roc_auc': 0.948, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.6, 'train_size': 92281, 'test_size': 33464, 'train_accounts': 590, 'test_accounts': 246}\n",
      "2025-11-12 21:12:41,345 - INFO - Yielding 70% split: 688 accounts, 108248 rows\n",
      "2025-11-12 21:12:42,004 - INFO - len(text_list)=108248 len(unique_texts)=65808 len(texts_to_embed)=0\n",
      "2025-11-12 21:12:44,090 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 21:12:44,628 - INFO - Fitting processor on 108248 rows...\n",
      "2025-11-12 21:12:44,629 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 21:12:44,630 - INFO - Transforming 108248 rows...\n",
      "2025-11-12 21:12:44,631 - INFO - Transform complete.\n",
      "2025-11-12 21:12:44,632 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 21:12:44,633 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 21:12:44,692 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 21:12:50,183 - INFO - Epoch 1/10 [5.49s] | Train Loss: 0.3685 | Test Loss: 0.3295 | F1: 0.7243 | ROC-AUC: 0.9114\n",
      "2025-11-12 21:12:56,727 - INFO - Epoch 2/10 [6.54s] | Train Loss: 0.2658 | Test Loss: 0.2423 | F1: 0.7450 | ROC-AUC: 0.9368\n",
      "2025-11-12 21:13:02,364 - INFO - Epoch 3/10 [5.64s] | Train Loss: 0.2454 | Test Loss: 0.2580 | F1: 0.7675 | ROC-AUC: 0.9315\n",
      "2025-11-12 21:13:09,867 - INFO - Epoch 4/10 [7.50s] | Train Loss: 0.2337 | Test Loss: 0.2473 | F1: 0.7135 | ROC-AUC: 0.9427\n",
      "2025-11-12 21:13:17,512 - INFO - Epoch 5/10 [7.64s] | Train Loss: 0.2256 | Test Loss: 0.2476 | F1: 0.7862 | ROC-AUC: 0.9465\n",
      "2025-11-12 21:13:23,728 - INFO - Epoch 6/10 [6.22s] | Train Loss: 0.2220 | Test Loss: 0.2196 | F1: 0.7736 | ROC-AUC: 0.9465\n",
      "2025-11-12 21:13:29,693 - INFO - Epoch 7/10 [5.96s] | Train Loss: 0.2187 | Test Loss: 0.2269 | F1: 0.7731 | ROC-AUC: 0.9437\n",
      "2025-11-12 21:13:35,914 - INFO - Epoch 8/10 [6.22s] | Train Loss: 0.2170 | Test Loss: 0.2404 | F1: 0.6972 | ROC-AUC: 0.9448\n",
      "2025-11-12 21:13:41,791 - INFO - Epoch 9/10 [5.88s] | Train Loss: 0.2137 | Test Loss: 0.2170 | F1: 0.7696 | ROC-AUC: 0.9502\n",
      "2025-11-12 21:13:47,975 - INFO - Epoch 10/10 [6.18s] | Train Loss: 0.2120 | Test Loss: 0.2256 | F1: 0.7833 | ROC-AUC: 0.9408\n",
      "2025-11-12 21:13:47,976 - INFO - Training complete.\n",
      "2025-11-12 21:13:47,985 - INFO - {'loss': 0.248, 'accuracy': 0.91, 'f1': 0.786, 'roc_auc': 0.947, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.7, 'train_size': 108248, 'test_size': 33464, 'train_accounts': 688, 'test_accounts': 246}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{16704: {'loss': 0.31,\n",
       "  'accuracy': 0.899,\n",
       "  'f1': 0.746,\n",
       "  'roc_auc': 0.909,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.1,\n",
       "  'train_size': 16704,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 98,\n",
       "  'test_accounts': 246},\n",
       " 29736: {'loss': 0.257,\n",
       "  'accuracy': 0.907,\n",
       "  'f1': 0.742,\n",
       "  'roc_auc': 0.916,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.2,\n",
       "  'train_size': 29736,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 196,\n",
       "  'test_accounts': 246},\n",
       " 42410: {'loss': 0.232,\n",
       "  'accuracy': 0.916,\n",
       "  'f1': 0.776,\n",
       "  'roc_auc': 0.938,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.3,\n",
       "  'train_size': 42410,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 295,\n",
       "  'test_accounts': 246},\n",
       " 59801: {'loss': 0.234,\n",
       "  'accuracy': 0.92,\n",
       "  'f1': 0.779,\n",
       "  'roc_auc': 0.937,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.4,\n",
       "  'train_size': 59801,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 393,\n",
       "  'test_accounts': 246},\n",
       " 75623: {'loss': 0.216,\n",
       "  'accuracy': 0.924,\n",
       "  'f1': 0.792,\n",
       "  'roc_auc': 0.945,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.5,\n",
       "  'train_size': 75623,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 492,\n",
       "  'test_accounts': 246},\n",
       " 92281: {'loss': 0.214,\n",
       "  'accuracy': 0.922,\n",
       "  'f1': 0.793,\n",
       "  'roc_auc': 0.948,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.6,\n",
       "  'train_size': 92281,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 590,\n",
       "  'test_accounts': 246},\n",
       " 108248: {'loss': 0.248,\n",
       "  'accuracy': 0.91,\n",
       "  'f1': 0.786,\n",
       "  'roc_auc': 0.947,\n",
       "  'embedder.model_name': 'albert-base-v2',\n",
       "  'train_frac': 0.7,\n",
       "  'train_size': 108248,\n",
       "  'test_size': 33464,\n",
       "  'train_accounts': 688,\n",
       "  'test_accounts': 246}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T19:13:48.326109Z",
     "start_time": "2025-11-12T19:13:48.322479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# runner3 = ExpRunner.copy(runner1)\n",
    "# runner3.feat_proc_params = FeatProcParams(n_bins=20, k_top=50)\n",
    "# runner3.run_torch([0.1])\n"
   ],
   "id": "a26f355bbf7e1888",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T21:09:15.776433Z",
     "start_time": "2025-11-12T21:03:30.169793Z"
    }
   },
   "cell_type": "code",
   "source": "results2 = runner2.run_torch(fracs)\n",
   "id": "5b1cc1626d140f29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-12 23:03:30,179 - INFO - Splitting 189987 rows by group 'accountId'...\n",
      "2025-11-12 23:03:30,267 - INFO - Train accounts: 984, Test accounts: 246\n",
      "2025-11-12 23:03:30,269 - INFO - SUCCESS: No account overlap between train and test sets.\n",
      "2025-11-12 23:03:30,280 - INFO - Split complete. Train: len=156523 accounts=984, Test: len=33464 accounts=246.\n",
      "2025-11-12 23:03:30,288 - INFO - Preparing to create 7 training set fractions...\n",
      "2025-11-12 23:03:30,310 - INFO - Yielding 10% split: 98 accounts, 16704 rows\n",
      "2025-11-12 23:03:30,438 - INFO - len(text_list)=16704 len(unique_texts)=10952 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:31,234 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:31,888 - INFO - Fitting processor on 16704 rows...\n",
      "2025-11-12 23:03:31,888 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:03:31,889 - INFO - Transforming 16704 rows...\n",
      "2025-11-12 23:03:31,891 - INFO - Transform complete.\n",
      "2025-11-12 23:03:31,892 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:03:31,893 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:03:31,913 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:03:33,113 - INFO - Epoch 1/10 [1.20s] | Train Loss: 0.5402 | Test Loss: 0.3942 | F1: 0.3268 | ROC-AUC: 0.8249\n",
      "2025-11-12 23:03:34,319 - INFO - Epoch 2/10 [1.21s] | Train Loss: 0.3881 | Test Loss: 0.3652 | F1: 0.7097 | ROC-AUC: 0.8953\n",
      "2025-11-12 23:03:35,496 - INFO - Epoch 3/10 [1.18s] | Train Loss: 0.3180 | Test Loss: 0.6218 | F1: 0.5331 | ROC-AUC: 0.8380\n",
      "2025-11-12 23:03:36,691 - INFO - Epoch 4/10 [1.19s] | Train Loss: 0.2786 | Test Loss: 0.2922 | F1: 0.5913 | ROC-AUC: 0.9210\n",
      "2025-11-12 23:03:37,814 - INFO - Epoch 5/10 [1.12s] | Train Loss: 0.2624 | Test Loss: 0.2841 | F1: 0.7387 | ROC-AUC: 0.9122\n",
      "2025-11-12 23:03:38,931 - INFO - Epoch 6/10 [1.12s] | Train Loss: 0.2503 | Test Loss: 0.3447 | F1: 0.6682 | ROC-AUC: 0.8837\n",
      "2025-11-12 23:03:40,104 - INFO - Epoch 7/10 [1.17s] | Train Loss: 0.2392 | Test Loss: 0.2801 | F1: 0.7168 | ROC-AUC: 0.9146\n",
      "2025-11-12 23:03:41,290 - INFO - Epoch 8/10 [1.19s] | Train Loss: 0.2363 | Test Loss: 0.2715 | F1: 0.7301 | ROC-AUC: 0.9194\n",
      "2025-11-12 23:03:42,452 - INFO - Epoch 9/10 [1.16s] | Train Loss: 0.2326 | Test Loss: 0.3754 | F1: 0.7030 | ROC-AUC: 0.9140\n",
      "2025-11-12 23:03:43,609 - INFO - Epoch 10/10 [1.16s] | Train Loss: 0.2237 | Test Loss: 0.2592 | F1: 0.7343 | ROC-AUC: 0.9295\n",
      "2025-11-12 23:03:43,610 - INFO - Training complete.\n",
      "2025-11-12 23:03:43,636 - INFO - {'loss': 0.284, 'accuracy': 0.902, 'f1': 0.739, 'roc_auc': 0.912, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.1, 'train_size': 16704, 'test_size': 33464, 'train_accounts': 98, 'test_accounts': 246}\n",
      "2025-11-12 23:03:43,645 - INFO - Yielding 20% split: 196 accounts, 29736 rows\n",
      "2025-11-12 23:03:43,814 - INFO - len(text_list)=29736 len(unique_texts)=19347 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:44,452 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:03:45,001 - INFO - Fitting processor on 29736 rows...\n",
      "2025-11-12 23:03:45,002 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:03:45,003 - INFO - Transforming 29736 rows...\n",
      "2025-11-12 23:03:45,004 - INFO - Transform complete.\n",
      "2025-11-12 23:03:45,004 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:03:45,006 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:03:45,042 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:03:46,608 - INFO - Epoch 1/10 [1.56s] | Train Loss: 0.5137 | Test Loss: 0.4197 | F1: 0.6848 | ROC-AUC: 0.8764\n",
      "2025-11-12 23:03:48,170 - INFO - Epoch 2/10 [1.56s] | Train Loss: 0.3277 | Test Loss: 0.3060 | F1: 0.7422 | ROC-AUC: 0.9043\n",
      "2025-11-12 23:03:49,726 - INFO - Epoch 3/10 [1.55s] | Train Loss: 0.2764 | Test Loss: 0.3424 | F1: 0.6772 | ROC-AUC: 0.8781\n",
      "2025-11-12 23:03:51,266 - INFO - Epoch 4/10 [1.54s] | Train Loss: 0.2596 | Test Loss: 0.2621 | F1: 0.7282 | ROC-AUC: 0.9191\n",
      "2025-11-12 23:03:52,868 - INFO - Epoch 5/10 [1.60s] | Train Loss: 0.2402 | Test Loss: 0.2612 | F1: 0.7608 | ROC-AUC: 0.9209\n",
      "2025-11-12 23:03:54,500 - INFO - Epoch 6/10 [1.63s] | Train Loss: 0.2377 | Test Loss: 0.2826 | F1: 0.6204 | ROC-AUC: 0.9271\n",
      "2025-11-12 23:03:56,217 - INFO - Epoch 7/10 [1.72s] | Train Loss: 0.2333 | Test Loss: 0.2713 | F1: 0.7554 | ROC-AUC: 0.9110\n",
      "2025-11-12 23:03:57,998 - INFO - Epoch 8/10 [1.78s] | Train Loss: 0.2269 | Test Loss: 0.2600 | F1: 0.7304 | ROC-AUC: 0.9173\n",
      "2025-11-12 23:03:59,826 - INFO - Epoch 9/10 [1.83s] | Train Loss: 0.2189 | Test Loss: 0.2483 | F1: 0.7235 | ROC-AUC: 0.9292\n",
      "2025-11-12 23:04:01,685 - INFO - Epoch 10/10 [1.86s] | Train Loss: 0.2161 | Test Loss: 0.2551 | F1: 0.7225 | ROC-AUC: 0.9279\n",
      "2025-11-12 23:04:01,686 - INFO - Training complete.\n",
      "2025-11-12 23:04:01,688 - INFO - {'loss': 0.261, 'accuracy': 0.908, 'f1': 0.761, 'roc_auc': 0.921, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.2, 'train_size': 29736, 'test_size': 33464, 'train_accounts': 196, 'test_accounts': 246}\n",
      "2025-11-12 23:04:01,697 - INFO - Yielding 30% split: 295 accounts, 42410 rows\n",
      "2025-11-12 23:04:01,961 - INFO - len(text_list)=42410 len(unique_texts)=26700 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:03,766 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:04,861 - INFO - Fitting processor on 42410 rows...\n",
      "2025-11-12 23:04:04,862 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:04:04,863 - INFO - Transforming 42410 rows...\n",
      "2025-11-12 23:04:04,863 - INFO - Transform complete.\n",
      "2025-11-12 23:04:04,864 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:04:04,865 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:04:04,902 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:04:08,540 - INFO - Epoch 1/10 [3.64s] | Train Loss: 0.4850 | Test Loss: 0.3557 | F1: 0.6780 | ROC-AUC: 0.8853\n",
      "2025-11-12 23:04:12,099 - INFO - Epoch 2/10 [3.56s] | Train Loss: 0.3029 | Test Loss: 0.2977 | F1: 0.5623 | ROC-AUC: 0.9281\n",
      "2025-11-12 23:04:15,692 - INFO - Epoch 3/10 [3.59s] | Train Loss: 0.2740 | Test Loss: 0.6315 | F1: 0.5537 | ROC-AUC: 0.9015\n",
      "2025-11-12 23:04:19,180 - INFO - Epoch 4/10 [3.49s] | Train Loss: 0.2605 | Test Loss: 0.3186 | F1: 0.5851 | ROC-AUC: 0.9271\n",
      "2025-11-12 23:04:22,812 - INFO - Epoch 5/10 [3.63s] | Train Loss: 0.2490 | Test Loss: 0.2394 | F1: 0.7435 | ROC-AUC: 0.9330\n",
      "2025-11-12 23:04:26,639 - INFO - Epoch 6/10 [3.83s] | Train Loss: 0.2398 | Test Loss: 0.2460 | F1: 0.7331 | ROC-AUC: 0.9384\n",
      "2025-11-12 23:04:30,319 - INFO - Epoch 7/10 [3.68s] | Train Loss: 0.2326 | Test Loss: 0.2581 | F1: 0.7001 | ROC-AUC: 0.9257\n",
      "2025-11-12 23:04:34,047 - INFO - Epoch 8/10 [3.73s] | Train Loss: 0.2289 | Test Loss: 0.2550 | F1: 0.7125 | ROC-AUC: 0.9377\n",
      "2025-11-12 23:04:37,804 - INFO - Epoch 9/10 [3.76s] | Train Loss: 0.2219 | Test Loss: 0.2299 | F1: 0.7668 | ROC-AUC: 0.9390\n",
      "2025-11-12 23:04:41,580 - INFO - Epoch 10/10 [3.78s] | Train Loss: 0.2201 | Test Loss: 0.2259 | F1: 0.7727 | ROC-AUC: 0.9443\n",
      "2025-11-12 23:04:41,581 - INFO - Training complete.\n",
      "2025-11-12 23:04:41,585 - INFO - {'loss': 0.226, 'accuracy': 0.917, 'f1': 0.773, 'roc_auc': 0.944, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.3, 'train_size': 42410, 'test_size': 33464, 'train_accounts': 295, 'test_accounts': 246}\n",
      "2025-11-12 23:04:41,596 - INFO - Yielding 40% split: 393 accounts, 59801 rows\n",
      "2025-11-12 23:04:42,041 - INFO - len(text_list)=59801 len(unique_texts)=37409 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:43,698 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:04:44,494 - INFO - Fitting processor on 59801 rows...\n",
      "2025-11-12 23:04:44,495 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:04:44,496 - INFO - Transforming 59801 rows...\n",
      "2025-11-12 23:04:44,497 - INFO - Transform complete.\n",
      "2025-11-12 23:04:44,497 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:04:44,499 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:04:44,552 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:04:49,102 - INFO - Epoch 1/10 [4.55s] | Train Loss: 0.4326 | Test Loss: 0.3931 | F1: 0.3665 | ROC-AUC: 0.8420\n",
      "2025-11-12 23:04:53,774 - INFO - Epoch 2/10 [4.67s] | Train Loss: 0.2958 | Test Loss: 0.2713 | F1: 0.7420 | ROC-AUC: 0.9065\n",
      "2025-11-12 23:04:58,620 - INFO - Epoch 3/10 [4.84s] | Train Loss: 0.2667 | Test Loss: 0.2543 | F1: 0.7486 | ROC-AUC: 0.9231\n",
      "2025-11-12 23:05:03,570 - INFO - Epoch 4/10 [4.95s] | Train Loss: 0.2516 | Test Loss: 0.2458 | F1: 0.7537 | ROC-AUC: 0.9275\n",
      "2025-11-12 23:05:08,314 - INFO - Epoch 5/10 [4.74s] | Train Loss: 0.2435 | Test Loss: 0.2845 | F1: 0.7347 | ROC-AUC: 0.9094\n",
      "2025-11-12 23:05:13,131 - INFO - Epoch 6/10 [4.82s] | Train Loss: 0.2369 | Test Loss: 0.2336 | F1: 0.7699 | ROC-AUC: 0.9345\n",
      "2025-11-12 23:05:17,813 - INFO - Epoch 7/10 [4.68s] | Train Loss: 0.2291 | Test Loss: 0.2563 | F1: 0.7485 | ROC-AUC: 0.9255\n",
      "2025-11-12 23:05:22,407 - INFO - Epoch 8/10 [4.59s] | Train Loss: 0.2277 | Test Loss: 0.2225 | F1: 0.7806 | ROC-AUC: 0.9424\n",
      "2025-11-12 23:05:27,105 - INFO - Epoch 9/10 [4.70s] | Train Loss: 0.2233 | Test Loss: 0.2362 | F1: 0.7782 | ROC-AUC: 0.9400\n",
      "2025-11-12 23:05:32,033 - INFO - Epoch 10/10 [4.93s] | Train Loss: 0.2197 | Test Loss: 0.2244 | F1: 0.7513 | ROC-AUC: 0.9437\n",
      "2025-11-12 23:05:32,034 - INFO - Training complete.\n",
      "2025-11-12 23:05:32,041 - INFO - {'loss': 0.222, 'accuracy': 0.92, 'f1': 0.781, 'roc_auc': 0.942, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.4, 'train_size': 59801, 'test_size': 33464, 'train_accounts': 393, 'test_accounts': 246}\n",
      "2025-11-12 23:05:32,062 - INFO - Yielding 50% split: 492 accounts, 75623 rows\n",
      "2025-11-12 23:05:38,224 - INFO - len(text_list)=75623 len(unique_texts)=46835 len(texts_to_embed)=0\n",
      "2025-11-12 23:05:44,988 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:05:45,804 - INFO - Fitting processor on 75623 rows...\n",
      "2025-11-12 23:05:45,805 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:05:45,806 - INFO - Transforming 75623 rows...\n",
      "2025-11-12 23:05:45,807 - INFO - Transform complete.\n",
      "2025-11-12 23:05:45,807 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:05:45,808 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:05:45,861 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:05:51,115 - INFO - Epoch 1/10 [5.25s] | Train Loss: 0.4109 | Test Loss: 0.2848 | F1: 0.6521 | ROC-AUC: 0.9228\n",
      "2025-11-12 23:05:56,273 - INFO - Epoch 2/10 [5.16s] | Train Loss: 0.2780 | Test Loss: 0.2568 | F1: 0.7728 | ROC-AUC: 0.9215\n",
      "2025-11-12 23:06:01,806 - INFO - Epoch 3/10 [5.53s] | Train Loss: 0.2578 | Test Loss: 0.2307 | F1: 0.7689 | ROC-AUC: 0.9372\n",
      "2025-11-12 23:06:07,470 - INFO - Epoch 4/10 [5.66s] | Train Loss: 0.2456 | Test Loss: 0.2719 | F1: 0.7565 | ROC-AUC: 0.9296\n",
      "2025-11-12 23:06:13,066 - INFO - Epoch 5/10 [5.60s] | Train Loss: 0.2331 | Test Loss: 0.2173 | F1: 0.7790 | ROC-AUC: 0.9469\n",
      "2025-11-12 23:06:18,674 - INFO - Epoch 6/10 [5.61s] | Train Loss: 0.2290 | Test Loss: 0.2490 | F1: 0.7756 | ROC-AUC: 0.9269\n",
      "2025-11-12 23:06:24,550 - INFO - Epoch 7/10 [5.88s] | Train Loss: 0.2228 | Test Loss: 0.2804 | F1: 0.6344 | ROC-AUC: 0.9378\n",
      "2025-11-12 23:06:30,265 - INFO - Epoch 8/10 [5.71s] | Train Loss: 0.2197 | Test Loss: 0.2292 | F1: 0.7852 | ROC-AUC: 0.9349\n",
      "2025-11-12 23:06:35,929 - INFO - Epoch 9/10 [5.66s] | Train Loss: 0.2171 | Test Loss: 0.2127 | F1: 0.7905 | ROC-AUC: 0.9459\n",
      "2025-11-12 23:06:41,982 - INFO - Epoch 10/10 [6.05s] | Train Loss: 0.2135 | Test Loss: 0.2176 | F1: 0.7729 | ROC-AUC: 0.9513\n",
      "2025-11-12 23:06:41,983 - INFO - Training complete.\n",
      "2025-11-12 23:06:41,990 - INFO - {'loss': 0.213, 'accuracy': 0.923, 'f1': 0.79, 'roc_auc': 0.946, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.5, 'train_size': 75623, 'test_size': 33464, 'train_accounts': 492, 'test_accounts': 246}\n",
      "2025-11-12 23:06:42,003 - INFO - Yielding 60% split: 590 accounts, 92281 rows\n",
      "2025-11-12 23:06:43,983 - INFO - len(text_list)=92281 len(unique_texts)=56280 len(texts_to_embed)=0\n",
      "2025-11-12 23:06:46,428 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:06:47,082 - INFO - Fitting processor on 92281 rows...\n",
      "2025-11-12 23:06:47,082 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:06:47,083 - INFO - Transforming 92281 rows...\n",
      "2025-11-12 23:06:47,084 - INFO - Transform complete.\n",
      "2025-11-12 23:06:47,084 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:06:47,085 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:06:47,146 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:06:53,103 - INFO - Epoch 1/10 [5.96s] | Train Loss: 0.3903 | Test Loss: 0.3132 | F1: 0.6007 | ROC-AUC: 0.9043\n",
      "2025-11-12 23:06:59,219 - INFO - Epoch 2/10 [6.11s] | Train Loss: 0.2732 | Test Loss: 0.2501 | F1: 0.7334 | ROC-AUC: 0.9321\n",
      "2025-11-12 23:07:06,054 - INFO - Epoch 3/10 [6.83s] | Train Loss: 0.2520 | Test Loss: 0.2468 | F1: 0.7436 | ROC-AUC: 0.9314\n",
      "2025-11-12 23:07:12,663 - INFO - Epoch 4/10 [6.61s] | Train Loss: 0.2383 | Test Loss: 0.2255 | F1: 0.7777 | ROC-AUC: 0.9400\n",
      "2025-11-12 23:07:19,484 - INFO - Epoch 5/10 [6.82s] | Train Loss: 0.2307 | Test Loss: 0.2335 | F1: 0.7436 | ROC-AUC: 0.9449\n",
      "2025-11-12 23:07:26,178 - INFO - Epoch 6/10 [6.69s] | Train Loss: 0.2275 | Test Loss: 0.2238 | F1: 0.7817 | ROC-AUC: 0.9410\n",
      "2025-11-12 23:07:32,843 - INFO - Epoch 7/10 [6.66s] | Train Loss: 0.2239 | Test Loss: 0.2535 | F1: 0.7614 | ROC-AUC: 0.9257\n",
      "2025-11-12 23:07:39,629 - INFO - Epoch 8/10 [6.79s] | Train Loss: 0.2185 | Test Loss: 0.2216 | F1: 0.7895 | ROC-AUC: 0.9435\n",
      "2025-11-12 23:07:46,493 - INFO - Epoch 9/10 [6.86s] | Train Loss: 0.2190 | Test Loss: 0.2264 | F1: 0.7876 | ROC-AUC: 0.9442\n",
      "2025-11-12 23:07:53,347 - INFO - Epoch 10/10 [6.85s] | Train Loss: 0.2176 | Test Loss: 0.2244 | F1: 0.7880 | ROC-AUC: 0.9471\n",
      "2025-11-12 23:07:53,348 - INFO - Training complete.\n",
      "2025-11-12 23:07:53,352 - INFO - {'loss': 0.222, 'accuracy': 0.92, 'f1': 0.789, 'roc_auc': 0.944, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.6, 'train_size': 92281, 'test_size': 33464, 'train_accounts': 590, 'test_accounts': 246}\n",
      "2025-11-12 23:07:53,369 - INFO - Yielding 70% split: 688 accounts, 108248 rows\n",
      "2025-11-12 23:07:55,617 - INFO - len(text_list)=108248 len(unique_texts)=65808 len(texts_to_embed)=0\n",
      "2025-11-12 23:07:58,167 - INFO - len(text_list)=33464 len(unique_texts)=20665 len(texts_to_embed)=0\n",
      "2025-11-12 23:07:58,873 - INFO - Fitting processor on 108248 rows...\n",
      "2025-11-12 23:07:58,873 - INFO - Categorical amount feature is disabled. Skipping vocab fit.\n",
      "2025-11-12 23:07:58,874 - INFO - Transforming 108248 rows...\n",
      "2025-11-12 23:07:58,875 - INFO - Transform complete.\n",
      "2025-11-12 23:07:58,876 - INFO - Transforming 33464 rows...\n",
      "2025-11-12 23:07:58,876 - INFO - Transform complete.\n",
      "Model Init: use_text=True, use_continuous=False, use_categorical=False\n",
      "Total input dim to MLP: 768\n",
      "2025-11-12 23:07:58,930 - INFO - Starting PyTorch training on cpu for 10 epochs...\n",
      "2025-11-12 23:08:05,961 - INFO - Epoch 1/10 [7.03s] | Train Loss: 0.3721 | Test Loss: 0.2713 | F1: 0.6816 | ROC-AUC: 0.9313\n",
      "2025-11-12 23:08:13,296 - INFO - Epoch 2/10 [7.33s] | Train Loss: 0.2660 | Test Loss: 0.2425 | F1: 0.7351 | ROC-AUC: 0.9400\n",
      "2025-11-12 23:08:20,981 - INFO - Epoch 3/10 [7.68s] | Train Loss: 0.2447 | Test Loss: 0.2411 | F1: 0.7811 | ROC-AUC: 0.9380\n",
      "2025-11-12 23:08:28,734 - INFO - Epoch 4/10 [7.75s] | Train Loss: 0.2353 | Test Loss: 0.2441 | F1: 0.7750 | ROC-AUC: 0.9336\n",
      "2025-11-12 23:08:36,473 - INFO - Epoch 5/10 [7.74s] | Train Loss: 0.2326 | Test Loss: 0.2413 | F1: 0.7459 | ROC-AUC: 0.9367\n",
      "2025-11-12 23:08:44,290 - INFO - Epoch 6/10 [7.82s] | Train Loss: 0.2253 | Test Loss: 0.2277 | F1: 0.7488 | ROC-AUC: 0.9467\n",
      "2025-11-12 23:08:52,268 - INFO - Epoch 7/10 [7.98s] | Train Loss: 0.2209 | Test Loss: 0.2335 | F1: 0.7512 | ROC-AUC: 0.9424\n",
      "2025-11-12 23:09:00,130 - INFO - Epoch 8/10 [7.86s] | Train Loss: 0.2179 | Test Loss: 0.2166 | F1: 0.7871 | ROC-AUC: 0.9489\n",
      "2025-11-12 23:09:08,070 - INFO - Epoch 9/10 [7.94s] | Train Loss: 0.2150 | Test Loss: 0.2288 | F1: 0.7693 | ROC-AUC: 0.9486\n",
      "2025-11-12 23:09:15,744 - INFO - Epoch 10/10 [7.67s] | Train Loss: 0.2137 | Test Loss: 0.2354 | F1: 0.7860 | ROC-AUC: 0.9440\n",
      "2025-11-12 23:09:15,745 - INFO - Training complete.\n",
      "2025-11-12 23:09:15,752 - INFO - {'loss': 0.217, 'accuracy': 0.92, 'f1': 0.787, 'roc_auc': 0.949, 'embedder.model_name': 'albert-base-v2', 'train_frac': 0.7, 'train_size': 108248, 'test_size': 33464, 'train_accounts': 688, 'test_accounts': 246}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e6aef1cf8d361873"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
